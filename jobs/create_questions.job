#!/bin/bash

#SBATCH --partition=gpu_h100
#SBATCH --gpus=2
#SBATCH --job-name=CreatingQuestions
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=6
#SBATCH --time=04:30:00
#SBATCH --output=slurm_output/questions_%A.out
#SBATCH --array=0-140%48

module purge
module load 2025
module load Miniconda3/25.5.1-1

cd $HOME/Github/text-to-sql-dataset/
conda env create -f conda-env.yml

source activate cowolff

srun python create_questions.py --schema_csv data/complete/filelist.csv --model Qwen/Qwen3-32B-FP8 --out data/complete/questions/questions_${SLURM_ARRAY_TASK_ID}.jsonl --tensor_parallel_size 2 --max_batch_size 256 --per_schema 5 --example_questions_file data/examples.csv --retries 3 --start $((SLURM_ARRAY_TASK_ID * 2500)) --end $((SLURM_ARRAY_TASK_ID * 2500 + 2500))
