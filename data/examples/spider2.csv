,id,db_or_proj,nl_prompt,sql_query,docs,dbms
0,bq011,ga4,"How many distinct pseudo users had positive engagement time  in the 7-day period ending on January 7, 2021 at 23:59:59, but had no positive engagement time in the 2-day period ending on the same date (January 7, 2021 at 23:59:59) ?","SELECT
  COUNT(DISTINCT MDaysUsers.user_pseudo_id) AS n_day_inactive_users_count
FROM
  (
    SELECT
      user_pseudo_id
    FROM
      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T
    CROSS JOIN
      UNNEST(T.event_params) AS event_params
    WHERE
      event_params.key = 'engagement_time_msec' AND event_params.value.int_value > 0
      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP('2021-01-07 23:59:59'), INTERVAL 7 DAY))
      AND _TABLE_SUFFIX BETWEEN '20210101' AND '20210107'
  ) AS MDaysUsers
LEFT JOIN
  (
    SELECT
      user_pseudo_id
    FROM
      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*` AS T
    CROSS JOIN
      UNNEST(T.event_params) AS event_params
    WHERE
      event_params.key = 'engagement_time_msec' AND event_params.value.int_value > 0
      AND event_timestamp > UNIX_MICROS(TIMESTAMP_SUB(TIMESTAMP('2021-01-07 23:59:59'), INTERVAL 2 DAY))
      AND _TABLE_SUFFIX BETWEEN '20210105' AND '20210107'
  ) AS NDaysUsers
ON MDaysUsers.user_pseudo_id = NDaysUsers.user_pseudo_id
WHERE
  NDaysUsers.user_pseudo_id IS NULL;","# GA4 - BigQuery Export schema

This article explains the format and schema of the Google Analytics 4 property data and the Google Analytics for Firebase data that is exported to BigQuery.

## Datasets

For each Google Analytics 4 property and each Firebase project that is linked to BigQuery, a single dataset named ""analytics_<property_id>"" is added to your BigQuery project. Property ID refers to your Analytics Property ID, which you can find in the property settings for your Google Analytics 4 property, and in App Analytics Settings in Firebase. Each Google Analytics 4 property and each app for which BigQuery exporting is enabled will export its data to that single dataset.

## Tables

Within each dataset, a table named `events_YYYYMMDD` is created each day if the Daily export option is enabled.

If the Streaming export option is enabled, a table named`events_intraday_YYYYMMDD`is created. This table is populated continuously as events are recorded throughout the day. This table is deleted at the end of each day once `events_YYYYMMDD` is complete.

Not all devices on which events are triggered send their data to Analytics on the same day the events are triggered. To account for this latency, Analytics will update the daily tables (`events_YYYYMMDD`) with events for those dates for up to three days after the dates of the events. Events will have the correct time stamp regardless of arriving late. Events that arrive after that three-day window are not recorded.

## Columns

Each column in the `events_YYYYMMDD`table represents an event-specific parameter. Note that some parameters are nested within RECORDS, and some RECORDS such as
`[items](https://support.google.com/analytics/answer/7029846?hl=en&ref_topic=9359001&sjid=16041154979191290935-EU#items)`
and
`[event_params](https://support.google.com/analytics/answer/7029846?hl=en&ref_topic=9359001&sjid=16041154979191290935-EU#event_params)`
are repeatable. Table columns are described below.

### event

The event fields contain information that uniquely identifies an event.

| Field name | Data type | Description |
|---|---|---|
| batch_event_index | INTEGER | A number indicating the sequential order of each event within a batch based on their order of occurrence on the device. |
| batch_ordering_id | INTEGER | A monotonically increasing number that is incremented each time a network request is sent from a given page. |
| batch_page_id | INTEGER | A sequential number assigned to a page that increases for each subsequent page within an engagement. |
| event_date | STRING | The date when the event was logged (YYYYMMDD format in the registered timezone of your app). |
| event_timestamp | INTEGER | The time (in microseconds, UTC) when the event was logged on the client. |
| event_previous_timestamp | INTEGER | The time (in microseconds, UTC) when the event was previously logged on the client. |
| event_name | STRING | The name of the event. |
| event_value_in_usd | FLOAT | The currency-converted value (in USD) of the event's ""value"" parameter. |
| event_bundle_sequence_id | INTEGER | The sequential ID of the bundle in which these events were uploaded. |
| event_server_timestamp_offset | INTEGER | Timestamp offset between collection time and upload time in micros. |

#### event_params RECORD

The event_params RECORD can store campaign-level and contextual event parameters as well as any user-defined event parameters. The event_params RECORD is repeated for each key that is associated with an event. The set of parameters stored in the event_params RECORD is unique to each implementation. To see the complete list of event parameters for your implementation, query the event parameter list.

| Field name | Data type | Description |
|---|---|---|
| event_params.key | STRING | The name of the event parameter. |
| event_params.value | RECORD | A record containing the event parameter's value. |
| event_params.value.string_value | STRING | If the event parameter is represented by a string, such as a URL or campaign name, it is populated in this field. |
| event_params.value.int_value | INTEGER | If the event parameter is represented by an integer, it is populated in this field. |
| event_params.value.double_value | FLOAT | If the event parameter is represented by a double value, it is populated in this field. |
| event_params.value.float_value | FLOAT | If the event parameter is represented by a floating point value, it is populated in this field. This field is not currently in use. |

#### item_params RECORD

The item_params RECORD can store item parameters as well as any user-defined item parameters. The set of parameters stored in the item_params RECORD is unique to each implementation.

| Field name | Data type | Description |
|---|---|---|
| item_params.key | STRING | The name of the item parameter. |
| item_params.value | RECORD | A record containing the item parameter’s value. |
| item_params.value.string_value | STRING | If the item parameter is represented by a string, it is populated in this field. |
| item_params.value.int_value | INTEGER | If the item parameter is represented by an integer, it is populated in this field. |
| item_params.value.double_value | FLOAT | If the item parameter is represented by a double value, it is populated in this field. |
| item_params.value.float_value | FLOAT | If the item parameter is represented by a floating point value, it is populated in this field. | 

### user

The user fields contain information that uniquely identifies the user associated with the event.

| Field name | Data type | Description |
|---|---|---|
| is_active_user | BOOLEAN | Whether the user was active (True) or inactive (False) at any point in the calendar day Included in only the daily tables ( events_YYYYMMDD) . |
| user_id | STRING | The unique ID assigned to a user. |
| user_pseudo_id | STRING | The pseudonymous id (e.g., app instance ID) for the user. |
| user_first_touch_timestamp | INTEGER | The time (in microseconds) at which the user first opened the app or visited the site. |

#### privacy_info fields

The privacy_info fields contain information based on the consent status of a user when consent mode is enabled .

| Field name | Data type | Description |
|---|---|---|
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user. Possible values: Yes, No, Unset |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user. Possible values: Yes, No, Unset |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data. Possible values: Yes, No, Unset |

#### user_properties RECORD

The user_properties RECORD contains any user properties that you have set. It is repeated for each key that is associated with a user.

| Field name | Data type | Description |
|---|---|---|
| user_properties.key | STRING | The name of the user property. |
| user_properties.value | RECORD | A record for the user property value. |
| user_properties.value.string_value | STRING | The string value of the user property. |
| user_properties.value.int_value | INTEGER | The integer value of the user property. |
| user_properties.value.double_value | FLOAT | The double value of the user property. |
| user_properties.value.float_value | FLOAT | This field is currently unused. |
| user_properties.value.set_timestamp_micros | INTEGER | The time (in microseconds) at which the user property was last set. |

#### user_ltv RECORD

The user_ltv RECORD contains Lifetime Value information about the user. This RECORD is not populated in intraday tables.

| Field name | Data type | Description |
|---|---|---|
| user_ltv.revenue | FLOAT | The Lifetime Value (revenue) of the user. This field is not populated in intraday tables. |
| user_ltv.currency | STRING | The Lifetime Value (currency) of the user. This field is not populated in intraday tables. | 

### device

The device RECORD contains information about the device from which the event originated.

| Field name | Data type | Description |
|---|---|---|
| device.category | STRING | The device category (mobile, tablet, desktop). |
| device.mobile_brand_name | STRING | The device brand name. |
| device.mobile_model_name | STRING | The device model name. |
| device.mobile_marketing_name | STRING | The device marketing name. |
| device.mobile_os_hardware_model | STRING | The device model information retrieved directly from the operating system. |
| device.operating_system | STRING | The operating system of the device. |
| device.operating_system_version | STRING | The OS version. |
| device.vendor_id | STRING | IDFV (present only if IDFA is not collected). |
| device.advertising_id | STRING | Advertising ID/IDFA. |
| device.language | STRING | The OS language. |
| device.time_zone_offset_seconds | INTEGER | The offset from GMT in seconds. |
| device.is_limited_ad_tracking | BOOLEAN | The device's Limit Ad Tracking setting. On iOS14+, returns false if the IDFA is non-zero. |
| device.web_info.browser | STRING | The browser in which the user viewed content. |
| device.web_info.browser_version | STRING | The version of the browser in which the user viewed content. |
| device.web_info.hostname | STRING | The hostname associated with the logged event. | 

### geo

The geo RECORD contains information about the geographic location where the event was initiated.

| Field name | Data type | Description |
|---|---|---|
| geo.continent | STRING | The continent from which events were reported, based on IP address. |
| geo.sub_continent | STRING | The subcontinent from which events were reported, based on IP address. |
| geo.country | STRING | The country from which events were reported, based on IP address. |
| geo.region | STRING | The region from which events were reported, based on IP address. |
| geo.metro | STRING | The metro from which events were reported, based on IP address. |
| geo.city | STRING | The city from which events were reported, based on IP address. | 

### app_info

The app_info RECORD contains information about the app in which the event was initiated. | Field name | Data type | Description |
|---|---|---|
| app_info.id | STRING | The package name or bundle ID of the app. |
| app_info.firebase_app_id | STRING | The Firebase App ID associated with the app |
| app_info.install_source | STRING | The store that installed the app. |
| app_info.version | STRING | The app's versionName (Android) or short bundle version. | 

### collected_traffic_source

The collected_traffic_source RECORD contains the traffic source data that was collected with the event.

| Field name | Data type | Description |
|---|---|---|
| manual_campaign_id | STRING | The manual campaign id (utm_id) that was collected with the event. |
| manual_campaign_name | STRING | The manual campaign name (utm_campaign) that was collected with the event. |
| manual_source | STRING | The manual campaign source (utm_source) that was collected with the event. Also includes parsed parameters from referral params, not just UTM values. |
| manual_medium | STRING | The manual campaign medium (utm_medium) that was collected with the event. Also includes parsed parameters from referral params, not just UTM values. |
| manual_term | STRING | The manual campaign keyword/term (utm_term) that was collected with the event. |
| manual_content | STRING | The additional manual campaign metadata (utm_content) that was collected with the event. |
| manual_creative_format | STRING | The manual campaign creative format (utm_creative_format) that was collected with the event. |
| manual_marketing_tactic | STRING | The manual campaign marketing tactic (utm_marketing_tactic) that was collected with the event. |
| manual_source_platform | STRING | The manual campaign source platform (utm_source_platform) that was collected with the event. |
| gclid | STRING | The Google click identifier that was collected with the event. |
| dclid | STRING | The DoubleClick Click Identifier for Display and Video 360 and Campaign Manager 360 that was collected with the event. |
| srsltid | STRING | The Google Merchant Center identifier that was collected with the event. | 

### session_traffic_source_last_click

The session_traffic_source_last_click RECORD contains the last-click attributed session traffic source data across Google ads and manual contexts, where available.

| Field Name | Data Type | Description |
|---|---|---|
| session_traffic_source_last_click. manual_campaign.campaign_id | STRING | The ID of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.campaign_name | STRING | The name of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.medium | STRING | The medium of the last clicked manual campaign (e.g., paid search, organic search, email) |
| session_traffic_source_last_click. manual_campaign.term | STRING | The keyword/search term of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.content | STRING | Additional metadata of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.source_platform | STRING | The platform of the last clicked manual campaign (e.g., search engine, social media) |
| session_traffic_source_last_click. manual_campaign.source | STRING | The specific source within the platform of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.creative_format | STRING | The format of the creative of the last clicked manual campaign |
| session_traffic_source_last_click. manual_campaign.marketing_tactic | STRING | The marketing tactic of the last clicked manual campaign |
| session_traffic_source_last_click. google_ads_campaign.customer_id | STRING | The customer ID associated with the Google Ads account |
| session_traffic_source_last_click. google_ads_campaign.account_name | STRING | The name of the Google Ads account |
| session_traffic_source_last_click. google_ads_campaign.campaign_id | STRING | The ID of the Google Ads campaign |
| session_traffic_source_last_click. google_ads_campaign.campaign_name | STRING | The name of the Google Ads campaign |
| session_traffic_source_last_click. google_ads_campaign.ad_group_id | STRING | The ID of the ad group within the Google Ads campaign |
| session_traffic_source_last_click. google_ads_campaign.ad_group_name | STRING | The name of the ad group within the Google Ads campaign | 

### traffic_source

The traffic_source RECORD contains information about the traffic source that first acquired the user. This record is not populated in intraday tables. Note: The traffic_source values do not change if the user interacts with subsequent campaigns after installation.

| Field name | Data type | Description |
|---|---|---|
| traffic_source.name | STRING | Name of the marketing campaign that first acquired the user. This field is not populated in intraday tables. |
| traffic_source.medium | STRING | Name of the medium (paid search, organic search, email, etc.) that first acquired the user. This field is not populated in intraday tables. |
| traffic_source.source | STRING | Name of the network that first acquired the user. This field is not populated in intraday tables. | 

### stream and platform

The stream and platform fields contain information about the stream and the app platform.

| Field name | Data type | Description |
|---|---|---|
| stream_id | STRING | The numeric ID of the data stream from which the event originated. |
| platform | STRING | The data stream platform (Web, IOS or Android) from which the event originated. | 

### ecommerce

This ecommerce RECORD contains information about any ecommerce events that have been setup on a website or app.

| Field name | Data type | Description |
|---|---|---|
| ecommerce.total_item_quantity | INTEGER | Total number of items in this event, which is the sum of items.quantity. |
| ecommerce.purchase_revenue_in_usd | FLOAT | Purchase revenue of this event, represented in USD with standard unit. Populated for purchase event only. |
| ecommerce.purchase_revenue | FLOAT | Purchase revenue of this event, represented in local currency with standard unit. Populated for purchase event only. |
| ecommerce.refund_value_in_usd | FLOAT | The amount of refund in this event, represented in USD with standard unit. Populated for refund event only. |
| ecommerce.refund_value | FLOAT | The amount of refund in this event, represented in local currency with standard unit. Populated for refund event only. |
| ecommerce.shipping_value_in_usd | FLOAT | The shipping cost in this event, represented in USD with standard unit. |
| ecommerce.shipping_value | FLOAT | The shipping cost in this event, represented in local currency. |
| ecommerce.tax_value_in_usd | FLOAT | The tax value in this event, represented in USD with standard unit. |
| ecommerce.tax_value | FLOAT | The tax value in this event, represented in local currency with standard unit. |
| ecommerce.transaction_id | STRING | The transaction ID of the ecommerce transaction. |
| ecommerce.unique_items | INTEGER | The number of unique items in this event, based on item_id, item_name, and item_brand. | 

### items

The items RECORD contains information about items included in an event. It is repeated for each item.

| Field name | Data type | Description |
|---|---|---|
| items.item_id | STRING | The ID of the item. |
| items.item_name | STRING | The name of the item. |
| items.item_brand | STRING | The brand of the item. |
| items.item_variant | STRING | The variant of the item. |
| items.item_category | STRING | The category of the item. |
| items.item_category2 | STRING | The sub category of the item. |
| items.item_category3 | STRING | The sub category of the item. |
| items.item_category4 | STRING | The sub category of the item. |
| items.item_category5 | STRING | The sub category of the item. |
| items.price_in_usd | FLOAT | The price of the item, in USD with standard unit. |
| items.price | FLOAT | The price of the item in local currency. |
| items.quantity | INTEGER | The quantity of the item. Quantity set to 1 if not specified. |
| items.item_revenue_in_usd | FLOAT | The revenue of this item, calculated as price_in_usd * quantity. It is populated for purchase events only, in USD with standard unit. |
| items.item_revenue | FLOAT | The revenue of this item, calculated as price * quantity. It is populated for purchase events only, in local currency with standard unit. |
| items.item_refund_in_usd | FLOAT | The refund value of this item, calculated as price_in_usd * quantity. It is populated for refund events only, in USD with standard unit. |
| items.item_refund | FLOAT | The refund value of this item, calculated as price * quantity. It is populated for refund events only, in local currency with standard unit. |
| items.coupon | STRING | Coupon code applied to this item. |
| items.affiliation | STRING | A product affiliation to designate a supplying company or brick and mortar store location. |
| items.location_id | STRING | The location associated with the item. |
| items.item_list_id | STRING | The ID of the list in which the item was presented to the user. |
| items.item_list_name | STRING | The name of the list in which the item was presented to the user. |
| Items.item_list_index | STRING | The position of the item in a list. |
| items.promotion_id | STRING | The ID of a product promotion. |
| items.promotion_name | STRING | The name of a product promotion. |
| items.creative_name | STRING | The name of a creative used in a promotional spot. |
| items.creative_slot | STRING | The name of a creative slot. |

#### item_params RECORD

The item_params RECORD stores the custom item parameters that you defined. Note that predefined item parameters like item_id, item_name etc, are not included here, instead they are exported as explicit fields. The set of parameters stored in the item_params RECORD is unique to each implementation. To learn more about ecommerce implementations and the Google Analytics 4 items array, see Measure ecommerce .

| Field name | Data type | Description |
|---|---|---|
| items.item_params.key | STRING | The name of the item parameter. |
| items.item_params.value | RECORD | A record containing the item parameter’s value. |
| items.item_params.value.string_value | STRING | If the item parameter is represented by a string, it is populated in this field. |
| items.item_params.value.int_value | INTEGER | If the item parameter is represented by an integer, it is populated in this field. |
| items.item_params.value.double_value | FLOAT | If the item parameter is represented by a double value, it is populated in this field. |
| items.item_params.value.float_value | FLOAT | If the item parameter is represented by a floating point value, it is populated in this field. | 

## Rows

Data for a single event may be represented in one or multiple rows, depending on whether it contains repeated RECORDS. A `page_view` event with multiple
`[event_params](https://support.google.com/analytics/answer/7029846?hl=en&ref_topic=9359001&sjid=16041154979191290935-EU#event_params)`,
for example, would look similar to the following table. The initial row contains the event name, date, timestamp and other non-repeated data items. The
`[event_params](https://support.google.com/analytics/answer/7029846?hl=en&ref_topic=9359001&sjid=16041154979191290935-EU#event_params)`
RECORD is repeated for each parameter associated with the event. These repeated RECORDS are populated in subsequent rows directly under the initial event row.

| event_date | event_timestamp | event_name | event_params.key | event_params_value.string_value |
|---|---|---|---|---|
| 20220222 | 1643673600483790 | page_view | page_location | https://example.com |
| - | - | - | page_title | Home |
| - | - | - | medium | referral |
| - | - | - | source | google |
| - | - | - | page_referrer | https://www.google.com |
| - | - | - | parameters... | values... |",snowflake
1,bq010,ga360,"Find the top-selling product among customers who bought 'Youtube Men’s Vintage Henley' in July 2017, excluding itself.","WITH GET_CUS_ID AS (
    SELECT 
        DISTINCT fullVisitorId as Henley_CUSTOMER_ID
    FROM 
        `bigquery-public-data.google_analytics_sample.ga_sessions_201707*`,
        UNNEST(hits) AS hits,
        UNNEST(hits.product) as product
    WHERE
        product.v2ProductName = ""YouTube Men's Vintage Henley""
        AND product.productRevenue IS NOT NULL
    )

SELECT
    product.v2ProductName AS other_purchased_products
FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_201707*` TAB_A 
    RIGHT JOIN GET_CUS_ID
    ON GET_CUS_ID.Henley_CUSTOMER_ID=TAB_A.fullVisitorId,
    UNNEST(hits) AS hits,
    UNNEST(hits.product) as product
WHERE
    TAB_A.fullVisitorId IN (
        SELECT * FROM GET_CUS_ID
    )
    AND product.v2ProductName <> ""YouTube Men's Vintage Henley""
    AND product.productRevenue IS NOT NULL
GROUP BY
    product.v2ProductName
ORDER BY
    SUM(product.productQuantity) DESC
LIMIT 1;","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
2,bq009,ga360,"Which traffic source has the highest total transaction revenue for the year 2017, and what is the difference in millions (rounded to two decimal places) between the highest and lowest monthly total transaction revenue for that traffic source?","WITH MONTHLY_REVENUE AS (
    SELECT 
        FORMAT_DATE(""%Y%m"", PARSE_DATE(""%Y%m%d"", date)) AS month,
        trafficSource.source AS source,
        ROUND(SUM(totals.totalTransactionRevenue) / 1000000, 2) AS revenue
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`
    GROUP BY 1, 2
),

YEARLY_REVENUE AS (
    SELECT
        source,
        SUM(revenue) AS total_revenue
    FROM MONTHLY_REVENUE
    GROUP BY source
),

TOP_SOURCE AS (
    SELECT 
        source
    FROM YEARLY_REVENUE
    ORDER BY total_revenue DESC
    LIMIT 1
),

SOURCE_MONTHLY_REVENUE AS (
    SELECT
        month,
        source,
        revenue
    FROM MONTHLY_REVENUE
    WHERE source IN (SELECT source FROM TOP_SOURCE)
),

REVENUE_DIFF AS (
    SELECT 
        source,
        ROUND(MAX(revenue), 2) AS max_revenue,
        ROUND(MIN(revenue), 2) AS min_revenue,
        ROUND(MAX(revenue) - MIN(revenue), 2) AS diff_revenue
    FROM SOURCE_MONTHLY_REVENUE
    GROUP BY source
)

SELECT 
    source,
    diff_revenue
FROM REVENUE_DIFF;
","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
3,bq001,ga360,"For each visitor who made at least one transaction in February 2017, how many days elapsed between the date of their first visit in February and the date of their first transaction in February, and on what type of device did they make that first transaction?","DECLARE start_date STRING DEFAULT '20170201';
DECLARE end_date STRING DEFAULT '20170228';

WITH visit AS (
    SELECT
        fullvisitorid,
        MIN(date) AS date_first_visit
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_*`
    WHERE
       _TABLE_SUFFIX BETWEEN start_date AND end_date
    GROUP BY fullvisitorid
),

transactions AS (
    SELECT
        fullvisitorid,
        MIN(date) AS date_transactions
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga,
        UNNEST(ga.hits) AS hits
    WHERE
        hits.transaction.transactionId IS NOT NULL
        AND
        _TABLE_SUFFIX BETWEEN start_date AND end_date
    GROUP BY fullvisitorid
),

device_transactions AS (
    SELECT DISTINCT
        fullvisitorid,
        date,
        device.deviceCategory
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga,
        UNNEST(ga.hits) AS hits
    WHERE
        hits.transaction.transactionId IS NOT NULL
        AND
        _TABLE_SUFFIX BETWEEN start_date AND end_date
),

visits_transactions AS (
    SELECT
        visit.fullvisitorid,
        date_first_visit,
        date_transactions,
        device_transactions.deviceCategory AS device_transaction
    FROM
        visit
        JOIN transactions
        ON visit.fullvisitorid = transactions.fullvisitorid
        JOIN device_transactions
        ON visit.fullvisitorid = device_transactions.fullvisitorid 
        AND transactions.date_transactions = device_transactions.date
)

SELECT
       fullvisitorid,
       DATE_DIFF(PARSE_DATE('%Y%m%d', date_transactions), PARSE_DATE('%Y%m%d', date_first_visit), DAY) AS time,
       device_transaction
FROM visits_transactions
ORDER BY fullvisitorid;","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
4,bq002,ga360,"During the first half of 2017,  focusing on hits product revenue, which traffic source generated the highest total product revenue, and what were the maximum daily, weekly, and monthly product revenues (in millions) for that top-performing source over this period?","DECLARE start_date STRING DEFAULT '20170101';
DECLARE end_date STRING DEFAULT '20170630';

WITH daily_revenue AS (
    SELECT
        trafficSource.source AS source,
        date,
        SUM(productRevenue) / 1000000 AS revenue
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_*`,
        UNNEST (hits) AS hits,
        UNNEST (hits.product) AS product
    WHERE
        _table_suffix BETWEEN start_date AND end_date
    GROUP BY
        source, date
),
weekly_revenue AS (
    SELECT
        source,
        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), 'W', EXTRACT(WEEK FROM (PARSE_DATE('%Y%m%d', date)))) AS week,
        SUM(revenue) AS revenue
    FROM daily_revenue
    GROUP BY source, week
),
monthly_revenue AS (
    SELECT
        source,
        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0', EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
        SUM(revenue) AS revenue
    FROM daily_revenue
    GROUP BY source, month
),

top_source AS (
    SELECT source, SUM(revenue) AS total_revenue
    FROM daily_revenue
    GROUP BY source
    ORDER BY total_revenue DESC
    LIMIT 1
),

max_revenues AS (
    (
      SELECT
        'Daily' AS time_type,
        date AS time,
        source,
        MAX(revenue) AS max_revenue
      FROM daily_revenue
      WHERE source = (SELECT source FROM top_source)
      GROUP BY source, date
      ORDER BY max_revenue DESC
      LIMIT 1
    )

    UNION ALL

    (
      SELECT
        'Weekly' AS time_type,
        week AS time,
        source,
        MAX(revenue) AS max_revenue
      FROM weekly_revenue
      WHERE source = (SELECT source FROM top_source)
      GROUP BY source, week
      ORDER BY max_revenue DESC
      LIMIT 1
    )

    UNION ALL

    (
      SELECT
          'Monthly' AS time_type,
          month AS time,
          source,
          MAX(revenue) AS max_revenue
      FROM monthly_revenue
      WHERE source = (SELECT source FROM top_source)
      GROUP BY source, month
      ORDER BY max_revenue DESC
      LIMIT 1
    )
)

SELECT
    max_revenue
FROM max_revenues
ORDER BY max_revenue DESC;
","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
5,bq003,ga360,"Between April 1 and July 31 of 2017, using the hits product revenue data along with the totals transactions to classify sessions as purchase (transactions ≥ 1 and productRevenue not null) or non-purchase (transactions null and productRevenue null), compare the average pageviews per visitor for each group by month","WITH cte1 AS (
    SELECT
        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), '0',
            EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
        SUM(totals.pageviews) / COUNT(DISTINCT fullVisitorId) AS avg_pageviews_non_purchase
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
        UNNEST (hits) AS hits,
        UNNEST (hits.product) AS product
    WHERE
        _table_suffix BETWEEN '0401' AND '0731'
        AND totals.transactions IS NULL
        AND product.productRevenue IS NULL
    GROUP BY month
),
cte2 AS (
    SELECT
        CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))), '0',
            EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
        SUM(totals.pageviews) / COUNT(DISTINCT fullVisitorId) AS avg_pageviews_purchase
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
        UNNEST (hits) AS hits,
        UNNEST (hits.product) AS product
    WHERE
        _table_suffix BETWEEN '0401' AND '0731'
        AND totals.transactions >= 1
        AND product.productRevenue IS NOT NULL
    GROUP BY month
)
SELECT
    month, avg_pageviews_purchase, avg_pageviews_non_purchase
FROM cte1 INNER JOIN cte2
USING(month)
ORDER BY month;","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
6,bq004,ga360,"In July 2017, among all visitors who bought any YouTube-related product, which distinct product—excluding those containing ‘YouTube’ in the product name—had the highest total quantity purchased?","with product_and_quatity AS (
    SELECT 
        DISTINCT v2ProductName AS other_purchased_products,
        SUM(productQuantity) AS quatity
    FROM
        `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
        UNNEST(hits) AS hits,
        UNNEST(hits.product) AS product
    WHERE
        _table_suffix BETWEEN '0701' AND '0731'
        AND NOT REGEXP_CONTAINS(LOWER(v2ProductName), 'youtube')
        AND fullVisitorID IN (
            SELECT 
            DISTINCT fullVisitorId
            FROM
                `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
                UNNEST(hits) AS hits,
                UNNEST(hits.product) AS product
            WHERE
                _table_suffix BETWEEN '0701' AND '0731'
                AND REGEXP_CONTAINS(LOWER(v2ProductName), 'youtube')
        )
    GROUP BY v2ProductName
)
SELECT other_purchased_products
FROM product_and_quatity
ORDER BY quatity DESC
LIMIT 1;","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
7,bq008,ga360,"In January 2017, among visitors whose campaign name contains 'Data Share' and who accessed any page starting with '/home', which page did they most commonly visit next, and what is the maximum time (in seconds) they spent on the '/home' page before moving on?","with page_visit_sequence AS (
    SELECT
        fullVisitorID,
        visitID,
        pagePath,
        LEAD(timestamp, 1) OVER (PARTITION BY fullVisitorId, visitID order by timestamp) - timestamp AS page_duration,
        LEAD(pagePath, 1) OVER (PARTITION BY fullVisitorId, visitID order by timestamp) AS next_page,
        RANK() OVER (PARTITION BY fullVisitorId, visitID order by timestamp) AS step_number
    FROM (
        SELECT
            pages.fullVisitorID,
            pages.visitID,
            pages.pagePath,
            visitors.campaign,
            MIN(pages.timestamp) timestamp
        FROM (
            SELECT
                fullVisitorId,
                visitId,
                trafficSource.campaign campaign
            FROM
                `bigquery-public-data.google_analytics_sample.ga_sessions_*`,
                UNNEST(hits) as hits
            WHERE
                _TABLE_SUFFIX BETWEEN '20170101' AND '20170131'
                AND hits.type='PAGE'
                AND REGEXP_CONTAINS(hits.page.pagePath, r'^/home')
                AND REGEXP_CONTAINS(trafficSource.campaign, r'Data Share')
        ) AS visitors
        JOIN(
            SELECT
                fullVisitorId,
                visitId,
                visitStartTime + hits.time / 1000 AS timestamp,
                hits.page.pagePath AS pagePath
            FROM
                `bigquery-public-data.google_analytics_sample.ga_sessions_*`,
                UNNEST(hits) as hits
            WHERE
                _TABLE_SUFFIX BETWEEN '20170101' AND '20170131'
        ) as pages
        ON
            visitors.fullVisitorID = pages.fullVisitorID
            AND visitors.visitID = pages.visitID
        GROUP BY 
            pages.fullVisitorID, visitors.campaign, pages.visitID, pages.pagePath
        ORDER BY 
            pages.fullVisitorID, pages.visitID, timestamp
    )
    ORDER BY fullVisitorId, visitID, step_number
),
most_common_next_page AS (
    SELECT
        next_page,
        COUNT(next_page) as page_count
    FROM page_visit_sequence
    WHERE
        next_page IS NOT NULL
        AND REGEXP_CONTAINS(pagePath, r'^/home')
    GROUP BY next_page
    ORDER BY page_count DESC
    LIMIT 1
),
max_page_duration AS (
    SELECT MAX(page_duration) as max_duration
    FROM page_visit_sequence
    WHERE
        page_duration IS NOT NULL
        AND REGEXP_CONTAINS(pagePath, r'^/home')
)
SELECT
    next_page,
    max_duration
FROM
    most_common_next_page,
    max_page_duration;
","## Documents about Google Analytics Sample - ga_sessions

This article explains the format and schema of the data that is imported into BigQuery.

## Datasets

For each Analytics view that is enabled for BigQuery integration, a dataset is added using the view ID as the name.

## Tables

Within each dataset, a table is imported for each day of export. Daily tables have the format ""ga\_sessions\_YYYYMMDD"".

Intraday data is imported at least three times a day. Intraday tables have the format ""ga\_sessions\_intraday\_YYYYMMDD"". During the same day, each import of intraday data overwrites the previous import in the same table.

When the daily import is complete, the intraday table from the previous day is deleted. For the current day, until the first intraday import, there is no intraday table. If an intraday-table write fails, then the previous day's intraday table is preserved.

Data for the current day is not final until the daily import is complete. You may notice differences between intraday and daily data based on active user sessions that cross the time boundary of last intraday import.

## Rows

Each row within a table corresponds to a session in Analytics 360.

## Columns

The columns within the export are listed below. In BigQuery, some columns may have nested fields and messages within them.

| Field Name | Data Type | Description |
| --- | --- | --- |
| clientId | STRING | Unhashed version of the Client ID for a given user associated with any given visit/session. |
| fullVisitorId | STRING | The unique visitor ID. |
| visitorId | NULL | This field is deprecated. Use ""fullVisitorId"" instead. |
| userId | STRING | Overridden User ID sent to Analytics. |
| visitNumber | INTEGER | The session number for this user. If this is the first session, then this is set to 1. |
| visitId | INTEGER | An identifier for this session. This is part of the value usually stored as the _utmb cookie. This is only unique to the user. For a completely unique ID, you should use a combination of fullVisitorId and visitId. |
| visitStartTime | INTEGER | The timestamp (expressed as POSIX time). |
| date | STRING | The date of the session in YYYYMMDD format. |
| totals | RECORD | This section contains aggregate values across the session. |
| totals.bounces | INTEGER | Total bounces (for convenience). For a bounced session, the value is 1, otherwise it is null. |
| totals.hits | INTEGER | Total number of hits within the session. |
| totals.newVisits | INTEGER | Total number of new users in session (for convenience). If this is the first visit, this value is 1, otherwise it is null. |
| totals.pageviews | INTEGER | Total number of pageviews within the session. |
| totals.screenviews | INTEGER | Total number of screenviews within the session. |
| totals.sessionQualityDim | INTEGER | An estimate of how close a particular session was to transacting, ranging from 1 to 100, calculated for each session. A value closer to 1 indicates a low session quality, or far from transacting, while a value closer to 100 indicates a high session quality, or very close to transacting. A value of 0 indicates that Session Quality is not calculated for the selected time range. |
| totals.timeOnScreen | INTEGER | The total time on screen in seconds. |
| totals.timeOnSite | INTEGER | Total time of the session expressed in seconds. |
| totals.totalTransactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| totals.transactionRevenue | INTEGER | This field is deprecated. Use ""totals.totalTransactionRevenue"" instead (see above). |
| totals.transactions | INTEGER | Total number of ecommerce transactions within the session. |
| totals.UniqueScreenViews | INTEGER | Total number of unique screenviews within the session. |
| totals.visits | INTEGER | The number of sessions (for convenience). This value is 1 for sessions with interaction events. The value is null if there are no interaction events in the session. |
| trafficSource | RECORD | This section contains information about the Traffic Source from which the session originated. |
| trafficSource.adContent | STRING | The ad content of the traffic source. Can be set by the utm_content URL parameter. |
| trafficSource.adwordsClickInfo | RECORD | This section contains information about the Google Ads click info if there is any associated with this session. Analytics uses the last non-direct click model. |
| trafficSource.<br>      adwordsClickInfo.adGroupId | INTEGER | The Google ad-group ID. |
| trafficSource.<br>      adwordsClickInfo.adNetworkType | STRING | Network Type. Takes one of the following values: {“Google Search"", ""Content"", ""Search partners"", ""Ad Exchange"", ""Yahoo Japan Search"", ""Yahoo Japan AFS"", “unknown”} |
| trafficSource.<br>      adwordsClickInfo.campaignId | INTEGER | The Google Ads campaign ID. |
| trafficSource.<br>      adwordsClickInfo.creativeId | INTEGER | The Google ad ID. |
| trafficSource.<br>      adwordsClickInfo.criteriaId | INTEGER | The ID for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.criteriaParameters | STRING | Descriptive string for the targeting criterion. |
| trafficSource.<br>      adwordsClickInfo.customerId | INTEGER | The Google Ads Customer ID. |
| trafficSource.<br>      adwordsClickInfo.gclId | STRING | The Google Click ID. |
| trafficSource.<br>      adwordsClickInfo.isVideoAd | BOOLEAN | True if it is a Trueview video ad. |
| trafficSource.<br>      adwordsClickInfo.page | INTEGER | Page number in search results where the ad was shown. |
| trafficSource.<br>      adwordsClickInfo.slot | STRING | Position of the Ad. Takes one of the following values:{“RHS"", ""Top""} |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria | RECORD | Google Ads targeting criteria for a click. There are multiple types of targeting criteria, but should have only one value for each criterion. |
| trafficSource.<br>      adwordsClickInfo.targetingCriteria.<br>      boomUserlistId | INTEGER | Remarketing list ID (if any) in Google Ads, derived from matching_criteria in click record. |
| trafficSource.campaign | STRING | The campaign value. Usually set by the utm_campaign URL parameter. |
| trafficSource.campaignCode | STRING | Value of the utm_id campaign tracking parameter, used for manual campaign tracking. |
| trafficSource.isTrueDirect | BOOLEAN | True if the source of the session was Direct (meaning the user typed the name of your website URL into the browser or came to your site via a bookmark), This field will also be true if 2 successive but distinct sessions have exactly the same campaign details. Otherwise NULL. |
| trafficSource.keyword | STRING | The keyword of the traffic source, usually set when the trafficSource.medium is ""organic"" or ""cpc"". Can be set by the utm_term URL parameter. |
| trafficSource.medium | STRING | The medium of the traffic source. Could be ""organic"", ""cpc"", ""referral"", or the value of the utm_medium URL parameter. |
| trafficSource.referralPath | STRING | If trafficSource.medium is ""referral"", then this is set to the path of the referrer. (The host name of the referrer is in trafficSource.source.) |
| trafficSource.source | STRING | The source of the traffic source. Could be the name of the search engine, the referring hostname, or a value of the utm_source URL parameter. |
| socialEngagementType | STRING | Engagement type, either ""Socially Engaged"" or ""Not Socially Engaged"". |
| channelGrouping | STRING | The Default Channel Group associated with an end user's session for this View. |
| device | RECORD | This section contains information about the user devices. |
| device.browser | STRING | The browser used (e.g., ""Chrome"" or ""Firefox""). |
| device.browserSize | STRING | The viewport size of users' browsers. This captures the initial dimensions of the viewport in pixels and is formatted as width x height, for example, 1920x960. |
| device.browserVersion | STRING | The version of the browser used. |
| device.deviceCategory | STRING | The type of device (Mobile, Tablet, Desktop). |
| device.mobileDeviceInfo | STRING | The branding, model, and marketing name used to identify the mobile device. |
| device.mobileDeviceMarketingName | STRING | The marketing name used for the mobile device. |
| device.mobileDeviceModel | STRING | The mobile device model. |
| device.mobileInputSelector | STRING | Selector (e.g., touchscreen, joystick, clickwheel, stylus) used on the mobile device. |
| device.operatingSystem | STRING | The operating system of the device (e.g., ""Macintosh"" or ""Windows""). |
| device.operatingSystemVersion | STRING | The version of the operating system. |
| device.isMobile<br><br>      This field is deprecated. Use device.deviceCategory instead. | BOOLEAN | If the user is on a mobile device, this value is true, otherwise false. |
| device.mobileDeviceBranding | STRING | The brand or manufacturer of the device. |
| device.flashVersion | STRING | The version of the Adobe Flash plugin that is installed on the browser. |
| device.javaEnabled | BOOLEAN | Whether or not Java is enabled in the browser. |
| device.language | STRING | The language the device is set to use. Expressed as the IETF language code. |
| device.screenColors | STRING | Number of colors supported by the display, expressed as the bit-depth (e.g., ""8-bit"", ""24-bit"", etc.). |
| device.screenResolution | STRING | The resolution of the device's screen, expressed in pixel width x height (e.g., ""800x600""). |
| customDimensions | RECORD | This section contains any user-level or session-level custom dimensions that are set for a session. This is a repeated field and has an entry for each dimension that is set. |
| customDimensions.index | INTEGER | The index of the custom dimension. |
| customDimensions.value | STRING | The value of the custom dimension. |
| geoNetwork | RECORD | This section contains information about the geography of the user. |
| geoNetwork.continent | STRING | The continent from which sessions originated, based on IP address. |
| geoNetwork.subContinent | STRING | The sub-continent from which sessions originated, based on IP address of the visitor. |
| geoNetwork.country | STRING | The country from which sessions originated, based on IP address. |
| geoNetwork.region | STRING | The region from which sessions originate, derived from IP addresses. In the U.S., a region is a state, such as New York. |
| geoNetwork.metro | STRING | The Designated Market Area (DMA) from which sessions originate. |
| geoNetwork.city | STRING | Users' city, derived from their IP addresses or Geographical IDs. |
| geoNetwork.cityId | STRING | Users' city ID, derived from their IP addresses or Geographical IDs. |
| geoNetwork.latitude | STRING | The approximate latitude of users' city, derived from their IP addresses or Geographical IDs. Locations north of the equator have positive latitudes and locations south of the equator have negative latitudes. |
| geoNetwork.longitude | STRING | The approximate longitude of users' city, derived from their IP addresses or Geographical IDs. Locations east of the prime meridian have positive longitudes and locations west of the prime meridian have negative longitudes. |
| geoNetwork.networkDomain | STRING | [No longer supported]<br>The domain name of user's ISP, derived from the domain name registered to the ISP's IP address. |
| geoNetwork.networkLocation | STRING | [No longer supported]<br>The names of the service providers used to reach the property. For example, if most users of the website come via the major cable internet service providers, its value will be these service providers' names. |
| hits | RECORD | This row and nested fields are populated for any and all types of hits. |
| hits.dataSource | STRING | The data source of a hit. By default, hits sent from analytics.js are reported as ""web"" and hits sent from the mobile SDKs are reported as ""app"". |
| hits.sourcePropertyInfo | RECORD | This section contains information about source property for rollup properties |
| hits.sourcePropertyInfo.<br>      sourcePropertyDisplayName | STRING | Source property display name of Roll-Up Properties. This is valid for only Roll-Up Properties. |
| hits.sourcePropertyInfo.<br>      sourcePropertyTrackingId | STRING | Source property tracking ID of roll-up properties. This is valid for only roll-up properties. |
| hits.eCommerceAction | RECORD | This section contains all of the ecommerce hits that occurred during the session. This is a repeated field and has an entry for each hit that was collected. |
| hits.eCommerceAction.action_type | STRING | The action type. Click through of product lists = 1, Product detail views = 2, Add product(s) to cart = 3, Remove product(s) from cart = 4, Check out = 5, Completed purchase = 6, Refund of purchase = 7, Checkout options = 8, Unknown = 0.<br>Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view""). |
| hits.eCommerceAction.option | STRING | This field is populated when a checkout option is specified. For example, a shipping option such as option = 'Fedex'. |
| hits.eCommerceAction.step | INTEGER | This field is populated when a checkout step is specified with the hit. |
| hits.exceptionInfo.exceptions | INTEGER | The number of exceptions sent to Google Analytics. |
| hits.exceptionInfo.fatalExceptions | INTEGER | The number of exceptions sent to Google Analytics where isFatal is set to true. |
| hits.experiment | RECORD | This row and the nested fields are populated for each hit that contains data for an experiment. |
| hits.experiment.experimentId | STRING | The ID of the experiment. |
| hits.experiment.experimentVariant | STRING | The variation or combination of variations present in a hit for an experiment. |
| hits.hitNumber | INTEGER | The sequenced hit number. For the first hit of each session, this is set to 1. |
| hits.hour | INTEGER | The hour in which the hit occurred (0 to 23). |
| hits.isSecure | BOOLEAN | This field is deprecated. |
| hits.isEntrance | BOOLEAN | If this hit was the first pageview or screenview hit of a session, this is set to true. |
| hits.isExit | BOOLEAN | If this hit was the last pageview or screenview hit of a session, this is set to true.<br>There is no comparable field for a Google Analytics 4 property. |
| hits.isInteraction | BOOLEAN | If this hit was an interaction, this is set to true. If this was a non-interaction hit (i.e., an event with interaction set to false), this is false. |
| hits.latencyTracking | RECORD | This section contains information about events in the Navigation Timing API. |
| hits.latencyTracking.domainLookupTime | INTEGER | The total time (in milliseconds) all samples spent in DNS lookup for this page. |
| hits.latencyTracking.domContentLoadedTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document and execute deferred and parser-inserted scripts (DOMContentLoaded). |
| hits.latencyTracking.domInteractiveTime | INTEGER | The time (in milliseconds), including the network time from users' locations to the site's server, the browser takes to parse the document (DOMInteractive). |
| hits.latencyTracking.domLatencyMetricsSample | INTEGER | Sample set (or count) of pageviews used to calculate the averages for site speed DOM metrics. |
| hits.latencyTracking.pageDownloadTime | INTEGER | The total time (in milliseconds) to download this page among all samples. |
| hits.latencyTracking.pageLoadSample | INTEGER | The sample set (or count) of pageviews used to calculate the average page load time. |
| hits.latencyTracking.pageLoadTime | INTEGER | Total time (in milliseconds), from pageview initiation (e.g., a click on a page link) to page load completion in the browser, the pages in the sample set take to load. |
| hits.latencyTracking.redirectionTime | INTEGER | The total time (in milliseconds) all samples spent in redirects before fetching this page. If there are no redirects, this is 0. |
| hits.latencyTracking.serverConnectionTime | INTEGER | Total time (in milliseconds) all samples spent in establishing a TCP connection to this page. |
| hits.latencyTracking.serverResponseTime | INTEGER | The total time (in milliseconds) the site's server takes to respond to users' requests among all samples; this includes the network time from users' locations to the server. |
| hits.latencyTracking.speedMetricsSample | INTEGER | The sample set (or count) of pageviews used to calculate the averages of site speed metrics. |
| hits.latencyTracking.userTimingCategory | STRING | For easier reporting purposes, this is used to categorize all user timing variables into logical groups. |
| hits.latencyTracking.userTimingLabel | STRING | The name of the resource's action being tracked. |
| hits.latencyTracking.userTimingSample | INTEGER | The number of hits sent for a particular userTimingCategory, userTimingLabel, or userTimingVariable. |
| hits.latencyTracking.userTimingValue | INTEGER | Total number of milliseconds for user timing. |
| hits.latencyTracking.userTimingVariable | STRING | Variable used to add flexibility to visualize user timings in the reports. |
| hits.minute | INTEGER | The minute in which the hit occurred (0 to 59). |
| hits.product.isImpression | BOOLEAN | TRUE if at least one user viewed this product (i.e., at least one impression) when it appeared in the product list. |
| hits.product.isClick | BOOLEAN | Whether users clicked this product when it appeared in the product list. |
| hits.product.customDimensions | RECORD | This section is populated for all hits containing product scope Custom Dimensions. |
| hits.product.customDimensions.index | INTEGER | The product scope Custom Dimensions index. |
| hits.product.customDimensions.value | STRING | The product scope Custom Dimensions value. |
| hits.product.customMetrics | RECORD | This section is populated for all hits containing product scope Custom Metrics. |
| hits.product.customMetrics.index | INTEGER | The product scope Custom Metrics index. |
| hits.product.customMetrics.value | INTEGER | The product scope Custom Metrics value. |
| hits.product.productListName | STRING | Name of the list in which the product is shown, or in which a click occurred. For example, ""Home Page Promotion"", ""Also Viewed"", ""Recommended For You"", ""Search Results List"", etc. |
| hits.product.productListPosition | INTEGER | Position of the product in the list in which it is shown. |
| hits.publisher.<br>      adsenseBackfillDfpClicks | INTEGER | The number of clicks on AdSense ads that served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpImpressions | INTEGER | The number of AdSense ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adsenseBackfillDfpMatchedQueries | INTEGER | The number of ad requests where AdSense was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adsenseBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by AdSense that viewability measurements were able to take into account (includes both in-view and not-in-view ads). |
| hits.publisheradsenseBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded AdSense revenue. |
| hits.publisher.adsenseBackfillDfpQueries | INTEGER | The number of ad requests made to AdSense by Google Ad Manager. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant AdSense ad clicks. |
| hits.publisher.<br>      adsenseBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served AdSense ad impressions. |
| hits.publisher.<br>      adsenseBackfillDfpViewableImpressions | INTEGER | The number of AdSense impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.adxBackfillDfpClicks | INTEGER | The number of clicks on Google Ad Manager ads served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpImpressions | INTEGER | The number of Google Ad Manager ad impressions that were served as Google Ad Manager backfill. |
| hits.publisher.<br>      adxBackfillDfpMatchedQueries | INTEGER | The number of ad requests where Google Ad Manager was trafficked as backfill and returned an ad creative to the page. |
| hits.publisher.<br>      adxBackfillDfpMeasurableImpressions | INTEGER | The number of ad impressions filled by Google Ad Manager that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.<br>      adxBackfillDfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.<br>      adxBackfillDfpQueries | INTEGER | The number of ad requests made to Google Ad Manager by Google Ad Manager. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant Google Ad Manager ad clicks. |
| hits.publisher.<br>      adxBackfillDfpRevenueCpm | INTEGER | The CPM revenue associated with the served Google Ad Manager ad impressions. |
| hits.publisher.<br>      adxBackfillDfpViewableImpressions | INTEGER | The number of Google Ad Manager impressions that met Google Ad Manager’s viewability standard. |
| hits.publisher.dfpAdGroup | STRING | The Google Ad Manager Line Item ID of the ad that served. |
| hits.publisher.dfpAdUnits | STRING | The IDs of the Google Ad Manager Ad Units present in the ad request. |
| hits.publisher.dfpClicks | INTEGER | The number of times Google Ad Manager ads were clicked. |
| hits.publisher.dfpImpressions | INTEGER | A Google Ad Manager ad impression is reported whenever an individual ad is displayed. For example, when a page with two ad units is viewed once, we display two impressions. |
| hits.publisher.dfpMatchedQueries | INTEGER | The number of ad requests where a creative was returned to the page. |
| hits.publisher.dfpMeasurableImpressions | INTEGER | The number of ad impressions that viewability measurements are able to take into account (includes both in-view and not-in-view ads). |
| hits.publisher.dfpNetworkId | STRING | The Google Ad Manager network ID that the ad request was sent to. |
| hits.publisher.dfpPagesViewed | INTEGER | The number of Google Analytics pageviews where Google Ad Manager recorded revenue. |
| hits.publisher.dfpQueries | INTEGER | The number of ad requests made to Google Ad Manager. |
| hits.publisher.dfpRevenueCpc | INTEGER | The CPC revenue associated with the resultant ad clicks, based on the rate-field value for each clicked ad in Google Ad Manager. |
| hits.publisher.dfpRevenueCpm | INTEGER | The CPM revenue associated with the served ad impressions, based on the rate-field value for each served ad in Google Ad Manager. |
| hits.publisher.dfpViewableImpressions | INTEGER | The number of impressions that met Google Ad Manager’s viewability standard. |
| hits.time | INTEGER | The number of milliseconds after the visitStartTime when this hit was registered. The first hit has a hits.time of 0 |
| hits.transaction.transactionCoupon | STRING | The coupon code associated with the transaction. |
| hits.referrer | STRING | The referring page, if the session has a goal completion or transaction. If this page is from the same domain, this is blank. |
| hits.refund | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce REFUND information. |
| hits.refund.localRefundAmount | INTEGER | Refund amount in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.refund.refundAmount | INTEGER | Refund amount, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.social | RECORD | This section is populated for each hit with type = ""SOCIAL"". |
| hits.social.hasSocialSourceReferral | STRING | A string, either Yes or No, that indicates whether sessions to the property are from a social source. |
| hits.social.socialInteractionAction | STRING | The social action passed with the social tracking code (Share, Tweet, etc.). |
| hits.social.socialInteractionNetwork | STRING | The the network passed with the social tracking code, e.g., Twitter. |
| hits.social.socialInteractionNetworkAction | STRING | For social interactions, this represents the social network being tracked. |
| hits.social.socialInteractions | INTEGER | The total number of social interactions. |
| hits.social.socialInteractionTarget | STRING | For social interactions, this is the URL (or resource) which receives the social network action. |
| hits.social.socialNetwork | STRING | The social network name. This is related to the referring social network for traffic sources; e.g., Blogger. |
| hits.social.uniqueSocialInteractions | INTEGER | The number of sessions during which the specified social action(s) occurred at least once. This is based on the the unique combination of socialInteractionNetwork, socialInteractionAction, and socialInteractionTarget. |
| hits.type | STRING | The type of hit. One of: ""PAGE"", ""TRANSACTION"", ""ITEM"", ""EVENT"", ""SOCIAL"", ""APPVIEW"", ""EXCEPTION"".<br>Timing hits are considered an event type in the Analytics backend. When you query time-related fields (e.g., hits.latencyTracking.pageLoadTime), choose hits.type as Event if you want to use hit.type in your queries. |
| hits.page | RECORD | This section is populated for each hit with type = ""PAGE"". |
| hits.page.pagePath | STRING | The URL path of the page. |
| hits.page.pagePathLevel1 | STRING | This dimension rolls up all the page paths in the 1st hierarchical level in pagePath. |
| hits.page.pagePathLevel2 | STRING | This dimension rolls up all the page paths in the 2nd hierarchical level in pagePath. |
| hits.page.pagePathLevel3 | STRING | This dimension rolls up all the page paths in the 3d hierarchical level in pagePath. |
| hits.page.pagePathLevel4 | STRING | This dimension rolls up all the page paths into hierarchical levels. Up to 4 pagePath levels may be specified. All additional levels in the pagePath hierarchy are also rolled up in this dimension. |
| hits.page.hostname | STRING | The hostname of the URL. |
| hits.page.pageTitle | STRING | The page title. |
| hits.page.searchKeyword | STRING | If this was a search results page, this is the keyword entered. |
| hits.product | RECORD | This row and nested fields will be populated for each hit that contains Enhanced Ecommerce PRODUCT data. |
| hits.product.localProductPrice | INTEGER | The price of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRefundAmount | INTEGER | The amount processed as part of a refund for a product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.localProductRevenue | INTEGER | The revenue of the product in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productBrand | STRING | The brand associated with the product. |
| hits.product.productPrice | INTEGER | The price of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productQuantity | INTEGER | The quantity of the product purchased. |
| hits.product.productRefundAmount | INTEGER | The amount processed as part of a refund for a product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productRevenue | INTEGER | The revenue of the product, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.product.productSKU | STRING | Product SKU. |
| hits.product.productVariant | STRING | Product Variant. |
| hits.product.v2ProductCategory | STRING | Product Category. |
| hits.product.v2ProductName | STRING | Product Name. |
| hits.promotion | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION information. |
| hits.promotion.promoCreative | STRING | The text or creative variation associated with the promotion. |
| hits.promotion.promoId | STRING | Promotion ID. |
| hits.promotion.promoName | STRING | Promotion Name. |
| hits.promotion.promoPosition | STRING | Promotion position on site. |
| hits.promotionActionInfo | RECORD | This row and nested fields are populated for each hit that contains Enhanced Ecommerce PROMOTION action information. |
| hits.promotionActionInfo.promoIsView | BOOLEAN | True if the Enhanced Ecommerce action is a promo view. |
| hits.promotionActionInfo.promoIsClick | BOOLEAN | True if the Enhanced Ecommerce action is a promo click. |
| hits.page.searchCategory | STRING | If this was a search-results page, this is the category selected. |
| hits.transaction | RECORD | This section is populated for each hit with type = ""TRANSACTION"". |
| hits.transaction.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.transaction.transactionRevenue | INTEGER | Total transaction revenue, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionTax | INTEGER | Total transaction tax, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.transactionShipping | INTEGER | Total transaction shipping cost, expressed as the value passed to Analytics multiplied by 10^6. (e.g., 2.40 would be given as 2400000). |
| hits.transaction.affiliation | STRING | The affiliate information passed to the ecommerce tracking code. |
| hits.transaction.currencyCode | STRING | The local currency code for the transaction. |
| hits.transaction.localTransactionRevenue | INTEGER | Total transaction revenue in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionTax | INTEGER | Total transaction tax in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.transaction.localTransactionShipping | INTEGER | Total transaction shipping cost in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item | RECORD | This section will be populated for each hit with type = ""ITEM"". |
| hits.item.transactionId | STRING | The transaction ID of the ecommerce transaction. |
| hits.item.productName | STRING | The name of the product. |
| hits.item.productCategory | STRING | The category of the product. |
| hits.item.productSku | STRING | The SKU or product ID. |
| hits.item.itemQuantity | INTEGER | The quantity of the product sold. |
| hits.item.itemRevenue | INTEGER | Total revenue from the item, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.item.currencyCode | STRING | The local currency code for the transaction. |
| hits.item.localItemRevenue | INTEGER | Total revenue from this item in local currency, expressed as the value passed to Analytics multiplied by 10^6 (e.g., 2.40 would be given as 2400000). |
| hits.contentGroup | RECORD | This section contains information about content grouping. Learn more |
| hits.contentGroup.contentGroupX | STRING | The content group on a property. A content group is a collection of content that provides a logical structure that can be determined by tracking-code or page-title/URL regex match, or predefined rules. (Index X can range from 1 to 5.) |
| hits.contentGroup.previousContentGroupX | STRING | Content group that was visited before another content group. (Index X can range from 1 to 5.) |
| hits.contentGroup.contentGroupUniqueViewsX | STRING | The number of unique content group views. Content group views in different sessions are counted as unique content group views. Both the pagePath and pageTitle are used to determine content group view uniqueness. (Index X can range from 1 to 5.) |
| hits.contentInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"". |
| hits.contentInfo.contentDescription | STRING | The description of the content being viewed as passed to the SDK. |
| hits.appInfo | RECORD | This section will be populated for each hit with type = ""APPVIEW"" or ""EXCEPTION"". |
| hits.appInfo.appInstallerId | STRING | ID of the installer (e.g., Google Play Store) from which the app was downloaded. |
| hits.appInfo.appName | STRING | The name of the application. |
| hits.appInfo.appVersion | STRING | The version of the application. |
| hits.appInfo.appId | STRING | The ID of the application. |
| hits.appInfo.screenName | STRING | The name of the screen. |
| hits.appInfo.landingScreenName | STRING | The landing screen of the session. |
| hits.appInfo.exitScreenName | STRING | The exit screen of the session. |
| hits.appInfo.screenDepth | STRING | The number of screenviews per session reported as a string. Can be useful for historgrams. |
| hits.exceptionInfo | RECORD | This section is populated for each hit with type = ""EXCEPTION"". |
| hits.exceptionInfo.description | STRING | The exception description. |
| hits.exceptionInfo.isFatal | BOOLEAN | If the exception was fatal, this is set to true. |
| hits.eventInfo | RECORD | This section is populated for each hit with type = ""EVENT"". |
| hits.eventInfo.eventCategory | STRING | The event category. |
| hits.eventInfo.eventAction | STRING | The event action. |
| hits.eventInfo.eventLabel | STRING | The event label. |
| hits.eventInfo.eventValue | INTEGER | The event value. |
| hits.customVariables | RECORD | This section contains any hit-level custom variables. This is a repeated field and has an entry for each variable that is set. |
| hits.customVariables.index | INTEGER | The index of the custom variable. |
| hits.customVariables.customVarName | STRING | The custom variable name. |
| hits.customVariables.customVarValue | STRING | The custom variable value. |
| hits.customDimensions | RECORD | This section contains any hit-level custom dimensions. This is a repeated field and has an entry for each dimension that is set. |
| hits.customDimensions.index | INTEGER | The index of the custom dimension. |
| hits.customDimensions.value | STRING | The value of the custom dimension. |
| hits.customMetrics | RECORD | This section contains any hit-level custom metrics. This is a repeated field and has an entry for each metric that is set. |
| hits.customMetrics.index | INTEGER | The index of the custom metric. |
| hits.customMetrics.value | INTEGER | The value of the custom metric. |
| privacy_info.ads_storage | STRING | Whether ad targeting is enabled for a user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.analytics_storage | STRING | Whether Analytics storage is enabled for the user.<br>Possible values: TRUE, FALSE, UNKNOWN |
| privacy_info.uses_transient_token | STRING | Whether a web user has denied Analytics storage and the developer has enabled measurement without cookies based on transient tokens in server data.<br>Possible values: TRUE, FALSE, UNKNOWN |",snowflake
8,bq269,ga360,"Between June 1, 2017, and July 31, 2017, consider only sessions that have non-null pageviews. Classify each session as ‘purchase’ if it has at least one transaction, or ‘non_purchase’ otherwise. For each month, sum each visitor’s total pageviews under each classification, then compute the average pageviews per visitor for both purchase and non-purchase groups in each month, and present the results side by side.","WITH visitor_pageviews AS (
  SELECT
    FORMAT_DATE('%Y%m', PARSE_DATE('%Y%m%d', date)) AS month,
    CASE WHEN totals.transactions > 0 THEN 'purchase' ELSE 'non_purchase' END AS purchase_status,
    fullVisitorId,
    SUM(totals.pageviews) AS total_pageviews
  FROM
    `bigquery-public-data.google_analytics_sample.ga_sessions_*`
  WHERE
    _TABLE_SUFFIX BETWEEN '20170601' AND '20170731'
    AND totals.pageviews IS NOT NULL
  GROUP BY
    month, purchase_status, fullVisitorId
),
avg_pageviews AS (
  SELECT
    month,
    purchase_status,
    AVG(total_pageviews) AS avg_pageviews_per_visitor
  FROM
    visitor_pageviews
  GROUP BY
    month, purchase_status
)
SELECT
  month,
  MAX(CASE WHEN purchase_status = 'purchase' THEN avg_pageviews_per_visitor END) AS avg_pageviews_purchase,
  MAX(CASE WHEN purchase_status = 'non_purchase' THEN avg_pageviews_per_visitor END) AS avg_pageviews_non_purchase
FROM
  avg_pageviews
GROUP BY
  month
ORDER BY
  month",,snowflake
9,bq268,ga360,"Identify the longest number of days between the first visit and the last recorded event (either the last visit or the first transaction) for a user, where the last recorded event is associated with a mobile device. The last recorded event could either be the last visit or the first transaction, and you should focus on users whose last recorded event occurred on a mobile device.","WITH 

visit AS (
SELECT fullvisitorid, MIN(date) AS date_first_visit, MAX(date) AS date_last_visit 
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` GROUP BY fullvisitorid),

device_visit AS (
SELECT DISTINCT fullvisitorid, date, device.deviceCategory
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*`),

transactions AS (
SELECT fullvisitorid, MIN(date) AS date_transactions, 1 AS transaction
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, UNNEST(ga.hits) AS hits
WHERE  hits.transaction.transactionId IS NOT NULL GROUP BY fullvisitorid),

device_transactions AS (
SELECT DISTINCT fullvisitorid, date, device.deviceCategory
FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, UNNEST(ga.hits) AS hits
WHERE hits.transaction.transactionId IS NOT NULL),

visits_transactions AS (
SELECT visit.fullvisitorid, date_first_visit, date_transactions, date_last_visit , 
       device_visit.deviceCategory AS device_last_visit, device_transactions.deviceCategory AS device_transaction, 
       IFNULL(transactions.transaction,0) AS transaction
FROM visit LEFT JOIN transactions ON visit.fullvisitorid = transactions.fullvisitorid
LEFT JOIN device_visit ON visit.fullvisitorid = device_visit.fullvisitorid 
AND visit.date_last_visit = device_visit.date

LEFT JOIN device_transactions ON visit.fullvisitorid = device_transactions.fullvisitorid 
AND transactions.date_transactions = device_transactions.date ),

mortality_table AS (
SELECT fullvisitorid, date_first_visit, 
       CASE WHEN date_transactions IS NULL THEN date_last_visit ELSE date_transactions  END AS date_event, 
       CASE WHEN device_transaction IS NULL THEN device_last_visit ELSE device_transaction END AS device, transaction
FROM visits_transactions )

SELECT DATE_DIFF(PARSE_DATE('%Y%m%d',date_event), PARSE_DATE('%Y%m%d', date_first_visit),DAY) AS time 
FROM mortality_table
WHERE device = 'mobile'
ORDER BY DATE_DIFF(PARSE_DATE('%Y%m%d',date_event), PARSE_DATE('%Y%m%d', date_first_visit),DAY) DESC
LIMIT 1",,snowflake
10,bq270,ga360,"What were the monthly add-to-cart and purchase conversion rates, calculated as a percentage of pageviews on product details, from January to March 2017?","WITH
  cte1 AS
    (SELECT
      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',
                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
      COUNT(hits.eCommerceAction.action_type) AS num_product_view
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
      UNNEST(hits) AS hits
    WHERE _table_suffix BETWEEN '0101' AND '0331'
      AND hits.eCommerceAction.action_type = '2'
    GROUP BY month),
  cte2 AS
    (SELECT
      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',
                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
      COUNT(hits.eCommerceAction.action_type) AS num_addtocart
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
      UNNEST(hits) AS hits
    WHERE _table_suffix BETWEEN '0101' AND '0331'
      AND hits.eCommerceAction.action_type = '3'
    GROUP BY month),
  cte3 AS
    (SELECT
      CONCAT(EXTRACT(YEAR FROM (PARSE_DATE('%Y%m%d', date))),'0',
                EXTRACT(MONTH FROM (PARSE_DATE('%Y%m%d', date)))) AS month,
      COUNT(hits.eCommerceAction.action_type) AS num_purchase
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_2017*`,
      UNNEST(hits) AS hits,
      UNNEST(hits.product) AS product
    WHERE _table_suffix BETWEEN '0101' AND '0331'
      AND hits.eCommerceAction.action_type = '6'
      AND product.productRevenue IS NOT NULL
    GROUP BY month)
SELECT 
  ROUND((num_addtocart/num_product_view * 100),2) AS add_to_cart_rate,
  ROUND((num_purchase/num_product_view * 100),2) AS purchase_rate
FROM cte1
  LEFT JOIN cte2
  USING(month) 
  LEFT JOIN cte3
  USING(month)
ORDER BY month;","# GA360 - eCommerce Action Type

| Action Type                    | hits.eCommerceAction.action_type Value |
| ------------------------------ | -------------------------------------- |
| Click through of product lists | 1                                      |
| Product detail views           | 2                                      |
| Add product(s) to cart         | 3                                      |
| Remove product(s) from cart    | 4                                      |
| Check out                      | 5                                      |
| Completed purchase             | 6                                      |
| Refund of purchase             | 7                                      |
| Checkout options               | 8                                      |
| Unknown                        | 0                                      |

Usually this action type applies to all the products in a hit, with the following exception: when hits.product.isImpression = TRUE, the corresponding product is a product impression that is seen while the product action is taking place (i.e., a ""product in list view"").

**Example query to calculate number of products in list views**:
SELECT
COUNT(hits.product.v2ProductName)
FROM [foo-160803:123456789.ga_sessions_20170101]
WHERE hits.product.isImpression == TRUE

**Example query to calculate number of products in detailed view**:
SELECT
COUNT(hits.product.v2ProductName),
FROM
[foo-160803:123456789.ga_sessions_20170101]
WHERE
hits.ecommerceaction.action_type = '2'
AND ( BOOLEAN(hits.product.isImpression) IS NULL OR BOOLEAN(hits.product.isImpression) == FALSE )",snowflake
11,bq275,ga360,Which visitor IDs belong to users whose first transaction occurred on a device explicitly labeled as 'mobile' on a later date than their first visit?,"WITH 
  visit AS (
    SELECT fullvisitorid, MIN(date) AS date_first_visit
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` 
    GROUP BY fullvisitorid
  ),
  
  transactions AS (
    SELECT fullvisitorid, MIN(date) AS date_transactions, 1 AS transaction
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, 
    UNNEST(ga.hits) AS hits 
    WHERE hits.transaction.transactionId IS NOT NULL 
    GROUP BY fullvisitorid
  ),

  device_transactions AS (
    SELECT DISTINCT fullvisitorid, date, device.deviceCategory AS device_transaction
    FROM `bigquery-public-data.google_analytics_sample.ga_sessions_*` AS ga, 
    UNNEST(ga.hits) AS hits 
    WHERE hits.transaction.transactionId IS NOT NULL
  ),

  visits_transactions AS (
    SELECT visit.fullvisitorid, date_first_visit, date_transactions, device_transaction
    FROM visit 
    LEFT JOIN transactions ON visit.fullvisitorid = transactions.fullvisitorid
    LEFT JOIN device_transactions ON visit.fullvisitorid = device_transactions.fullvisitorid 
    AND transactions.date_transactions = device_transactions.date
  )

SELECT fullvisitorid 
FROM visits_transactions
WHERE DATE_DIFF(PARSE_DATE('%Y%m%d', date_transactions), PARSE_DATE('%Y%m%d', date_first_visit), DAY) > 0
AND device_transaction = ""mobile"";",,snowflake
12,bq374,ga360,"Calculates the percentage of new users who, between August 1, 2016, and April 30, 2017, both stayed on the site for more than 5 minutes during their initial visit and made a purchase on a subsequent visit at any later time, relative to the total number of new users in the same period.","WITH initial_visits AS (
    SELECT
        fullVisitorId,
        MIN(visitStartTime) AS initialVisitStartTime
    FROM
        `bigquery-public-data.google_analytics_sample.*`
    WHERE
        totals.newVisits = 1
        AND date BETWEEN '20160801' AND '20170430'
    GROUP BY
        fullVisitorId
),
qualified_initial_visits AS (
  SELECT
    s.fullVisitorId,
    s.visitStartTime AS initialVisitStartTime,
    s.totals.timeOnSite AS time_on_site
  FROM
    `bigquery-public-data.google_analytics_sample.*` s
  JOIN initial_visits i
    ON s.fullVisitorId = i.fullVisitorId
    AND s.visitStartTime = i.initialVisitStartTime
  WHERE
    s.totals.timeOnSite > 300
),
filtered_data AS (
  SELECT
    q.fullVisitorId,
    q.time_on_site,
    IF(COUNTIF(s.visitStartTime > q.initialVisitStartTime AND s.totals.transactions > 0) > 0, 1, 0) AS will_buy_on_return_visit
  FROM
    qualified_initial_visits q
  LEFT JOIN `bigquery-public-data.google_analytics_sample.*` s
    ON q.fullVisitorId = s.fullVisitorId
  GROUP BY
    q.fullVisitorId, q.time_on_site
),
matching_users AS (
  SELECT
    fullVisitorId
  FROM
    filtered_data
  WHERE
    time_on_site > 300 AND will_buy_on_return_visit = 1
),
total_new_users AS (
  SELECT
    COUNT(DISTINCT fullVisitorId) AS total_new_users
  FROM
    `bigquery-public-data.google_analytics_sample.*`
  WHERE
    totals.newVisits = 1
    AND date BETWEEN '20160801' AND '20170430'
),
final_counts AS (
  SELECT
    COUNT(DISTINCT fullVisitorId) AS users_matching_criteria
  FROM
    matching_users
)
SELECT
  (final_counts.users_matching_criteria / total_new_users.total_new_users) * 100 AS percentage_matching_criteria
FROM
  final_counts,
  total_new_users;
",,snowflake
13,sf_bq029,PATENTS,"Get the average number of inventors per patent and the total count of patent publications in Canada (CA) for each 5-year period from 1960 to 2020, based on publication dates. Only include patents that have at least one inventor listed, and group results by 5-year intervals (1960-1964, 1965-1969, etc.).",,,snowflake
14,sf_bq026,PATENTS,"For the assignee who has been the most active in the patent category 'A61', I'd like to know the five patent jurisdictions code where they filed the most patents during their busiest year, separated by commas.",,,snowflake
15,sf_bq091,PATENTS,In which year did the assignee with the most applications in the patent category 'A61' file the most?,"WITH AA AS (
    SELECT 
        FIRST_VALUE(""assignee_harmonized"") OVER (PARTITION BY ""application_number"" ORDER BY ""application_number"") AS assignee_harmonized,
        FIRST_VALUE(""filing_date"") OVER (PARTITION BY ""application_number"" ORDER BY ""application_number"") AS filing_date,
        ""application_number""
    FROM 
        PATENTS.PATENTS.PUBLICATIONS AS pubs
        , LATERAL FLATTEN(input => pubs.""cpc"") AS c
    WHERE 
        c.value:""code"" LIKE 'A61%'
),

PatentApplications AS (
    SELECT 
        ANY_VALUE(assignee_harmonized) as assignee_harmonized,
        ANY_VALUE(filing_date) as filing_date
    FROM AA
    GROUP BY ""application_number""
),

AssigneeApplications AS (
SELECT 
    COUNT(*) AS total_applications,
    a.value::STRING AS assignee_name,
    CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year
FROM 
    PatentApplications
    , LATERAL FLATTEN(input => assignee_harmonized) AS a
GROUP BY 
    a.value::STRING, filing_year
),

TotalApplicationsPerAssignee AS (
    SELECT
        assignee_name,
        SUM(total_applications) AS total_applications
    FROM 
        AssigneeApplications
    GROUP BY 
        assignee_name
    ORDER BY 
        total_applications DESC
    LIMIT 1
),

MaxYearForTopAssignee AS (
    SELECT
        aa.assignee_name,
        aa.filing_year,
        aa.total_applications
    FROM 
        AssigneeApplications aa
    INNER JOIN
        TotalApplicationsPerAssignee tapa ON aa.assignee_name = tapa.assignee_name
    ORDER BY 
        aa.total_applications DESC
    LIMIT 1
)

SELECT filing_year
FROM 
    MaxYearForTopAssignee
    
",,snowflake
16,sf_bq099,PATENTS,"For patent class A01B3, I want to analyze the information of the top 3 assignees based on the total number of applications. Please provide the following five pieces of information: the name of this assignee,  total number of applications, the year with the most applications, the number of applications in that year, and the country code with the most applications during that year.","WITH PatentApplications AS (
   SELECT 
        ""assignee_harmonized"" AS assignee_harmonized,
        ""filing_date"" AS filing_date,
        ""country_code"" AS country_code,
        ""application_number"" AS application_number
    FROM 
        PATENTS.PATENTS.PUBLICATIONS AS pubs,
        LATERAL FLATTEN(input => pubs.""cpc"") AS c
    WHERE c.value:""code"" LIKE 'A01B3%'

),

AssigneeApplications AS (
    SELECT 
        COUNT(*) AS year_country_cnt,
        a.value:""name"" AS assignee_name,
        CAST(FLOOR(filing_date / 10000) AS INT) AS filing_year,
        apps.country_code as country_code
    FROM 
        PatentApplications as apps,
        LATERAL FLATTEN(input => assignee_harmonized) AS a
    GROUP BY 
        assignee_name, filing_year, country_code
),

RankedApplications AS (
    SELECT
        assignee_name,
        filing_year,
        country_code,
        year_country_cnt,
        SUM(year_country_cnt) OVER (PARTITION BY assignee_name, filing_year) AS total_cnt,
        ROW_NUMBER() OVER (PARTITION BY assignee_name, filing_year ORDER BY year_country_cnt DESC) AS rn
    FROM
        AssigneeApplications
),

AggregatedData AS (
    SELECT
        total_cnt AS year_cnt,
        assignee_name,
        filing_year,
        country_code
    FROM
        RankedApplications
    WHERE
        rn = 1
)


SELECT 
    total_count,
    REPLACE(assignee_name, '""', '') AS assignee_name,
    year_cnt,
    filing_year,
    country_code
FROM (
    SELECT 
        year_cnt,
        assignee_name,
        filing_year,
        country_code,
        SUM(year_cnt) OVER (PARTITION BY assignee_name) AS total_count,
        ROW_NUMBER() OVER (PARTITION BY assignee_name ORDER BY year_cnt DESC) AS rn
    FROM
        AggregatedData
    ORDER BY assignee_name
) sub
WHERE rn = 1
ORDER BY total_count
DESC
LIMIT 3",,snowflake
17,sf_bq033,PATENTS,"How many U.S. publications related to IoT (where the abstract includes the phrase 'internet of things') were filed each month from 2008 to 2022, including months with no filings?","WITH Patent_Matches AS (
    SELECT
      TO_DATE(CAST(ANY_VALUE(patentsdb.""filing_date"") AS STRING), 'YYYYMMDD') AS Patent_Filing_Date,
      patentsdb.""application_number"" AS Patent_Application_Number,
      MAX(abstract_info.value:""text"") AS Patent_Title,
      MAX(abstract_info.value:""language"") AS Patent_Title_Language
    FROM
      PATENTS.PATENTS.PUBLICATIONS AS patentsdb,
      LATERAL FLATTEN(input => patentsdb.""abstract_localized"") AS abstract_info
    WHERE
      LOWER(abstract_info.value:""text"") LIKE '%internet of things%'
      AND patentsdb.""country_code"" = 'US'
    GROUP BY
      Patent_Application_Number
),

Date_Series_Table AS (
    SELECT
        DATEADD(day, seq4(), DATE '2008-01-01') AS day,
        0 AS Number_of_Patents
    FROM
        TABLE(
            GENERATOR(
                ROWCOUNT => 5479
            )
        )
    ORDER BY
        day
)

SELECT
  TO_CHAR(Date_Series_Table.day, 'YYYY-MM') AS Patent_Date_YearMonth,
  COUNT(Patent_Matches.Patent_Application_Number) AS Number_of_Patent_Applications
FROM
  Date_Series_Table
  LEFT JOIN Patent_Matches
    ON Date_Series_Table.day = Patent_Matches.Patent_Filing_Date
WHERE
    Date_Series_Table.day < DATE '2023-01-01'
GROUP BY
  TO_CHAR(Date_Series_Table.day, 'YYYY-MM')
ORDER BY
  Patent_Date_YearMonth;
",,snowflake
18,sf_bq209,PATENTS,"Can you calculate the number of utility patents that were granted in 2010 and have exactly one forward citation within a 10-year window following their application/filing date? For this analysis, forward citations should be counted as distinct citing application numbers that cited the patent within 10 years after the patent's own filing date.","WITH patents_sample AS (
    SELECT
        t1.""publication_number"",
        t1.""application_number""
    FROM
        PATENTS.PATENTS.PUBLICATIONS t1
    WHERE
        TO_DATE(
            CASE
                WHEN t1.""grant_date"" != 0 THEN TO_CHAR(t1.""grant_date"")
                ELSE NULL
            END, 
            'YYYYMMDD'
        ) BETWEEN TO_DATE('20100101', 'YYYYMMDD') AND TO_DATE('20101231', 'YYYYMMDD')
),
forward_citation AS (
    SELECT
        patents_sample.""publication_number"",
        COUNT(DISTINCT t3.""citing_application_number"") AS ""forward_citations""
    FROM
        patents_sample
        LEFT JOIN (
            SELECT
                x2.""publication_number"",
                TO_DATE(
                    CASE
                        WHEN x2.""filing_date"" != 0 THEN TO_CHAR(x2.""filing_date"")
                        ELSE NULL
                    END,
                    'YYYYMMDD'
                ) AS ""filing_date""
            FROM
                PATENTS.PATENTS.PUBLICATIONS x2
            WHERE
                x2.""filing_date"" != 0
        ) t2
            ON t2.""publication_number"" = patents_sample.""publication_number""
        LEFT JOIN (
            SELECT
                x3.""publication_number"" AS ""citing_publication_number"",
                x3.""application_number"" AS ""citing_application_number"",
                TO_DATE(
                    CASE
                        WHEN x3.""filing_date"" != 0 THEN TO_CHAR(x3.""filing_date"")
                        ELSE NULL
                    END,
                    'YYYYMMDD'
                ) AS ""joined_filing_date"",
                cite.value:""publication_number""::STRING AS ""cited_publication_number""
            FROM
                PATENTS.PATENTS.PUBLICATIONS x3,
                LATERAL FLATTEN(INPUT => x3.""citation"") cite
            WHERE
                x3.""filing_date"" != 0
        ) t3
            ON patents_sample.""publication_number"" = t3.""cited_publication_number""
            AND t3.""joined_filing_date"" BETWEEN t2.""filing_date"" AND DATEADD(YEAR, 10, t2.""filing_date"")
    GROUP BY
        patents_sample.""publication_number""
)

SELECT
    COUNT(*)
FROM
    forward_citation
WHERE
    ""forward_citations"" = 1;
",,snowflake
19,sf_bq027,PATENTS,"For patents granted between 2010 and 2018, provide the publication number of each patent and the number of backward citations it has received in the SEA category.",,,snowflake
20,sf_bq210,PATENTS,How many US B2 patents granted between 2008 and 2018 contain claims that do not include the word 'claim'?,"WITH patents_sample AS (
  SELECT 
    t1.""publication_number"" AS publication_number,
    claim.value:""text"" AS claims_text
  FROM 
    PATENTS.PATENTS.PUBLICATIONS t1,
    LATERAL FLATTEN(input => t1.""claims_localized"") AS claim
  WHERE 
    t1.""country_code"" = 'US'
    AND t1.""grant_date"" BETWEEN 20080101 AND 20181231
    AND t1.""grant_date"" != 0
    AND t1.""publication_number"" LIKE '%B2%'
),
Publication_data AS (
  SELECT
    publication_number,
    COUNT_IF(claims_text NOT LIKE '%claim%') AS nb_indep_claims
  FROM
    patents_sample
  GROUP BY
    publication_number
)

SELECT COUNT(nb_indep_claims)
FROM Publication_data
WHERE nb_indep_claims != 0",,snowflake
21,sf_bq211,PATENTS,"Among patents granted between 2010 and 2023 in CN, how many of them belong to families that have a total of over one distinct applications?",,,snowflake
22,sf_bq213,PATENTS,What is the most common 4-digit IPC code among US B2 utility patents granted from June to August in 2022?,"WITH interim_table as(
SELECT 
    t1.""publication_number"", 
    SUBSTR(ipc_u.value:""code"", 0, 4) as ipc4
FROM 
    PATENTS.PATENTS.PUBLICATIONS t1,
    LATERAL FLATTEN(input => t1.""ipc"") AS ipc_u
WHERE
""country_code"" = 'US'  
AND ""grant_date"" between 20220601 AND 20220831
  AND ""grant_date"" != 0
  AND ""publication_number"" LIKE '%B2%'  
GROUP BY 
    t1.""publication_number"", 
    ipc4
) 
SELECT 
ipc4
FROM 
interim_table 
GROUP BY ipc4
ORDER BY COUNT(""publication_number"") DESC
LIMIT 1","### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
23,sf_bq212,PATENTS,"For United States utility patents under the B2 classification granted between June and September of 2022, identify the most frequent 4-digit IPC code for each patent. Then, list the publication numbers and IPC4 codes of patents where this code appears 10 or more times.",,"### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
24,sf_bq214,PATENTS_GOOGLE,"For United States utility patents under the B2 classification granted between 2010 and 2014, find the one with the most forward citations within a month of its filing date, and identify the most similar patent from the same filing year, regardless of its type.",,"### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
25,sf_bq216,PATENTS_GOOGLE,Identify the top five patents filed in the same year as `US-9741766-B2` that are most similar to it based on technological similarities. Please provide the publication numbers.,"WITH patents_sample AS (
    SELECT 
        ""publication_number"", 
        ""application_number""
    FROM
        PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS
    WHERE
        ""publication_number"" = 'US-9741766-B2'
),
flattened_t5 AS (
    SELECT
        t5.""publication_number"",
        f.value AS element_value,
        f.index AS pos
    FROM
        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t5,
        LATERAL FLATTEN(input => t5.""embedding_v1"") AS f
),
flattened_t6 AS (
    SELECT
        t6.""publication_number"",
        f.value AS element_value,
        f.index AS pos
    FROM
        PATENTS_GOOGLE.PATENTS_GOOGLE.ABS_AND_EMB t6,
        LATERAL FLATTEN(input => t6.""embedding_v1"") AS f
),
similarities AS (
    SELECT
        t1.""publication_number"" AS base_publication_number,
        t4.""publication_number"" AS similar_publication_number,
        SUM(ft5.element_value * ft6.element_value) AS similarity
    FROM
        (SELECT * FROM patents_sample LIMIT 1) t1
    LEFT JOIN (
        SELECT 
            x3.""publication_number"",
            EXTRACT(YEAR, TO_DATE(CAST(x3.""filing_date"" AS STRING), 'YYYYMMDD')) AS focal_filing_year
        FROM 
            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x3
        WHERE 
            x3.""filing_date"" != 0
    ) t3 ON t3.""publication_number"" = t1.""publication_number""
    LEFT JOIN (
        SELECT 
            x4.""publication_number"",
            EXTRACT(YEAR, TO_DATE(CAST(x4.""filing_date"" AS STRING), 'YYYYMMDD')) AS filing_year
        FROM 
            PATENTS_GOOGLE.PATENTS_GOOGLE.PUBLICATIONS x4
        WHERE 
            x4.""filing_date"" != 0
    ) t4 ON
        t4.""publication_number"" != t1.""publication_number""
        AND t3.focal_filing_year = t4.filing_year
    LEFT JOIN flattened_t5 AS ft5 ON ft5.""publication_number"" = t1.""publication_number""
    LEFT JOIN flattened_t6 AS ft6 ON ft6.""publication_number"" = t4.""publication_number""
    AND ft5.pos = ft6.pos  -- Align vector positions
    GROUP BY
        t1.""publication_number"", t4.""publication_number""
)
SELECT
    s.similar_publication_number,
    s.similarity
FROM (
    SELECT
        s.*,
        ROW_NUMBER() OVER (PARTITION BY s.base_publication_number ORDER BY s.similarity DESC) AS seqnum
    FROM
        similarities s
) s
WHERE
    seqnum <= 5;
","### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
26,sf_bq247,PATENTS_GOOGLE,"From the publications dataset, first identify the top six families with the most publications whose family_id is not '-1'. Then, using the abs_and_emb table (joined on publication_number), provide each of those families’ IDs alongside every non-empty abstract associated with their publications.",,,snowflake
27,sf_bq127,PATENTS_GOOGLE,"For each publication family whose earliest publication was first published in January 2015, please provide the earliest publication date, the distinct publication numbers, their country codes, the distinct CPC and IPC codes, distinct families (namely, the ids) that cite and are cited by this publication family. Please present all lists as comma-separated values, sorted alphabetically","WITH fam AS (
  SELECT DISTINCT
    ""family_id""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""PUBLICATIONS""
),

crossover AS (
  SELECT
    ""publication_number"",
    ""family_id""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""PUBLICATIONS""
),

pub AS (
  SELECT
    ""family_id"",
    MIN(""publication_date"") AS ""publication_date"",
    LISTAGG(""publication_number"", ',') WITHIN GROUP (ORDER BY ""publication_number"") AS ""publication_number"",
    LISTAGG(""country_code"", ',') WITHIN GROUP (ORDER BY ""country_code"") AS ""country_code""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""PUBLICATIONS"" AS p
  GROUP BY
    ""family_id""
),

tech_class AS (
  SELECT
    p.""family_id"",
    LISTAGG(DISTINCT cpc.value:""code""::STRING, ',') WITHIN GROUP (ORDER BY cpc.value:""code""::STRING) AS ""cpc"",
    LISTAGG(DISTINCT ipc.value:""code""::STRING, ',') WITHIN GROUP (ORDER BY ipc.value:""code""::STRING) AS ""ipc""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""PUBLICATIONS"" AS p
    CROSS JOIN LATERAL FLATTEN(input => p.""cpc"") AS cpc
    CROSS JOIN LATERAL FLATTEN(input => p.""ipc"") AS ipc
  GROUP BY
    p.""family_id""
),

cit AS (
  SELECT
    p.""family_id"",
    LISTAGG(crossover.""family_id"", ',') WITHIN GROUP (ORDER BY crossover.""family_id"" ASC) AS ""citation""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""PUBLICATIONS"" AS p
    CROSS JOIN LATERAL FLATTEN(input => p.""citation"") AS citation
    LEFT JOIN
      crossover
    ON
      citation.value:""publication_number""::STRING = crossover.""publication_number""
  GROUP BY
    p.""family_id""
),

tmp_gpr AS (
  SELECT
    ""family_id"",
    LISTAGG(crossover.""publication_number"", ',') AS ""cited_by_publication_number""
  FROM
    ""PATENTS_GOOGLE"".""PATENTS_GOOGLE"".""ABS_AND_EMB"" AS p
    CROSS JOIN LATERAL FLATTEN(input => p.""cited_by"") AS cited_by
    LEFT JOIN
      crossover
    ON
      cited_by.value:""publication_number""::STRING = crossover.""publication_number""
  GROUP BY
    ""family_id""
),

gpr AS (
  SELECT
    tmp_gpr.""family_id"",
    LISTAGG(crossover.""family_id"", ',') WITHIN GROUP (ORDER BY crossover.""family_id"" ASC) AS ""cited_by""
  FROM
    tmp_gpr
    CROSS JOIN LATERAL FLATTEN(input => SPLIT(tmp_gpr.""cited_by_publication_number"", ',')) AS cited_by_publication_number
    LEFT JOIN
      crossover
    ON
      cited_by_publication_number.value::STRING = crossover.""publication_number""
  GROUP BY
    tmp_gpr.""family_id""
)

SELECT
  fam.""family_id"",
  pub.""publication_date"",
  pub.""publication_number"",
  pub.""country_code"",
  tech_class.""cpc"",
  tech_class.""ipc"",
  cit.""citation"",
  gpr.""cited_by""
FROM
  fam
  LEFT JOIN pub ON fam.""family_id"" = pub.""family_id""
  LEFT JOIN tech_class ON fam.""family_id"" = tech_class.""family_id""
  LEFT JOIN cit ON fam.""family_id"" = cit.""family_id""
  LEFT JOIN gpr ON fam.""family_id"" = gpr.""family_id""
WHERE
  pub.""publication_date"" BETWEEN 20150101 AND 20150131;
",,snowflake
28,sf_bq215,PATENTS,Which US patent (with a B2 kind code and a grant date between 2015 and 2018) has the highest originality score calculated as 1 - (the sum of squared occurrences of distinct 4-digit IPC codes in its backward citations divided by the square of the total occurrences of these 4-digit IPC codes)?,,"### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
29,sf_bq222,PATENTS,"Find the CPC technology areas in Germany that had the highest exponential moving average (smoothing factor 0.1) of patent filings per year, specifically for patents granted in December 2016. For each CPC group at level 4, show the full title, CPC group, and the year with the highest exponential moving average of patent filings.","WITH patent_cpcs AS (
    SELECT
        cd.""parents"",
        CAST(FLOOR(""filing_date"" / 10000) AS INT) AS ""filing_year""
    FROM (
        SELECT MAX(""cpc"") AS ""cpc"", MAX(""filing_date"") AS ""filing_date""
        FROM ""PATENTS"".""PATENTS"".""PUBLICATIONS""
        WHERE ""application_number"" != ''
          AND ""country_code"" = 'DE'
          AND ""grant_date"" >= 20161201
          AND ""grant_date"" <= 20161231
        GROUP BY ""application_number""
    ), LATERAL FLATTEN(INPUT => ""cpc"") AS cpcs
    JOIN ""PATENTS"".""PATENTS"".""CPC_DEFINITION"" cd ON cd.""symbol"" = cpcs.value:""code""
    WHERE cpcs.value:""first"" = TRUE
      AND ""filing_date"" > 0
),
yearly_counts AS (
    SELECT
        ""cpc_group"",
        ""filing_year"",
        COUNT(*) AS ""cnt""
    FROM (
        SELECT
            cpc_parent.VALUE AS ""cpc_group"",  -- Corrected reference to flattened ""parents""
            ""filing_year""
        FROM patent_cpcs,
             LATERAL FLATTEN(INPUT => ""parents"") AS cpc_parent  -- Corrected reference to flattened ""parents""
    )
    GROUP BY ""cpc_group"", ""filing_year""
),
moving_avg AS (
    SELECT
        ""cpc_group"",
        ""filing_year"",
        ""cnt"",
        AVG(""cnt"") OVER (PARTITION BY ""cpc_group"" ORDER BY ""filing_year"" ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS ""moving_avg""
    FROM yearly_counts
)
SELECT 
    c.""titleFull"",  -- Ensure correct column name (check case)
    REPLACE(""cpc_group"", '""', '') AS ""cpc_group"",
    MAX(""filing_year"") AS ""best_filing_year""
FROM moving_avg
JOIN ""PATENTS"".""PATENTS"".""CPC_DEFINITION"" c ON ""cpc_group"" = c.""symbol""
WHERE c.""level"" = 4
GROUP BY c.""titleFull"", ""cpc_group""
ORDER BY c.""titleFull"", ""cpc_group"" ASC;
","### Document: Sliding Window Calculation for Weighted Moving Average

#### 1. **Overview**
In the SQL query, the **Weighted Moving Average (WMA)** method is applied to smooth the annual patent filing counts for each CPC technology area and identify the ""best year"" for each CPC group. This sliding window calculation is used to highlight years with significant patent filing activity by giving more weight to recent years while considering past data.

The goal of this method is to reduce the impact of short-term fluctuations and better capture long-term trends in patent filing activities, particularly in fast-evolving technology areas.

#### 2. **Weighted Moving Average (WMA) Calculation**

##### 2.1 **Definition**
Weighted Moving Average (WMA) is a method where each data point is given a different weight, with more recent data points typically receiving higher weights. This approach is useful for identifying trends over time while minimizing the effect of older data that might not be as relevant.

##### 2.2 **Formula**
The formula for calculating the Weighted Moving Average is as follows:

\[
WMA_t = \alpha \cdot x_t + (1 - \alpha) \cdot WMA_{t-1}
\]

Where:
- \(WMA_t\): The weighted moving average for the current year (t).
- \(x_t\): The patent filing count for the current year.
- \(WMA_{t-1}\): The weighted moving average for the previous year.
- \(\alpha\): The smoothing factor (in this case, 0.1).

##### 2.3 **Explanation**
- **Smoothing Factor (\(\alpha\))**: The smoothing factor determines how much weight is given to the most recent data point. In this case, the smoothing factor is 0.1, meaning 10% of the weight is assigned to the current year's filing count, and the remaining 90% is based on the previous year’s moving average.
- **Sliding Window**: As we move through the years, the weighted average continuously updates using the most recent filing count and the previous year's weighted average. This creates a ""sliding window"" where each year's filing count is incorporated into the calculation.
",snowflake
30,sf_bq221,PATENTS,"Identify the CPC technology areas with the highest exponential moving average of patent filings each year (with a smoothing factor of 0.2), considering only the first CPC code for each patent that has a valid filing date and a non-empty application number, and report the full CPC title along with the best year associated with the highest exponential moving average for each CPC group at level 5.","WITH patent_cpcs AS (
    SELECT
        cd.""parents"",
        CAST(FLOOR(""filing_date"" / 10000) AS INT) AS ""filing_year""
    FROM (
        SELECT
            MAX(""cpc"") AS ""cpc"", MAX(""filing_date"") AS ""filing_date""
        FROM
            PATENTS.PATENTS.PUBLICATIONS
        WHERE 
            ""application_number"" != ''
        GROUP BY
            ""application_number""
    ) AS publications
    , LATERAL FLATTEN(INPUT => ""cpc"") AS cpcs
    JOIN
        PATENTS.PATENTS.CPC_DEFINITION cd ON cd.""symbol"" = cpcs.value:""code""
    WHERE 
        cpcs.value:""first"" = TRUE
          AND ""filing_date"" > 0

),
yearly_counts AS (
    SELECT
        ""cpc_group"",
        ""filing_year"",
        COUNT(*) AS ""cnt""
    FROM (
        SELECT
            cpc_parent.value::STRING AS ""cpc_group"",
            ""filing_year""
        FROM patent_cpcs,
             LATERAL FLATTEN(input => patent_cpcs.""parents"") AS cpc_parent
    )
    GROUP BY ""cpc_group"", ""filing_year""
),
ordered_counts AS (
    SELECT
        ""cpc_group"",
        ""filing_year"",
        ""cnt"",
        ROW_NUMBER() OVER (PARTITION BY ""cpc_group"" ORDER BY ""filing_year"" ASC) AS rn
    FROM yearly_counts
),
recursive_ema AS (
    -- Anchor member: first year per cpc_group
    SELECT
        ""cpc_group"",
        ""filing_year"",
        ""cnt"",
        ""cnt"" * 0.2 + 0 * 0.8 AS ""ema"",
        rn
    FROM ordered_counts
    WHERE rn = 1

    UNION ALL

    -- Recursive member: subsequent years
    SELECT
        oc.""cpc_group"",
        oc.""filing_year"",
        oc.""cnt"",
        oc.""cnt"" * 0.2 + re.""ema"" * 0.8 AS ""ema"",
        oc.rn
    FROM ordered_counts oc
    JOIN recursive_ema re
        ON oc.""cpc_group"" = re.""cpc_group""
       AND oc.rn = re.rn + 1
),
max_ema AS (
    SELECT
        ""cpc_group"",
        ""filing_year"",
        ""ema""
    FROM recursive_ema
),
ranked_ema AS (
    SELECT
        me.""cpc_group"",
        me.""filing_year"",
        me.""ema"",
        ROW_NUMBER() OVER (
            PARTITION BY me.""cpc_group"" 
            ORDER BY me.""ema"" DESC, me.""filing_year"" DESC
        ) AS rn_rank
    FROM max_ema me
)
SELECT 
    c.""titleFull"",
    REPLACE(r.""cpc_group"", '""', '') AS ""cpc_group"",
    r.""filing_year"" AS ""best_filing_year""
FROM ranked_ema r
JOIN ""PATENTS"".""PATENTS"".""CPC_DEFINITION"" c 
    ON r.""cpc_group"" = c.""symbol""
WHERE 
    c.""level"" = 5
    AND r.rn_rank = 1
ORDER BY 
    c.""titleFull"", 
    ""cpc_group"" ASC;
","### Document: Sliding Window Calculation for Weighted Moving Average

#### 1. **Overview**
In the SQL query, the **Weighted Moving Average (WMA)** method is applied to smooth the annual patent filing counts for each CPC technology area and identify the ""best year"" for each CPC group. This sliding window calculation is used to highlight years with significant patent filing activity by giving more weight to recent years while considering past data.

The goal of this method is to reduce the impact of short-term fluctuations and better capture long-term trends in patent filing activities, particularly in fast-evolving technology areas.

#### 2. **Weighted Moving Average (WMA) Calculation**

##### 2.1 **Definition**
Weighted Moving Average (WMA) is a method where each data point is given a different weight, with more recent data points typically receiving higher weights. This approach is useful for identifying trends over time while minimizing the effect of older data that might not be as relevant.

##### 2.2 **Formula**
The formula for calculating the Weighted Moving Average is as follows:

\[
WMA_t = \alpha \cdot x_t + (1 - \alpha) \cdot WMA_{t-1}
\]

Where:
- \(WMA_t\): The weighted moving average for the current year (t).
- \(x_t\): The patent filing count for the current year.
- \(WMA_{t-1}\): The weighted moving average for the previous year.
- \(\alpha\): The smoothing factor (in this case, 0.1).

##### 2.3 **Explanation**
- **Smoothing Factor (\(\alpha\))**: The smoothing factor determines how much weight is given to the most recent data point. In this case, the smoothing factor is 0.1, meaning 10% of the weight is assigned to the current year's filing count, and the remaining 90% is based on the previous year’s moving average.
- **Sliding Window**: As we move through the years, the weighted average continuously updates using the most recent filing count and the previous year's weighted average. This creates a ""sliding window"" where each year's filing count is incorporated into the calculation.
",snowflake
31,sf_bq223,PATENTS,"Which assignees, excluding DENSO CORP itself, have cited patents assigned to DENSO CORP, and what are the titles of the primary CPC subclasses associated with these citations? Provide the name of each citing assignee (excluding DENSO CORP), the full title of the primary CPC subclass (based on the first CPC code), and the count of citations grouped by the citing assignee and the CPC subclass title. Ensure that only citations of patents with valid filing dates are considered, and focus on the first CPC code for each citing patent. The results should specifically exclude DENSO CORP as a citing assignee.","SELECT
    REPLACE(citing_assignee, '""', '') AS citing_assignee,
    cpcdef.""titleFull"" AS cpc_title,
    COUNT(*) AS number
FROM (
    SELECT
        pubs.""publication_number"" AS citing_publication_number,
        cite.value:""publication_number"" AS cited_publication_number,
        citing_assignee_s.value:""name"" AS citing_assignee,
        SUBSTR(cpcs.value:""code"", 1, 4) AS citing_cpc_subclass
    FROM 
        PATENTS.PATENTS.PUBLICATIONS AS pubs
    , LATERAL FLATTEN(input => pubs.""citation"") AS cite
    , LATERAL FLATTEN(input => pubs.""assignee_harmonized"") AS citing_assignee_s
    , LATERAL FLATTEN(input => pubs.""cpc"") AS cpcs
    WHERE
        cpcs.value:""first"" = TRUE
) AS pubs
JOIN (
    SELECT
        ""publication_number"" AS cited_publication_number,
        cited_assignee_s.value:""name"" AS cited_assignee
    FROM
        PATENTS.PATENTS.PUBLICATIONS
    , LATERAL FLATTEN(input => ""assignee_harmonized"") AS cited_assignee_s
) AS refs
    ON pubs.cited_publication_number = refs.cited_publication_number
JOIN
    PATENTS.PATENTS.CPC_DEFINITION AS cpcdef
    ON cpcdef.""symbol"" = pubs.citing_cpc_subclass
WHERE
    refs.cited_assignee = 'DENSO CORP'
    AND pubs.citing_assignee != 'DENSO CORP'
GROUP BY
    citing_assignee, cpcdef.""titleFull""
","### IPC Codes: Handling Main IPC Code Selection

When dealing with the `ipc` field in the `patents-public-data.patents.publications` dataset, it is important to understand the structure of this field, especially the subfield `first`. This subfield is a boolean that indicates whether a given IPC code is the main code for the publication number in question. This is crucial because each patent publication can be associated with multiple IPC codes, signifying the various aspects of the technology covered by the patent.

However, not every publication in the dataset has a designated main IPC code. This lack of a clearly identified main IPC code complicates the process of determining the most relevant IPC code for each publication, as selecting a single IPC code from multiple possibilities without clear prioritization can lead to inconsistent or skewed analyses.

This approach ensures a more consistent and representative selection of IPC codes across the dataset, facilitating more accurate and meaningful analysis of patent trends and classifications. By focusing on the most frequently occurring 4-digit IPC code, the view helps overcome the limitations posed by the absence of a designated main IPC code, thereby enhancing the reliability of patent-related studies and insights derived from this data.

Here is an example

```
SELECT 
    t1.publication_number, 
    SUBSTR(ipc_u.code, 0, 4) as ipc4, 
    COUNT(
    SUBSTR(ipc_u.code, 0, 4)
    ) as ipc4_count 
FROM 
    `patents-public-data.patents.publications` t1, 
    UNNEST(ipc) AS ipc_u 
GROUP BY 
    t1.publication_number, 
    ipc4

```



# Text Embeddings (Similarity)

Patent documents are rich with textual data. In fact, most of the information contained in a patent document is text. This includes the `abstract_localized`, `description_localized`, and `claims_localized`. Textual data can be a powerful tool to analyze and compare patent scope and content across patents. However, before being able to use textual data, it needs to be vectorized or transformed into text embeddings that can be used by machine learning models. Therefore, creating text embeddings from the textual data of patents is necessary to compare patent contents. Technically speaking, running an NLP algorithm that creates embeddings for all U.S. patents is computationally difficult.

Nevertheless, Google runs their own machine learning algorithm which transforms patent text metadata into text embeddings which they report in `patents-public-data.google_patents_research.publications` table. The textual embeddings of one patent, without any knowledge on the algorithm being used to create them, are meaningless on their own. However, the embeddings are powerful when it comes to comparing textual content of two or more patents. Embeddings can be used to calculate a similarity score between any two patents. This similarity score is calculated by applying the dot product of the embeddings vector of the patents, as shown below:

The similarity \( \text{Similarty}_{i,k} \) between two patents \( i \) and \( k \) is calculated as the dot product of their embedding vectors:

\[
\text{Similarty}_{i,k} = \mathbf{v}_i \cdot \mathbf{v}_k
\]

where

\[
\mathbf{v}_i = [v_{i1}, v_{i2}, v_{i3}, \ldots, v_{iN}]
\]
and
\[
\mathbf{v}_k = [v_{k1}, v_{k2}, v_{k3}, \ldots, v_{kN}]
\]

are the embedding vectors for patents \( i \) and \( k \) respectively. The higher the dot product, the more similar the patents.





# Originality (Trajtenberg)

One of the most important measures of a patent is ""basicness"". The aspects of basicness are tough to measure. Nevertheless, some literature finds that important aspects of these measures are embodied in the relationship between the invention and the technological predcessors and successors it is connected to through, for example, patent citations. We can thus use patent citations to construct measures that identify basicness and appropriability. Trajtenberg et al. 1997 provide a number of these measures. They distinguish between:

1. Forward-looking measures: measures that are derived from the relationship between an invention and subsequent technologies that build upon it. These measures are thus constructed from the forward citations. One example of a forward-looking basicness measure they provide is Generality, which is calculated as:

\[
\text{GENERALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITING}_{G_k}}{\text{NCITING}_i} \right)
\]


2. Backward-looking measures: measures that are derived from the relationship between a given patent and the body of knowledge that preceded it. These measure are thus constructed from the backward citations. One example of a backward-looking basicness measure they provide is Orginality, which is calculated as:

\[
\text{ORIGINALITY}_i = 1 - \sum_{k=1}^{N_i} \left( \frac{\text{NCITED}_{i,k}}{\text{NCITED}_i} \right)
\]

With **NCITING** and **NCITED** defined as the number of patents citing the focal patent and the number of patents cited by the focal patent, respectively. Index `i` corresponds to the focal patent considered, and `k` is the index of patent classes. For example, **NCITED_2,3** refers to the number of patents in patent class 3 and cited by our focal patent 2.

",snowflake
32,sf_bq420,PATENTS_USPTO,"Can you identify the top 5 patents that were initially rejected under section 101 with no allowed claims, based on the length of their granted claims? The patents should have been granted in the US between 2010 and 2023. Additionally, ensure to select the first office action date for each application. Please include their first publication numbers, along with their first publication dates, length of the filed claims and grant dates.",,,snowflake
33,sf_bq207,PATENTS_USPTO,"Could you provide the earliest publication numbers, corresponding application numbers, claim numbers, and word counts for the top 100 independent patent claims, based on the highest word count, retrieved from claims stats within uspto_oce_claims (filtered by ind_flg='1'), matched with their publication numbers from uspto_oce_claims match, and further joined with patents publications to ensure only the earliest publication for each application is included, ordered by descending word count, and limited to the top 100 results?",,,snowflake
34,sf_bq128,PATENTSVIEW,"Retrieve the following information for U.S. patents filed between January 1, 2014, and February 1, 2014. The patent title and abstract. The publication date of the patent. The number of backward citations for each patent (i.e., the number of patents cited by the current patent before its filing date). The number of forward citations for each patent within the first 5 years of its publication (i.e., the number of patents that cited the current patent within 5 years after its publication). For each patent, ensure the forward citations are counted only for citations within 5 years after the publication date, and backward citations are counted for citations before the filing date.","SELECT
    patent.""title"",
    patent.""abstract"",
    app.""date"" AS publication_date,
    filterData.""bkwdCitations"",
    filterData.""fwrdCitations_5""
FROM
    ""PATENTSVIEW"".""PATENTSVIEW"".""PATENT"" AS patent
JOIN
    ""PATENTSVIEW"".""PATENTSVIEW"".""APPLICATION"" AS app
    ON app.""patent_id"" = patent.""id""
JOIN (
    SELECT
        DISTINCT cpc.""patent_id"",
        IFNULL(citation_5.""bkwdCitations"", 0) AS ""bkwdCitations"",
        IFNULL(citation_5.""fwrdCitations_5"", 0) AS ""fwrdCitations_5""
    FROM
        ""PATENTSVIEW"".""PATENTSVIEW"".""CPC_CURRENT"" AS cpc
    LEFT JOIN (
        SELECT
            b.""patent_id"",
            b.""bkwdCitations"",
            f.""fwrdCitations_5""
        FROM (
            SELECT 
                cited.""citation_id"" AS ""patent_id"",
                IFNULL(COUNT(*), 0) AS ""fwrdCitations_5""
            FROM 
                ""PATENTSVIEW"".""PATENTSVIEW"".""USPATENTCITATION"" AS cited
            JOIN
                ""PATENTSVIEW"".""PATENTSVIEW"".""APPLICATION"" AS apps
                ON cited.""citation_id"" = apps.""patent_id""
            WHERE
                apps.""country"" = 'US'
                AND cited.""date"" >= apps.""date""
                AND TRY_CAST(cited.""date"" AS DATE) <= DATEADD(YEAR, 5, TRY_CAST(apps.""date"" AS DATE)) -- 5-year citation window
            GROUP BY 
                cited.""citation_id""
        ) AS f
        JOIN (
            SELECT 
                cited.""patent_id"",
                IFNULL(COUNT(*), 0) AS ""bkwdCitations""
            FROM 
                ""PATENTSVIEW"".""PATENTSVIEW"".""USPATENTCITATION"" AS cited
            JOIN
                ""PATENTSVIEW"".""PATENTSVIEW"".""APPLICATION"" AS apps
                ON cited.""patent_id"" = apps.""patent_id""
            WHERE
                apps.""country"" = 'US'
                AND cited.""date"" < apps.""date"" -- backward citation count
            GROUP BY 
                cited.""patent_id""
        ) AS b
        ON b.""patent_id"" = f.""patent_id""
        WHERE
            b.""bkwdCitations"" IS NOT NULL
            AND f.""fwrdCitations_5"" IS NOT NULL
    ) AS citation_5 
    ON cpc.""patent_id"" = citation_5.""patent_id""
    WHERE 
        cpc.""subsection_id"" IN ('C05', 'C06', 'C07', 'C08', 'C09', 'C10', 'C11', 'C12', 'C13')
        OR cpc.""group_id"" IN ('A01G', 'A01H', 'A61K', 'A61P', 'A61Q', 'B01F', 'B01J', 'B81B', 'B82B', 'B82Y', 'G01N', 'G16H')
) AS filterData
ON app.""patent_id"" = filterData.""patent_id""
WHERE
    TRY_CAST(app.""date"" AS DATE) < '2014-02-01' 
    AND TRY_CAST(app.""date"" AS DATE) >= '2014-01-01';
","## Patent Citations

Patent citations are citations to other patent documents. They take the form of backward (cited) and forward (citing) citations. Backward citations, also referred to as back citations or cited patent documents, refer to earlier patent applications or grants that affect the scope of the claims of an application. Forward citations or citing documents refer to later filings of applications that are affected by the scope of the claims of the cited document.

## Our Focus

We only focus on patents which are applied in the USA, and particularly in chemistry, biology, and medical-related fields. Concretely, the subsection id of Cooperative Patent Classification (CPC) should be chosen from C5 to C13, or its group id is among the following set:
- A01G
- A01H
- A61K
- A61P
- A61Q
- B01F
- B01J
- B81B
- B82B
- B82Y
- G01N
- G16H",snowflake
35,sf_bq246,PATENTSVIEW,"Retrieve U.S. patents with the number of forward citations within the first 3 years after the patent application date (i.e., patents citing the current patent within 3 years). Only include patents with both backward citations within 1 year before the application date and forward citations within 1 year after the application date. The query should focus on specific CPC categories, sort results by backward citations in descending order, and return the patent with the most backward citations, limiting to one result.","SELECT filterData.""fwrdCitations_3""
FROM
  PATENTSVIEW.PATENTSVIEW.APPLICATION AS app
JOIN (
  SELECT DISTINCT 
    cpc.""patent_id"", 
    IFNULL(citation_3.""bkwdCitations_3"", 0) AS ""bkwdCitations_3"", 
    IFNULL(citation_3.""fwrdCitations_3"", 0) AS ""fwrdCitations_3""
  FROM
    PATENTSVIEW.PATENTSVIEW.CPC_CURRENT AS cpc
  LEFT JOIN (
    SELECT 
      b.""patent_id"", 
      b.""bkwdCitations_3"", 
      f.""fwrdCitations_3""
    FROM 
      (SELECT 
         cited.""patent_id"",
         COUNT(*) AS ""fwrdCitations_3""
       FROM 
         PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited
       JOIN
         PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps
         ON cited.""patent_id"" = apps.""patent_id""
       WHERE
         apps.""country"" = 'US'
         AND cited.""date"" >= apps.""date""
         AND TRY_CAST(cited.""date"" AS DATE) <= DATEADD(YEAR, 1, TRY_CAST(apps.""date"" AS DATE)) -- Citation within 1 year
       GROUP BY 
         cited.""patent_id""
      ) AS f
    JOIN (
      SELECT 
        cited.""patent_id"",
        COUNT(*) AS ""bkwdCitations_3""
      FROM 
        PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited
      JOIN
        PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps
        ON cited.""patent_id"" = apps.""patent_id""
      WHERE
        apps.""country"" = 'US'
        AND cited.""date"" < apps.""date""
        AND TRY_CAST(cited.""date"" AS DATE) >= DATEADD(YEAR, -1, TRY_CAST(apps.""date"" AS DATE)) -- Citation within 1 year before
      GROUP BY 
        cited.""patent_id""
    ) AS b
    ON b.""patent_id"" = f.""patent_id""
    WHERE 
      b.""bkwdCitations_3"" IS NOT NULL
      AND f.""fwrdCitations_3"" IS NOT NULL
  ) AS citation_3 
  ON cpc.""patent_id"" = citation_3.""patent_id""
) AS filterData
ON app.""patent_id"" = filterData.""patent_id""
ORDER BY filterData.""bkwdCitations_3"" DESC
LIMIT 1;
",,snowflake
36,sf_bq052,PATENTSVIEW,"Retrieve the following information for U.S. patents: The patent ID, title, and application date. The number of backward citations within 1 month before the application date (i.e., patents that cited the current patent before its application). The number of forward citations within 1 month after the application date (i.e., patents that cited the current patent after its application). The abstract text of the patent. Only include patents that belong to specific CPC categories, such as subsection 'C05' or group 'A01G'. The query should filter patents to include only those that have at least one backward citation or one forward citation in the 1-month period specified. Sort the results by application date and return all matching records.","SELECT
    app.""patent_id"" AS ""patent_id"",
    patent.""title"",
    app.""date"" AS ""application_date"",
    filterData.""bkwdCitations_1"",
    filterData.""fwrdCitations_1"",
    summary.""text"" AS ""summary_text""
FROM
    PATENTSVIEW.PATENTSVIEW.BRF_SUM_TEXT AS summary
JOIN
    PATENTSVIEW.PATENTSVIEW.PATENT AS patent
    ON summary.""patent_id"" = patent.""id""
JOIN
    PATENTSVIEW.PATENTSVIEW.APPLICATION AS app
    ON app.""patent_id"" = summary.""patent_id""
JOIN (
    SELECT DISTINCT
        cpc.""patent_id"",
        IFNULL(citation_1.""bkwdCitations_1"", 0) AS ""bkwdCitations_1"",
        IFNULL(citation_1.""fwrdCitations_1"", 0) AS ""fwrdCitations_1""
    FROM
        PATENTSVIEW.PATENTSVIEW.CPC_CURRENT AS cpc
    JOIN (
        SELECT
            b.""patent_id"",
            b.""bkwdCitations_1"",
            f.""fwrdCitations_1""
        FROM (
            SELECT
                cited.""patent_id"",
                COUNT(*) AS ""fwrdCitations_1""
            FROM
                PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited
            JOIN
                PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps
                ON cited.""patent_id"" = apps.""patent_id""
            WHERE
                apps.""country"" = 'US'
                AND cited.""date"" >= apps.""date""
                AND TRY_CAST(cited.""date"" AS DATE) <= DATEADD(MONTH, 1, TRY_CAST(apps.""date"" AS DATE)) -- Citation within 1 month
            GROUP BY
                cited.""patent_id""
        ) AS f
        JOIN (
            SELECT
                cited.""patent_id"",
                COUNT(*) AS ""bkwdCitations_1""
            FROM
                PATENTSVIEW.PATENTSVIEW.USPATENTCITATION AS cited
            JOIN
                PATENTSVIEW.PATENTSVIEW.APPLICATION AS apps
                ON cited.""patent_id"" = apps.""patent_id""
            WHERE
                apps.""country"" = 'US'
                AND cited.""date"" < apps.""date""
                AND TRY_CAST(cited.""date"" AS DATE) >= DATEADD(MONTH, -1, TRY_CAST(apps.""date"" AS DATE)) -- Citation within 1 month before
            GROUP BY
                cited.""patent_id""
        ) AS b
        ON b.""patent_id"" = f.""patent_id""
        WHERE
            b.""bkwdCitations_1"" IS NOT NULL
            AND f.""fwrdCitations_1"" IS NOT NULL
            AND (b.""bkwdCitations_1"" > 0 OR f.""fwrdCitations_1"" > 0)
    ) AS citation_1
    ON cpc.""patent_id"" = citation_1.""patent_id""
    WHERE
        cpc.""subsection_id"" = 'C05'
        OR cpc.""group_id"" = 'A01G'
) AS filterData
ON app.""patent_id"" = filterData.""patent_id""
ORDER BY app.""date"";
",,snowflake
37,sf_bq036,GITHUB_REPOS,What was the average number of GitHub commits made per month in 2016 for repositories containing Python code?,,,snowflake
38,sf_bq100,GITHUB_REPOS,"How can we identify the top 10 most frequently used packages in GitHub repository contents by looking for import statements enclosed in parentheses, splitting any multi-line imports by newlines, extracting package names that appear within double quotes, counting how often these packages appear, ignoring any null results, and finally ordering them in descending order of their frequency? The final answer should remove the quotation marks.",,,snowflake
39,sf_bq101,GITHUB_REPOS,"From GitHub Repos contents, how can we identify the top 10 most frequently imported package names in Java source files by splitting each file's content into lines, filtering for valid import statements, extracting only the package portion using a suitable regex, grouping by these extracted package names, counting their occurrences, and finally returning the 10 packages that appear most often in descending order of frequency?",,,snowflake
40,sf_bq182,GITHUB_REPOS_DATE,"Which primary programming languages, determined by the highest number of bytes in each repository, had at least 100 PullRequestEvents on January 18, 2023 across all their repositories?","WITH
  event_data AS (
    SELECT
      ""type"",
      EXTRACT(YEAR FROM TO_TIMESTAMP(""created_at"" / 1000000)) AS ""year"",
      EXTRACT(QUARTER FROM TO_TIMESTAMP(""created_at"" / 1000000)) AS ""quarter"",
      REGEXP_REPLACE(
        ""repo""::variant:""url""::string,
        'https:\\/\\/github\\.com\\/|https:\\/\\/api\\.github\\.com\\/repos\\/',
        ''
      ) AS ""name""
    FROM GITHUB_REPOS_DATE.DAY._20230118
  ),

  repo_languages AS (
    SELECT
      ""repo_name"" AS ""name"",
      ""lang""
    FROM (
      SELECT
        ""repo_name"",
        FIRST_VALUE(""language"") OVER (
          PARTITION BY ""repo_name"" ORDER BY ""bytes"" DESC
        ) AS ""lang""
      FROM (
        SELECT
          ""repo_name"",
          ""language"".value:""name"" AS ""language"",
          ""language"".value:""bytes"" AS ""bytes""
        FROM GITHUB_REPOS_DATE.GITHUB_REPOS.LANGUAGES,
        LATERAL FLATTEN(INPUT => ""language"") AS ""language""
      )
    )
    WHERE ""lang"" IS NOT NULL
    GROUP BY ""repo_name"", ""lang""
  ),

  joined_data AS (
    SELECT
      a.""type"" AS ""type"",
      b.""lang"" AS ""language"",
      a.""year"" AS ""year"",
      a.""quarter"" AS ""quarter""
    FROM event_data a
    JOIN repo_languages b
      ON a.""name"" = b.""name""
  ),

  count_data AS (
    SELECT
      ""language"",
      ""year"",
      ""quarter"",
      ""type"",
      COUNT(*) AS ""count""
    FROM joined_data
    GROUP BY ""type"", ""language"", ""year"", ""quarter""
    ORDER BY ""year"", ""quarter"", ""count"" DESC
  )

SELECT
  REPLACE(""language"", '""', '') AS ""language_name"",
  ""count""
FROM count_data
WHERE ""count"" >= 5
  AND ""type"" = 'PullRequestEvent';
",,snowflake
41,sf_bq217,GITHUB_REPOS_DATE,"On January 18, 2023, how many pull request creation events occurred in GitHub repositories that include JavaScript as one of their programming languages? Use data from the githubarchive  table for the events and the languages table for repository language information.",,,snowflake
42,sf_bq191,GITHUB_REPOS_DATE,"From the 2017 GitHub WatchEvent data, find the top two repositories that have more than 300 distinct watchers, ensuring the results are joined with the 'sample_files' table so that we return each repository's name along with its distinct watcher count, and limit the output to the two repositories with the highest watcher counts.",,,snowflake
43,sf_bq224,GITHUB_REPOS_DATE,"Which repository with an approved license in `licenses.md` had the highest combined total of forks, issues, and watches in April 2022?","WITH allowed_repos AS (
    SELECT 
        ""repo_name"",
        ""license""
    FROM 
        GITHUB_REPOS_DATE.GITHUB_REPOS.LICENSES
    WHERE 
        ""license"" IN (
            'gpl-3.0', 'artistic-2.0', 'isc', 'cc0-1.0', 'epl-1.0', 'gpl-2.0',
            'mpl-2.0', 'lgpl-2.1', 'bsd-2-clause', 'apache-2.0', 'mit', 'lgpl-3.0'
        )
),
watch_counts AS (
    SELECT 
        TRY_PARSE_JSON(""repo""):""name""::STRING AS ""repo"",
        COUNT(DISTINCT TRY_PARSE_JSON(""actor""):""login""::STRING) AS ""watches""
    FROM 
        GITHUB_REPOS_DATE.MONTH._202204
    WHERE 
        ""type"" = 'WatchEvent'
    GROUP BY 
        TRY_PARSE_JSON(""repo""):""name""
),
issue_counts AS (
    SELECT 
        TRY_PARSE_JSON(""repo""):""name""::STRING AS ""repo"",
        COUNT(*) AS ""issue_events""
    FROM 
        GITHUB_REPOS_DATE.MONTH._202204
    WHERE 
        ""type"" = 'IssuesEvent'
    GROUP BY 
        TRY_PARSE_JSON(""repo""):""name""
),
fork_counts AS (
    SELECT 
        TRY_PARSE_JSON(""repo""):""name""::STRING AS ""repo"",
        COUNT(*) AS ""forks""
    FROM 
        GITHUB_REPOS_DATE.MONTH._202204
    WHERE 
        ""type"" = 'ForkEvent'
    GROUP BY 
        TRY_PARSE_JSON(""repo""):""name""
)
SELECT 
    ar.""repo_name""
FROM 
    allowed_repos AS ar
INNER JOIN 
    fork_counts AS fc ON ar.""repo_name"" = fc.""repo""
INNER JOIN 
    issue_counts AS ic ON ar.""repo_name"" = ic.""repo""
INNER JOIN 
    watch_counts AS wc ON ar.""repo_name"" = wc.""repo""
ORDER BY 
    (fc.""forks"" + ic.""issue_events"" + wc.""watches"") DESC
LIMIT 1;
",,snowflake
44,sf_bq192,GITHUB_REPOS_DATE,"Find the most active Python repository on GitHub based on watcher count, issues, and forks. The query should select repositories with specific open-source licenses (`artistic-2.0`, `isc`, `mit`, `apache-2.0`), count distinct watchers, issue events, and forks for each repository in April 2022, and include only those with `.py` files on the `master` branch. Join the license data with watch counts, issue events, and fork counts, then sort by a combined metric of forks, issues, and watches, returning the name and count of the most active repository.",,,snowflake
45,sf_bq225,GITHUB_REPOS,"From the GitHub repository files in 'github_repos.sample_files' joined with 'github_repos.sample_contents', which 10 programming languages occur most frequently (based on recognized file extensions) in files that have non-empty content, ordered by their file counts in descending order?",,"## Programming Languages and File Extensions

We are interested in the following languages and file extensions:

| language | file extensions |
| -- | -- |
| Assembly     | .asm, .nasm              |
| C            | .c, .h                   |
| C#           | .cs                      |
| C++          | .c++, .cpp, .h++, .hpp   |
| CSS          | .css                     |
| Clojure      | .clj                     |
| Common Lisp  | .lisp                    |
| D            | .d                       |
| Dart         | .dart                    |
| Dockerfile   | Dockerfile, .dockerfile  |
| Elixir       | .ex, .exs                |
| Erlang       | .erl                     |
| Go           | .go                      |
| Groovy       | .groovy                  |
| HTML         | .html, .htm              |
| Haskell      | .hs                      |
| Haxe         | .hx                      |
| JSON         | .json                    |
| Java         | .java                    |
| JavaScript   | .js, .cjs                |
| Julia        | .jl                      |
| Kotlin       | .kt, .ktm, .kts          |
| Lua          | .lua                     |
| MATLAB       | .matlab, .m              |
| Markdown     | .md, .markdown, .mdown   |
| PHP          | .php                     |
| PowerShell   | .ps1, .psd1, .psm1       |
| Python       | .py                      |
| R            | .r                       |
| Ruby         | .rb                      |
| Rust         | .rs                      |
| SCSS         | .scss                    |
| SQL          | .sql                     |
| Sass         | .sass                    |
| Scala        | .scala                   |
| Shell        | .sh, .bash               |
| Swift        | .swift                   |
| TypeScript   | .ts                      |
| Vue          | .vue                     |
| XML          | .xml                     |
| YAML         | .yml, .yaml              |
",snowflake
46,sf_bq180,GITHUB_REPOS,"Get the top 5 most frequently used module names from Python (`.py`) and R (`.r`) scripts, counting occurrences of modules in `import` and `from` statements for Python, and `library()` calls for R. The query should consider only Python and R files, group by module name, and return the top 5 modules ordered by frequency.",,,snowflake
47,sf_bq233,GITHUB_REPOS,"Can you analyze the joined data from github repos files and github_repos contents, focusing only on files ending with '.py' or '.r', then extract Python modules from 'import' or 'from ... import' lines and R libraries from 'library(...)' lines, count their occurrences, and finally list the results sorted by language and by the number of occurrences in descending order?","WITH extracted_modules AS (
SELECT 
    el.""file_id"" AS ""file_id"", 
    el.""repo_name"", 
    el.""path"" AS ""path_"", 
    REPLACE(line.value, '""', '') AS ""line_"",
    CASE
        WHEN ENDSWITH(el.""path"", '.py') THEN 'python'
        WHEN ENDSWITH(el.""path"", '.r') THEN 'r'
        ELSE NULL
    END AS ""language"",
    CASE
        WHEN ENDSWITH(el.""path"", '.py') THEN
            ARRAY_CAT(
                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\bimport\\s+(\\w+)', 1, 1, 'e')),
                ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, '\\bfrom\\s+(\\w+)', 1, 1, 'e'))
            )
        WHEN ENDSWITH(el.""path"", '.r') THEN
            ARRAY_CONSTRUCT(REGEXP_SUBSTR(line.value, 'library\\s*\\(\\s*([^\\s)]+)\\s*\\)', 1, 1, 'e'))
        ELSE ARRAY_CONSTRUCT()
    END AS ""modules""
FROM (
    SELECT
        ct.""id"" AS ""file_id"", 
        fl.""repo_name"" AS ""repo_name"", 
        fl.""path"", 
        SPLIT(REPLACE(ct.""content"", '\n', ' \n'), '\n') AS ""lines""
    FROM 
        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_FILES AS fl
    JOIN 
        GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_CONTENTS AS ct 
        ON fl.""id"" = ct.""id""
) AS el,
LATERAL FLATTEN(input => el.""lines"") AS line 
WHERE
    (
        ENDSWITH(""path_"", '.py') 
        AND 
        (
            ""line_"" LIKE 'import %' 
            OR 
            ""line_"" LIKE 'from %'
        )
    )
    OR
    (
        ENDSWITH(""path_"", '.r') 
        AND 
        ""line_"" LIKE 'library%('
    )

),
module_counts AS (
    SELECT 
        em.""language"",
        f.value::STRING AS ""module"",
        COUNT(*) AS ""occurrence_count""
    FROM 
        extracted_modules AS em,
        LATERAL FLATTEN(input => em.""modules"") AS f
    WHERE 
        em.""modules"" IS NOT NULL
        AND f.value IS NOT NULL
    GROUP BY 
        em.""language"", 
        f.value
),
python AS (
    SELECT 
        ""language"",
        ""module"",
        ""occurrence_count""
    FROM 
        module_counts
    WHERE 
        ""language"" = 'python'
),
rlanguage AS (
    SELECT 
        ""language"",
        ""module"",
        ""occurrence_count""
    FROM 
        module_counts AS mc_inner
    WHERE 
        ""language"" = 'r'
)
SELECT 
    *
FROM 
    python
UNION ALL
SELECT 
    *
FROM 
    rlanguage
ORDER BY 
    ""language"", 
    ""occurrence_count"" DESC;",,snowflake
48,sf_bq248,GITHUB_REPOS,"Among all repositories that do not use any programming language whose name (case-insensitively) includes the substring ""python,"" what is the proportion of files whose paths include ""readme.md"" and whose contents contain the phrase ""Copyright (c)""?","WITH requests AS (
    SELECT 
        D.""id"",
        D.""content"",
        E.""repo_name"",
        E.""path""
    FROM 
        (
            SELECT 
                ""id"",
                ""content""
            FROM 
                GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS
            GROUP BY 
                ""id"", ""content""
        ) AS D
    INNER JOIN 
        (
            SELECT 
                C.""id"",
                C.""repo_name"",
                C.""path""
            FROM 
                (
                    SELECT 
                        ""id"",
                        ""repo_name"",
                        ""path""
                    FROM 
                        GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES
                    WHERE 
                        LOWER(""path"") LIKE '%readme.md'
                    GROUP BY 
                        ""path"", ""id"", ""repo_name""
                ) AS C
            INNER JOIN 
                (
                    SELECT 
                        ""repo_name"",
                        language_struct.value:""name""::STRING AS ""language_name""
                    FROM 
                        GITHUB_REPOS.GITHUB_REPOS.LANGUAGES,
                        LATERAL FLATTEN(input => ""language"") AS language_struct
                    WHERE 
                        LOWER(language_struct.value:""name""::STRING) NOT LIKE '%python%'
                    GROUP BY 
                        ""language_name"", ""repo_name""
                ) AS F
            ON 
                C.""repo_name"" = F.""repo_name""
        ) AS E
    ON 
        D.""id"" = E.""id""
)
SELECT 
    (SELECT COUNT(*) FROM requests WHERE ""content"" LIKE '%Copyright (c)%') / COUNT(*) AS ""proportion""
FROM 
    requests;
",,snowflake
49,sf_bq193,GITHUB_REPOS,"Retrieve all non-empty, non-commented lines from `README.md` files in GitHub repositories, excluding lines that are comments (either starting with `#` for Markdown or `//` for code comments). For each line, calculate how often each unique line appears across all repositories and return a comma-separated list of the programming languages used in each repository containing that line, sorted alphabetically, with the results ordered by the frequency of occurrence in descending order.","WITH content_extracted AS (
    SELECT 
        ""D"".""id"" AS ""id"",
        ""repo_name"",
        ""path"",
        SPLIT(""content"", '\n') AS ""lines"",
        ""language_name""
    FROM 
        (
            SELECT 
                ""id"",
                ""content""
            FROM 
                ""GITHUB_REPOS"".""GITHUB_REPOS"".""SAMPLE_CONTENTS""
        ) AS ""D""
    INNER JOIN 
        (
            SELECT 
                ""id"",
                ""C"".""repo_name"" AS ""repo_name"",
                ""path"",
                ""language_name""
            FROM 
                (
                    SELECT 
                        ""id"",
                        ""repo_name"",
                        ""path""
                    FROM 
                        ""GITHUB_REPOS"".""GITHUB_REPOS"".""SAMPLE_FILES""
                    WHERE 
                        LOWER(""path"") LIKE '%readme.md'
                ) AS ""C""
            INNER JOIN 
                (
                    SELECT 
                        ""repo_name"",
                        ""language_struct"".value:""name"" AS ""language_name""
                    FROM 
                        (
                            SELECT 
                                ""repo_name"", 
                                ""language""
                            FROM 
                                ""GITHUB_REPOS"".""GITHUB_REPOS"".""LANGUAGES""
                        )
                    CROSS JOIN 
                        LATERAL FLATTEN(INPUT => ""language"") AS ""language_struct""
                ) AS ""F""
            ON 
                ""C"".""repo_name"" = ""F"".""repo_name""
        ) AS ""E""
    ON 
        ""E"".""id"" = ""D"".""id""
),
non_empty_lines AS (
    SELECT 
        ""line"".value AS ""line_"",
        ""language_name""
    FROM 
        content_extracted,
        LATERAL FLATTEN(INPUT => ""lines"") AS ""line""
    WHERE 
        TRIM(""line"".value) != ''
        AND NOT STARTSWITH(TRIM(""line"".value), '#')
        AND NOT STARTSWITH(TRIM(""line"".value), '//')
),
aggregated_languages AS (
    SELECT 
        ""line_"",
        COUNT(*) AS ""frequency"",
        ARRAY_AGG(""language_name"") AS ""languages""
    FROM 
        non_empty_lines
    GROUP BY 
        ""line_""
)

SELECT 
    REGEXP_REPLACE(""line_"", '^""|""$', '') AS ""line"",
    ""frequency"",
    ARRAY_TO_STRING(ARRAY_SORT(""languages""), ', ') AS ""languages_sorted""
FROM 
    aggregated_languages
ORDER BY 
    ""frequency"" DESC;

",,snowflake
50,sf_bq295,GITHUB_REPOS_DATE,"Using the 2017 GitHub Archive data for watch events, which three repositories that include at least one Python file (with a .py extension) smaller than 15,000 bytes and containing the substring ""def "" in its content have the highest total number of watch events for that year?","WITH watched_repos AS (
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201701
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201702
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201703
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201704
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201705
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201706
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201707
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201708
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201709
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201710
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201711
    WHERE
        ""type"" = 'WatchEvent'
    UNION ALL
    SELECT
        PARSE_JSON(""repo""):""name""::STRING AS ""repo""
    FROM 
        GITHUB_REPOS_DATE.MONTH._201712
    WHERE
        ""type"" = 'WatchEvent'
),

repo_watch_counts AS (
    SELECT
        ""repo"",
        COUNT(*) AS ""watch_count""
    FROM
        watched_repos
    GROUP BY
        ""repo""
)

SELECT
    REPLACE(r.""repo"", '""', '') AS ""repo"",
    r.""watch_count""
FROM
    GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_FILES AS f
JOIN
    GITHUB_REPOS_DATE.GITHUB_REPOS.SAMPLE_CONTENTS AS c
    ON f.""id"" = c.""id""
JOIN 
    repo_watch_counts AS r
    ON f.""repo_name"" = r.""repo""
WHERE
    f.""path"" LIKE '%.py' 
    AND c.""size"" < 15000 
    AND POSITION('def ' IN c.""content"") > 0
GROUP BY
    r.""repo"", r.""watch_count""
ORDER BY
    r.""watch_count"" DESC
LIMIT 
    3;
",,snowflake
51,sf_bq249,GITHUB_REPOS,"Please provide a report on the number of occurrences of specific line types across files from the GitHub repository. Categorize a line as 'trailing' if it ends with a blank character, as 'Space' if it starts with a space, and as 'Other' if it meets neither condition. The report should include the total number of occurrences for each category, considering all lines across all files.",,,snowflake
52,sf_bq375,GITHUB_REPOS,"Determine which file type among Python (.py), C (.c), Jupyter Notebook (.ipynb), Java (.java), and JavaScript (.js) in the GitHub codebase has the most files with a directory depth greater than 10, and provide the file count.",,,snowflake
53,sf_bq255,GITHUB_REPOS,"How many commit messages are there in repositories that use the 'Shell' programming language and 'apache-2.0' license, where the length of the commit message is more than 5 characters but less than 10,000 characters, and the messages do not start with the word 'merge', 'update' or 'test'?","SELECT
  COUNT(commits_table.""message"") AS ""num_messages""
FROM (
  SELECT
    L.""repo_name"",
    language_struct.value:""name""::STRING AS ""language_name""
  FROM
    GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS L,
    LATERAL FLATTEN(input => L.""language"") AS language_struct
) AS lang_table
JOIN 
  GITHUB_REPOS.GITHUB_REPOS.LICENSES AS license_table
ON 
  license_table.""repo_name"" = lang_table.""repo_name""
JOIN (
  SELECT
    *
  FROM
    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS
) AS commits_table
ON 
  commits_table.""repo_name"" = lang_table.""repo_name""
WHERE
  license_table.""license"" LIKE 'apache-2.0'
  AND lang_table.""language_name"" LIKE 'Shell'
  AND LENGTH(commits_table.""message"") > 5
  AND LENGTH(commits_table.""message"") < 10000
  AND LOWER(commits_table.""message"") NOT LIKE 'update%'
  AND LOWER(commits_table.""message"") NOT LIKE 'test%'
  AND LOWER(commits_table.""message"") NOT LIKE 'merge%';
",,snowflake
54,sf_bq194,GITHUB_REPOS,"Among all Python (*.py), R (*.r, *.R, *.Rmd, *.rmd), and IPython notebook (*.ipynb) files in the GitHub sample dataset, which library or module is identified as the second most frequently imported or loaded based on the extracted import statements?",,,snowflake
55,sf_bq377,GITHUB_REPOS,Extract and count the frequency of all package names listed in the require section of JSON-formatted content,"WITH json_files AS (
  SELECT
    c.""id"",
    TRY_PARSE_JSON(c.""content""):""require"" AS ""dependencies""
  FROM
    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS c
),
package_names AS (
  SELECT
    f.key AS ""package_name""
  FROM
    json_files,
    LATERAL FLATTEN(input => ""dependencies"") AS f
)
SELECT
  ""package_name"",
  COUNT(*) AS ""count""
FROM
  package_names
WHERE
  ""package_name"" IS NOT NULL
GROUP BY
  ""package_name""
ORDER BY
  ""count"" DESC;
",,snowflake
56,sf_bq359,GITHUB_REPOS,List the repository names and commit counts for the top two GitHub repositories with JavaScript as the primary language and the highest number of commits.,"WITH repositories AS (
    SELECT
        t2.""repo_name"",
        t2.""language""
    FROM (
        SELECT
            t1.""repo_name"",
            t1.""language"",
            RANK() OVER (PARTITION BY t1.""repo_name"" ORDER BY t1.""language_bytes"" DESC) AS ""rank""
        FROM (
            SELECT
                l.""repo_name"",
                lang.value:""name""::STRING AS ""language"",
                lang.value:""bytes""::NUMBER AS ""language_bytes""
            FROM
                GITHUB_REPOS.GITHUB_REPOS.LANGUAGES AS l,
                LATERAL FLATTEN(input => l.""language"") AS lang
        ) AS t1
    ) AS t2
    WHERE t2.""rank"" = 1
),
python_repo AS (
    SELECT
        ""repo_name"",
        ""language""
    FROM
        repositories
    WHERE
        ""language"" = 'JavaScript'
)
SELECT 
    sc.""repo_name"", 
    COUNT(sc.""commit"") AS ""num_commits""
FROM 
    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_COMMITS AS sc
INNER JOIN 
    python_repo 
ON 
    python_repo.""repo_name"" = sc.""repo_name""
GROUP BY 
    sc.""repo_name""
ORDER BY 
    ""num_commits"" DESC
LIMIT 2;
",,snowflake
57,sf_bq252,GITHUB_REPOS,"Could you please find the name of the repository that contains the most copied non-binary Swift file in the dataset, ensuring each file is uniquely identified by its ID?","WITH selected_repos AS (
  SELECT
    f.""id"",
    f.""repo_name"" AS ""repo_name"",
    f.""path"" AS ""path""
  FROM
    GITHUB_REPOS.GITHUB_REPOS.SAMPLE_FILES AS f
),
deduped_files AS (
  SELECT
    f.""id"",
    MIN(f.""repo_name"") AS ""repo_name"",
    MIN(f.""path"") AS ""path""
  FROM
    selected_repos AS f
  GROUP BY
    f.""id""
)
SELECT
  f.""repo_name""
FROM
  deduped_files AS f
  JOIN GITHUB_REPOS.GITHUB_REPOS.SAMPLE_CONTENTS AS c 
  ON f.""id"" = c.""id""
WHERE
  NOT c.""binary""
  AND f.""path"" LIKE '%.swift'
ORDER BY c.""copies"" DESC
LIMIT 1;
",,snowflake
58,sf_bq251,PYPI,"I want to know the GitHub project URLs for the top 3 most downloaded PyPI packages based on download count. First, extract PyPI package metadata including name, version, and project URLs. Filter these URLs to only include those that link to GitHub repositories. Use a regular expression to clean the GitHub URLs by removing unnecessary parts like 'issues', 'pull', 'blob', and 'tree' paths, keeping only the main repository URL. For packages with multiple versions, use only the most recent version based on upload time. Join this data with download metrics to determine the most downloaded packages. Return only the cleaned GitHub repository URLs (without quotation marks) for the top 3 packages by total download count, ensuring that only packages with valid GitHub URLs are included in the results.",,,snowflake
59,bq019,cms_data,"In the 2014 CMS Medicare inpatient charges data, which DRG definition has the highest total number of discharges, and among the top three cities with the most discharges for that DRG definition, what are their respective weighted average total payments (weighted by total discharges)",,,snowflake
60,bq234,cms_data,What is the most prescribed medication in each state in 2014?,"SELECT
  A.state,
  drug_name,
  total_claim_count
FROM (
  SELECT
    generic_name AS drug_name,
    nppes_provider_state AS state,
    ROUND(SUM(total_claim_count)) AS total_claim_count,
    ROUND(SUM(total_day_supply)) AS day_supply,
    ROUND(SUM(total_drug_cost)) / 1e6 AS total_cost_millions
  FROM
    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`
  GROUP BY
    state,
    drug_name) A
INNER JOIN (
  SELECT
    state,
    MAX(total_claim_count) AS max_total_claim_count
  FROM (
    SELECT
      nppes_provider_state AS state,
      ROUND(SUM(total_claim_count)) AS total_claim_count
    FROM
      `bigquery-public-data.cms_medicare.part_d_prescriber_2014`
    GROUP BY
      state,
      generic_name)
  GROUP BY
    state) B
ON
  A.state = B.state
  AND A.total_claim_count = B.max_total_claim_count;",,snowflake
61,bq235,cms_data,Can you tell me which healthcare provider incurs the highest combined average costs for both outpatient and inpatient services in 2014?,"SELECT
  Provider_Name
FROM
(
SELECT
  OP.provider_state AS State,
  OP.provider_city AS City,
  OP.provider_id AS Provider_ID,
  OP.provider_name AS Provider_Name,
  ROUND(OP.average_OP_cost) AS Average_OP_Cost,
  ROUND(IP.average_IP_cost) AS Average_IP_Cost,
  ROUND(OP.average_OP_cost + IP.average_IP_cost) AS Combined_Average_Cost
FROM (
  SELECT
    provider_state,
    provider_city,
    provider_id,
    provider_name,
    SUM(average_total_payments*outpatient_services)/SUM(outpatient_services) AS average_OP_cost
  FROM
    `bigquery-public-data.cms_medicare.outpatient_charges_2014`
  GROUP BY
    provider_state,
    provider_city,
    provider_id,
    provider_name ) AS OP
INNER JOIN (
  SELECT
    provider_state,
    provider_city,
    provider_id,
    provider_name,
    SUM(average_medicare_payments*total_discharges)/SUM(total_discharges) AS average_IP_cost
  FROM
    `bigquery-public-data.cms_medicare.inpatient_charges_2014`
  GROUP BY
    provider_state,
    provider_city,
    provider_id,
    provider_name ) AS IP
ON
  OP.provider_id = IP.provider_id
  AND OP.provider_state = IP.provider_state
  AND OP.provider_city = IP.provider_city
  AND OP.provider_name = IP.provider_name
ORDER BY
  combined_average_cost DESC
LIMIT
  1
);",,snowflake
62,bq172,cms_data,"For the drug with the highest total number of prescriptions in New York State during 2014, could you list the top five states with the highest total claim counts for this drug? Please also include their total claim counts and total drug costs. ","WITH ny_top_drug AS (
  SELECT
    drug_name AS drug_name,
    ROUND(SUM(total_claim_count)) AS total_claim_count
  FROM
    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`
  WHERE
    nppes_provider_state = 'NY'
  GROUP BY
    drug_name
  ORDER BY
    total_claim_count DESC
  LIMIT 1
),
top_5_states AS (
  SELECT
    nppes_provider_state AS state,
    SUM(total_claim_count) AS total_claim_count,
    SUM(total_drug_cost) AS total_drug_cost
  FROM
    `bigquery-public-data.cms_medicare.part_d_prescriber_2014`
  WHERE
    drug_name = (SELECT drug_name FROM ny_top_drug)
  GROUP BY
    state
  ORDER BY
    total_claim_count DESC
  LIMIT 5
)
SELECT
  state,
  total_claim_count,
  total_drug_cost
FROM
  top_5_states;",,snowflake
63,bq177,cms_data,"For the provider whose total inpatient Medicare cost from 2011 through 2015 is the highest (computed as the sum of average_medicare_payments multiplied by total_discharges), please list that provider’s yearly average inpatient cost and yearly average outpatient cost for each calendar year in this period, where the inpatient cost is calculated as the average of (average_medicare_payments × total_discharges) and the outpatient cost is calculated as the average of (average_total_payments × outpatient_services).",,,snowflake
64,bq354,cms_data,"Could you provide the percentage of participants for standard acne, atopic dermatitis, psoriasis, and vitiligo as defined by the International Classification of Diseases 10-CM (ICD-10-CM), including their subcategories? Please include all related concepts mapped to the standard ICD-10-CM codes (L70 for acne, L20 for atopic dermatitis, L40 for psoriasis, and L80 for vitiligo) by utilizing concept relationships, including descendant concepts. The percentage should be calculated based on the total number of participants, considering only the standard concepts and their related descendants.","WITH skin_condition_ICD_concept_ids AS (
    SELECT
        concept_id,
        CASE concept_code
            WHEN 'L70' THEN 'Acne'
            WHEN 'L20' THEN 'Atopic dermatitis'
            WHEN 'L40' THEN 'Psoriasis'
            ELSE 'Vitiligo'
        END AS skin_condition
    FROM
        `bigquery-public-data.cms_synthetic_patient_data_omop.concept`
    WHERE
        concept_code IN ('L70', 'L20', 'L40', 'L80')
        AND vocabulary_id = 'ICD10CM'
),
standard_concept_ids AS (
    SELECT
        concept_id
    FROM
        `bigquery-public-data.cms_synthetic_patient_data_omop.concept`
    WHERE
        standard_concept = 'S'
),
skin_condition_standard_concept_ids AS (
    SELECT
        s.skin_condition,
        r.concept_id_2 AS concept_id
    FROM
        skin_condition_ICD_concept_ids s
    JOIN
        `bigquery-public-data.cms_synthetic_patient_data_omop.concept_relationship` r
    ON
        s.concept_id = r.concept_id_1
    JOIN
        standard_concept_ids sc
    ON
        sc.concept_id = r.concept_id_2
    WHERE
        r.relationship_id = 'Maps to'
),
all_skin_concept_ids AS (
    SELECT DISTINCT
        skin_condition,
        concept_id
    FROM
        skin_condition_standard_concept_ids
),
descendant_concept_ids AS (
    SELECT
        a.skin_condition,
        ca.descendant_concept_id AS concept_id
    FROM
        all_skin_concept_ids a
    JOIN
        `bigquery-public-data.cms_synthetic_patient_data_omop.concept_ancestor` ca
    ON
        a.concept_id = ca.ancestor_concept_id
),
participants_with_condition AS (
    SELECT
        d.skin_condition,
        COUNT(DISTINCT co.person_id) AS nb_of_participants_with_skin_condition
    FROM
        `bigquery-public-data.cms_synthetic_patient_data_omop.condition_occurrence` co
    JOIN
        descendant_concept_ids d
    ON
        co.condition_concept_id = d.concept_id
    GROUP BY
        d.skin_condition
),
total_participants AS (
    SELECT
        COUNT(DISTINCT person_id) AS nb_of_participants
    FROM
        `bigquery-public-data.cms_synthetic_patient_data_omop.person`
)
SELECT
    p.skin_condition,
    100 * p.nb_of_participants_with_skin_condition / t.nb_of_participants AS percentage_of_participants
FROM
    participants_with_condition p,
    total_participants t",,snowflake
65,bq355,cms_data,Please tell me the percentage of participants not using quinapril and related medications(Quinapril RxCUI: 35208).,"WITH quinapril_concept AS (
    SELECT concept_id
    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.concept`
    WHERE concept_code = ""35208"" AND vocabulary_id = ""RxNorm""
),
quinapril_related_medications AS (
    SELECT DISTINCT descendant_concept_id AS concept_id
    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.concept_ancestor`
    WHERE ancestor_concept_id IN (SELECT concept_id FROM quinapril_concept)
),
participants_with_quinapril AS (
    SELECT COUNT(DISTINCT person_id) AS count
    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.drug_exposure`
    WHERE drug_concept_id IN (SELECT concept_id FROM quinapril_related_medications)
),
total_participants AS (
    SELECT COUNT(DISTINCT person_id) AS count
    FROM `bigquery-public-data.cms_synthetic_patient_data_omop.person`
)
SELECT
    100 - (100 * participants_with_quinapril.count / total_participants.count) AS without_quinapril
FROM
    participants_with_quinapril, total_participants",,snowflake
66,bq032,noaa_data,Can you provide the latitude of the final coordinates for the hurricane that traveled the second longest distance in the North Atlantic during 2020?,"WITH hurricane_geometry AS (
  SELECT
    * EXCEPT (longitude, latitude),
    ST_GEOGPOINT(longitude, latitude) AS geom,
    MAX(usa_wind) OVER (PARTITION BY sid) AS max_wnd_speed
  FROM
    `bigquery-public-data.noaa_hurricanes.hurricanes`
  WHERE
    season = '2020'
    AND basin = 'NA'
    AND name != 'NOT NAMED'
),
dist_between_points AS (
  SELECT
    sid,
    name,
    season,
    iso_time,
    max_wnd_speed,
    geom,
    ST_DISTANCE(geom, LAG(geom, 1) OVER (PARTITION BY sid ORDER BY iso_time ASC)) / 1000 AS dist
  FROM
    hurricane_geometry
),
total_distances AS (
  SELECT
    sid,
    name,
    season,
    iso_time,
    max_wnd_speed,
    geom,
    SUM(dist) OVER (PARTITION BY sid ORDER BY iso_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_distance,
    SUM(dist) OVER (PARTITION BY sid) AS total_dist
  FROM
    dist_between_points
),
ranked_hurricanes AS (
  SELECT
    *,
    DENSE_RANK() OVER (ORDER BY total_dist DESC) AS dense_rank
  FROM
    total_distances
)

SELECT
  ST_Y(geom)
FROM
  ranked_hurricanes
WHERE
  dense_rank = 2
ORDER BY
cumulative_distance
DESC
LIMIT 1
;","Categories: Geospatial functions


## ST_DISTANCE

Returns the minimum geodesic distance between two GEOGRAPHY or the minimum Euclidean distance between two GEOMETRY objects.

## Syntax

ST_DISTANCE( <geography_or_geometry_expression_1> , <geography_or_geometry_expression_2> )


## Arguments


geography_or_geometry_expression_1The argument must be of type GEOGRAPHY or GEOMETRY.

geography_or_geometry_expression_2The argument must be of type GEOGRAPHY or GEOMETRY.


## Returns

Returns a REAL value, which represents the distance:

For GEOGRAPHY input values, the distance is in meters.
For GEOMETRY input values, the distance is computed with the same units used to define the input coordinates.


## Usage notes


Returns NULL if one or more input points are NULL.

For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows the distance in meters between two points 1 degree apart along the equator (approximately 111 kilometers or 69 miles).

WITH d AS
    ( ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0)) ) SELECT d / 1000 AS kilometers, d / 1609 AS miles;
+---------------+--------------+
|    KILOMETERS |        MILES |
|---------------+--------------|
| 111.195101177 | 69.108204585 |
+---------------+--------------+


This shows use of the ST_DISTANCE function with NULL values:

SELECT ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(NULL, NULL));
+-----------------------------------------------------------+
| ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(NULL, NULL)) |
|-----------------------------------------------------------|
|                                                      NULL |
+-----------------------------------------------------------+



## GEOMETRY examples

The following example compares the distance calculated for GEOGRAPHY and GEOMETRY input objects.

SELECT ST_DISTANCE(TO_GEOMETRY('POINT(0 0)'), TO_GEOMETRY('POINT(1 1)')) AS geometry_distance,
       ST_DISTANCE(TO_GEOGRAPHY('POINT(0 0)'), TO_GEOGRAPHY('POINT(1 1)')) AS geography_distance;

+-------------------+--------------------+
| GEOMETRY_DISTANCE | GEOGRAPHY_DISTANCE |
|-------------------+--------------------|
|       1.414213562 |   157249.628092508 |
+-------------------+--------------------+",snowflake
67,bq119,noaa_data,"Please show information about the hurricane with the third longest total travel distance in the North Atlantic during 2020, including its travel coordinates, the cumulative travel distance (in kilometers) at each point, and the maximum sustained wind speed at those times.","WITH hurricane_geometry AS (
  SELECT
    * EXCEPT (longitude, latitude),
    ST_GEOGPOINT(longitude, latitude) AS geom,
  FROM
    `bigquery-public-data.noaa_hurricanes.hurricanes`
  WHERE
    season = '2020'
    AND basin = 'NA'
    AND name != 'NOT NAMED'
),
dist_between_points AS (
  SELECT
    sid,
    name,
    season,
    iso_time,
    usa_wind,
    geom,
    ST_DISTANCE(geom, LAG(geom, 1) OVER (PARTITION BY sid ORDER BY iso_time ASC)) / 1000 AS dist
  FROM
    hurricane_geometry
),
total_distances AS (
  SELECT
    sid,
    name,
    season,
    iso_time,
    usa_wind,
    geom,
    SUM(dist) OVER (PARTITION BY sid ORDER BY iso_time ASC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cumulative_distance,
    SUM(dist) OVER (PARTITION BY sid) AS total_dist
  FROM
    dist_between_points
),
ranked_hurricanes AS (
  SELECT
    *,
    DENSE_RANK() OVER (ORDER BY total_dist DESC) AS dense_rank
  FROM
    total_distances
)

SELECT
  geom,cumulative_distance,usa_wind
FROM
  ranked_hurricanes
WHERE
  dense_rank = 3
ORDER BY
cumulative_distance;","Categories: Geospatial functions


## ST_DISTANCE

Returns the minimum geodesic distance between two GEOGRAPHY or the minimum Euclidean distance between two GEOMETRY objects.

## Syntax

ST_DISTANCE( <geography_or_geometry_expression_1> , <geography_or_geometry_expression_2> )


## Arguments


geography_or_geometry_expression_1The argument must be of type GEOGRAPHY or GEOMETRY.

geography_or_geometry_expression_2The argument must be of type GEOGRAPHY or GEOMETRY.


## Returns

Returns a REAL value, which represents the distance:

For GEOGRAPHY input values, the distance is in meters.
For GEOMETRY input values, the distance is computed with the same units used to define the input coordinates.


## Usage notes


Returns NULL if one or more input points are NULL.

For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows the distance in meters between two points 1 degree apart along the equator (approximately 111 kilometers or 69 miles).

WITH d AS
    ( ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0)) ) SELECT d / 1000 AS kilometers, d / 1609 AS miles;
+---------------+--------------+
|    KILOMETERS |        MILES |
|---------------+--------------|
| 111.195101177 | 69.108204585 |
+---------------+--------------+


This shows use of the ST_DISTANCE function with NULL values:

SELECT ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(NULL, NULL));
+-----------------------------------------------------------+
| ST_DISTANCE(ST_MAKEPOINT(0, 0), ST_MAKEPOINT(NULL, NULL)) |
|-----------------------------------------------------------|
|                                                      NULL |
+-----------------------------------------------------------+



## GEOMETRY examples

The following example compares the distance calculated for GEOGRAPHY and GEOMETRY input objects.

SELECT ST_DISTANCE(TO_GEOMETRY('POINT(0 0)'), TO_GEOMETRY('POINT(1 1)')) AS geometry_distance,
       ST_DISTANCE(TO_GEOGRAPHY('POINT(0 0)'), TO_GEOGRAPHY('POINT(1 1)')) AS geography_distance;

+-------------------+--------------------+
| GEOMETRY_DISTANCE | GEOGRAPHY_DISTANCE |
|-------------------+--------------------|
|       1.414213562 |   157249.628092508 |
+-------------------+--------------------+",snowflake
68,sf_bq117,NOAA_DATA,"What is the total number of severe storm events that occurred in the most affected month over the past 15 years according to NOAA records, considering only the top 100 storm events with the highest property damage?",,,snowflake
69,bq419,noaa_data,"Which 5 states had the most storm events from 1980 to 1995, considering only the top 1000 states with the highest event counts each year? Please use state abbreviations.","WITH s80 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1980` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),
s81 as
(SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1981` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),
s82 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1982` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s83 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1983` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s84 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1984` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s85 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1985` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s86 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1986` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s87 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1987` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s88 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1988` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s89 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1989` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s90 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1990` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s91 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1991` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),
s92 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1992` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s93 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1993` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s94 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1994` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000),

s95 as
  (SELECT state, COUNT(event_id) as num_events
  FROM `bigquery-public-data.noaa_historic_severe_storms.storms_1995` 
  GROUP BY state 
  ORDER BY num_events DESC
  LIMIT 1000)

SELECT s80.state, 
s80.num_events + s81.num_events +  s82.num_events +  s83.num_events +  s84.num_events +  s85.num_events +  s86.num_events +  s87.num_events + s88.num_events +  s89.num_events +  s90.num_events + s91.num_events + s92.num_events + s93.num_events + s94.num_events + s95.num_events as total_events 
FROM s80 FULL JOIN s81 ON s80.state = s81.state
FULL JOIN s82 ON s82.state = s81.state
FULL JOIN s83 ON s83.state = s81.state
FULL JOIN s84 ON s84.state = s81.state
FULL JOIN s85 ON s85.state = s81.state
FULL JOIN s86 ON s86.state = s81.state
FULL JOIN s87 ON s87.state = s81.state
FULL JOIN s88 ON s88.state = s81.state
FULL JOIN s89 ON s89.state = s81.state
FULL JOIN s90 ON s90.state = s81.state
FULL JOIN s91 ON s91.state = s81.state 
FULL JOIN s92 ON s92.state = s81.state
FULL JOIN s93 ON s93.state = s81.state
FULL JOIN s94 ON s94.state = s81.state
FULL JOIN s95 ON s95.state = s81.state

ORDER BY total_events DESC
LIMIT 5;",,snowflake
70,sf_bq071,NOAA_DATA_PLUS,"Can you provide the count of hurricanes and a list of hurricane names (sorted alphabetically and separated by commas) for each city and its associated zip code, where the hurricanes fall within the boundaries of the zip codes? Please exclude any unnamed hurricanes, and sort the results by the count of hurricanes in descending order. The output should include the following columns: city, zip code, state, count of hurricanes, and the list of hurricanes.",,"Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
71,sf_bq236,NOAA_DATA_PLUS,What are the top 5 zip codes of the areas in the United States that have experienced the most hail storm events in the past 10 years? Don't use data from hail reports table.,"SELECT
  CONCAT(""city"", ', ', ""state_name"") AS ""city"",
  ""zip_code"",
  COUNT(""event_id"") AS ""count_storms""
FROM (
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2014
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2015
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2016
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2017
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2018
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2019
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2020
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2021
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2022
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2023
    UNION ALL
    SELECT *
    FROM NOAA_DATA_PLUS.NOAA_HISTORIC_SEVERE_STORMS.STORMS_2024
) AS storms
JOIN NOAA_DATA_PLUS.GEO_US_BOUNDARIES.ZIP_CODES
  ON ST_WITHIN(ST_GEOGFROMWKB(storms.""event_point""), ST_GEOGFROMWKB(""zip_code_geom""))
WHERE
   LOWER(storms.""event_type"") = 'hail'
GROUP BY
  ""zip_code"", 
  ""city"", 
  ""state_name""
ORDER BY
  ""count_storms"" DESC
LIMIT 5;
","Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
72,bq356,noaa_data,"Among all NOAA GSOD weather stations that recorded valid daily temperature data (non-missing temp, max, min) in 2019 and whose period of record began on or before January 1, 2000, and continued through at least June 30, 2019, how many of these stations achieved 90% or more of the maximum possible number of valid temperature-record days in 2019?",,,snowflake
73,bq042,noaa_data,"Can you help me retrieve the average temperature, average wind speed, and precipitation for LaGuardia Airport in NYC on June 12 for each year from 2011 through 2020, specifically using the station ID 725030?","SELECT
  -- Create a timestamp from the date components.
  TIMESTAMP(CONCAT(year,""-"",mo,""-"",da)) AS timestamp,
  -- Replace numerical null values with actual null
  AVG(IF (temp=9999.9,
      null,
      temp)) AS temperature,
  AVG(IF (wdsp=""999.9"",
      null,
      CAST(wdsp AS Float64))) AS wind_speed,
  AVG(IF (prcp=99.99,
      0,
      prcp)) AS precipitation
FROM
  `bigquery-public-data.noaa_gsod.gsod20*`
WHERE
  CAST(YEAR AS INT64) > 2010
  AND CAST(YEAR AS INT64) < 2021
  AND CAST(MO AS INT64) = 6
  AND CAST(DA AS INT64) = 12
  AND stn = ""725030"" -- La Guardia
GROUP BY
  timestamp
ORDER BY
  timestamp ASC;",,snowflake
74,bq394,noaa_data,"What are the top 3 months between 2010 and 2014 with the smallest sum of absolute differences between the average air temperature, wet bulb temperature, dew point temperature, and sea surface temperature, including respective years and sum of differences? Please present the year and month in numerical format.","WITH DailyAverages AS (
    SELECT 
        year, month, day,
        air_temperature,
        wetbulb_temperature,
        dewpoint_temperature,
        sea_surface_temp
    FROM 
        `bigquery-public-data.noaa_icoads.icoads_core_*`
    WHERE
        _TABLE_SUFFIX BETWEEN '2010' AND '2014'
),

MonthlyAverages AS (
    SELECT 
        year,
        month,
        AVG(air_temperature) AS avg_air_temperature,
        AVG(wetbulb_temperature) AS avg_wetbulb_temperature,
        AVG(dewpoint_temperature) AS avg_dewpoint_temperature,
        AVG(sea_surface_temp) AS avg_sea_surface_temp
    FROM 
        DailyAverages
    WHERE
        air_temperature IS NOT NULL
        AND wetbulb_temperature IS NOT NULL
        AND dewpoint_temperature IS NOT NULL
        AND sea_surface_temp IS NOT NULL
    GROUP BY 
        year, month
),

DifferenceSums AS (
    SELECT
        year,
        month,
        (ABS(avg_air_temperature - avg_wetbulb_temperature) +
        ABS(avg_air_temperature - avg_dewpoint_temperature) +
        ABS(avg_air_temperature - avg_sea_surface_temp) +
        ABS(avg_wetbulb_temperature - avg_dewpoint_temperature) +
        ABS(avg_wetbulb_temperature - avg_sea_surface_temp) +
        ABS(avg_dewpoint_temperature - avg_sea_surface_temp)) AS sum_of_differences
    FROM 
        MonthlyAverages
)

SELECT
    year,
    month,
    sum_of_differences
FROM
    DifferenceSums
ORDER BY
    sum_of_differences ASC
LIMIT 3;",,snowflake
75,bq357,noaa_data,"What are the latitude and longitude coordinates and dates between 2005 and 2015 with the top 5 highest daily average wind speeds, excluding records with missing wind speed values? Using data from tables start with prefix ""icoads_core"".","WITH DailyAverages AS (
    SELECT 
        year, month, day, latitude, longitude,
        AVG(wind_speed) AS avg_wind_speed,
    FROM 
        `bigquery-public-data.noaa_icoads.icoads_core_*`
    WHERE
        _TABLE_SUFFIX BETWEEN '2005' AND '2015'
    GROUP BY 
        year, month, day, latitude, longitude
)
SELECT 
    year, month, day, latitude, longitude,
    avg_wind_speed,

FROM 
    DailyAverages
WHERE
    avg_wind_speed IS NOT NULL

ORDER BY avg_wind_speed DESC LIMIT 5",,snowflake
76,bq181,noaa_data,"What percentage of weather stations recorded valid temperature data (with no missing or invalid values) for at least 90% of the days in 2022, where the temperature, maximum, and minimum values are neither NULL nor equal to 9999.9, and the station has a valid identifier (USAF code not equal to '999999'), out of all available stations in the NOAA GSOD database?",,,snowflake
77,bq045,noaa_data,Which weather stations in Washington State recorded more than 150 rainy days in 2023 but fewer rainy days compared to 2022? Defining a “rainy day” as one having precipitation greater than zero millimeters and not equal to 99.99. Only include stations with valid precipitation data.,"WITH WashingtonStations2023 AS 
    (
        SELECT 
            weather.stn AS station_id,
            ANY_VALUE(station.name) AS name
        FROM
            `bigquery-public-data.noaa_gsod.stations` AS station
        INNER JOIN
            `bigquery-public-data.noaa_gsod.gsod2023` AS weather
        ON
            station.usaf = weather.stn
        WHERE
            station.state = 'WA' 
            AND 
            station.usaf != '999999'
        GROUP BY
            station_id
    ),
prcp2023 AS (
SELECT
    washington_stations.name,
    (
        SELECT 
            COUNT(*)
        FROM
            `bigquery-public-data.noaa_gsod.gsod2023` AS weather
        WHERE
            washington_stations.station_id = weather.stn
            AND
            prcp > 0
            AND
            prcp !=99.99
    )
    AS rainy_days
FROM 
    WashingtonStations2023 AS washington_stations
ORDER BY
    rainy_days DESC
),
WashingtonStations2022 AS 
    (
        SELECT 
            weather.stn AS station_id,
            ANY_VALUE(station.name) AS name
        FROM
            `bigquery-public-data.noaa_gsod.stations` AS station
        INNER JOIN
            `bigquery-public-data.noaa_gsod.gsod2022` AS weather
        ON
            station.usaf = weather.stn
        WHERE
            station.state = 'WA' 
            AND 
            station.usaf != '999999'
        GROUP BY
            station_id
    ),
prcp2022 AS (
SELECT
    washington_stations.name,
    (
        SELECT 
            COUNT(*)
        FROM
            `bigquery-public-data.noaa_gsod.gsod2022` AS weather
        WHERE
            washington_stations.station_id = weather.stn
            AND
            prcp > 0
            AND
            prcp != 99.99
    )
    AS rainy_days
FROM 
    WashingtonStations2022 AS washington_stations
ORDER BY
    rainy_days DESC
)

SELECT prcp2023.name
FROM prcp2023
JOIN prcp2022
on prcp2023.name = prcp2022.name
WHERE prcp2023.rainy_days > 150
AND prcp2023.rainy_days < prcp2022.rainy_days",,snowflake
78,sf_bq358,NEW_YORK_CITIBIKE_1,"Can you tell me which bike trip in New York City on July 15, 2015, started and ended in ZIP Code areas with the highest average temperature for that day, as recorded by the Central Park weather station (WBAN '94728')? If there's more than one trip that meets these criteria, I'd like to know about the one that starts in the smallest ZIP Code and ends in the largest ZIP Code. Please return the starting and ending ZIP Codes of this trip.","SELECT
    ""ZIPSTART"".""zip_code"" AS zip_code_start,
    ""ZIPEND"".""zip_code"" AS zip_code_end
FROM  
    ""NEW_YORK_CITIBIKE_1"".""NEW_YORK_CITIBIKE"".""CITIBIKE_TRIPS"" AS ""TRI""
INNER JOIN
    ""NEW_YORK_CITIBIKE_1"".""GEO_US_BOUNDARIES"".""ZIP_CODES"" AS ""ZIPSTART""
    ON ST_WITHIN(
        ST_POINT(""TRI"".""start_station_longitude"", ""TRI"".""start_station_latitude""),
        ST_GEOGFROMWKB(""ZIPSTART"".""zip_code_geom"")
    )
INNER JOIN
    ""NEW_YORK_CITIBIKE_1"".""GEO_US_BOUNDARIES"".""ZIP_CODES"" AS ""ZIPEND""
    ON ST_WITHIN(
        ST_POINT(""TRI"".""end_station_longitude"", ""TRI"".""end_station_latitude""),
        ST_GEOGFROMWKB(""ZIPEND"".""zip_code_geom"")
    )
INNER JOIN
    ""NEW_YORK_CITIBIKE_1"".""NOAA_GSOD"".""GSOD2015"" AS ""WEA""
    ON TO_DATE(TO_CHAR(""WEA"".""year"") || LPAD(TO_CHAR(""WEA"".""mo""), 2, '0') || LPAD(TO_CHAR(""WEA"".""da""), 2, '0'), 'YYYYMMDD') = DATE_TRUNC('DAY', TO_TIMESTAMP_NTZ(TO_NUMBER(""TRI"".""starttime"") / 1000000))
WHERE
    ""WEA"".""wban"" = '94728'
    AND DATE_TRUNC('DAY', TO_TIMESTAMP_NTZ(TO_NUMBER(""TRI"".""starttime"") / 1000000)) = DATE '2015-07-15'
ORDER BY 
    ""WEA"".""temp"" DESC, ""ZIPSTART"".""zip_code"" ASC, ""ZIPEND"".""zip_code"" DESC
LIMIT 1;
","Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
79,bq290,noaa_data,"Can you calculate the difference in maximum temperature, minimum temperature, and average temperature between US and UK weather stations for each day in October 2023, using the date field, and excluding records with missing or invalid temperature values?","with 

stations_selected as (
  select
    usaf,
    wban,
    country,
    name
  from
    `bigquery-public-data.noaa_gsod.stations`
  where
    country in ('US', 'UK')
),

data_filtered as (
  select
    gsod.*,
    stations.country
  from
    `bigquery-public-data.noaa_gsod.gsod2023` gsod
  join
    stations_selected stations
  on
    gsod.stn = stations.usaf
    and gsod.wban = stations.wban
  where
    date(gsod.date) between '2023-10-01' and '2023-10-31'
    and gsod.temp != 9999.9
),

-- US Metrics
us_metrics as (
  select
    date(date) as metric_date,
    avg(temp) as avg_temp_us,
    min(temp) as min_temp_us,
    max(temp) as max_temp_us
  from
    data_filtered
  where
    country = 'US'
  group by
    metric_date
),

-- UK Metrics
uk_metrics as (
  select
    date(date) as metric_date,
    avg(temp) as avg_temp_uk,
    min(temp) as min_temp_uk,
    max(temp) as max_temp_uk
  from
    data_filtered
  where
    country = 'UK'
  group by
    metric_date
),

-- Temperature Differences
temp_differences as (
  select
    us.metric_date,
    us.max_temp_us - uk.max_temp_uk as max_temp_diff,
    us.min_temp_us - uk.min_temp_uk as min_temp_diff,
    us.avg_temp_us - uk.avg_temp_uk as avg_temp_diff
  from
    us_metrics us
  join
    uk_metrics uk
  on
    us.metric_date = uk.metric_date
)

select 
  metric_date, 
  max_temp_diff, 
  min_temp_diff, 
  avg_temp_diff
from 
  temp_differences
order by
  metric_date;",,snowflake
80,bq031,noaa_data,"Provide the daily weather data for Rochester from January 1 to March 31, 2019, including temperature (in Celsius), precipitation (in centimeters), and wind speed (in meters per second). For each variable, calculate the 8-day moving average (including the current day and the previous 7 days). Also, calculate the difference between the moving average on each day and the moving averages for the previous 1 to 8 days (i.e., lag1 to lag8). The result should include: The daily values for temperature, precipitation, and wind speed.The 8-day moving averages for each variable. The differences between the moving averages for each of the previous 1 to 8 days (e.g., the difference between today's moving average and the moving average from 1 day ago, from 2 days ago, and so on). Round all values to one decimal place. The data should be ordered by date, starting from January 9, 2019.","WITH transrate AS (
    SELECT
        DATE(CAST(year AS INT64), CAST(mo AS INT64), CAST(da AS INT64)) AS observation_date
        , ROUND((temp - 32.0) / 1.8, 1) AS temp_mean_c -- using Celsius instead of Fahrenheit
        , ROUND(prcp * 2.54, 1) AS prcp_cm -- from inches to centimeters
        , ROUND(CAST(wdsp AS FLOAT64) * 1.852 / 3.6, 1) AS wdsp_ms -- from knots to meters per second
    FROM `bigquery-public-data.noaa_gsod.gsod*`
    WHERE _TABLE_SUFFIX = ""2019""
        AND CAST(mo AS INT64) <= 3
        AND stn in (SELECT usaf FROM `bigquery-public-data.noaa_gsod.stations` WHERE name = ""ROCHESTER"")
),

moving_avg AS (
    SELECT
        observation_date
        , temp_mean_c
        , prcp_cm
        , wdsp_ms
        , AVG(temp_mean_c) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS temp_moving_avg
        , AVG(prcp_cm) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS prcp_moving_avg
        , AVG(wdsp_ms) OVER (ORDER BY observation_date ROWS 7 PRECEDING) AS wdsp_moving_avg
    FROM transrate
),

lag_moving_avg AS (
    SELECT
        observation_date
        , temp_mean_c
        , prcp_cm
        , wdsp_ms
        , LAG(temp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_temp_moving_avg
        , LAG(prcp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_prcp_moving_avg
        , LAG(wdsp_moving_avg, 1) OVER (ORDER BY observation_date) AS lag1_wdsp_moving_avg

        , LAG(temp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_temp_moving_avg
        , LAG(prcp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_prcp_moving_avg
        , LAG(wdsp_moving_avg, 2) OVER (ORDER BY observation_date) AS lag2_wdsp_moving_avg

        , LAG(temp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_temp_moving_avg
        , LAG(prcp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_prcp_moving_avg
        , LAG(wdsp_moving_avg, 3) OVER (ORDER BY observation_date) AS lag3_wdsp_moving_avg

        , LAG(temp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_temp_moving_avg
        , LAG(prcp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_prcp_moving_avg
        , LAG(wdsp_moving_avg, 4) OVER (ORDER BY observation_date) AS lag4_wdsp_moving_avg

        , LAG(temp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_temp_moving_avg
        , LAG(prcp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_prcp_moving_avg
        , LAG(wdsp_moving_avg, 5) OVER (ORDER BY observation_date) AS lag5_wdsp_moving_avg

        , LAG(temp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_temp_moving_avg
        , LAG(prcp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_prcp_moving_avg
        , LAG(wdsp_moving_avg, 6) OVER (ORDER BY observation_date) AS lag6_wdsp_moving_avg

        , LAG(temp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_temp_moving_avg
        , LAG(prcp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_prcp_moving_avg
        , LAG(wdsp_moving_avg, 7) OVER (ORDER BY observation_date) AS lag7_wdsp_moving_avg

        , LAG(temp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_temp_moving_avg
        , LAG(prcp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_prcp_moving_avg
        , LAG(wdsp_moving_avg, 8) OVER (ORDER BY observation_date) AS lag8_wdsp_moving_avg
    FROM moving_avg
)

SELECT
    observation_date
    , temp_mean_c
    , prcp_cm
    , wdsp_ms

    , ROUND(lag1_temp_moving_avg, 1) AS lag1_temp_moving_avg
    , ROUND(lag1_prcp_moving_avg, 1) AS lag1_prcp_moving_avg
    , ROUND(lag1_wdsp_moving_avg, 1) AS lag1_wdsp_moving_avg
    
    , ROUND(lag1_temp_moving_avg - lag2_temp_moving_avg, 1) AS diff2_temp_moving_avg
    , ROUND(lag1_prcp_moving_avg - lag2_prcp_moving_avg, 1) AS diff2_prcp_moving_avg
    , ROUND(lag1_wdsp_moving_avg - lag2_wdsp_moving_avg, 1) AS diff2_wdsp_moving_avg
    , ROUND(lag2_temp_moving_avg, 1) AS lag2_temp_moving_avg
    , ROUND(lag2_prcp_moving_avg, 1) AS lag2_prcp_moving_avg
    , ROUND(lag2_wdsp_moving_avg, 1) AS lag2_wdsp_moving_avg
    
    , ROUND(lag2_temp_moving_avg - lag3_temp_moving_avg, 1) AS diff3_temp_moving_avg
    , ROUND(lag2_prcp_moving_avg - lag3_prcp_moving_avg, 1) AS diff3_prcp_moving_avg
    , ROUND(lag2_wdsp_moving_avg - lag3_wdsp_moving_avg, 1) AS diff3_wdsp_moving_avg
    , ROUND(lag3_temp_moving_avg, 1) AS lag3_temp_moving_avg
    , ROUND(lag3_prcp_moving_avg, 1) AS lag3_prcp_moving_avg
    , ROUND(lag3_wdsp_moving_avg, 1) AS lag3_wdsp_moving_avg
    
    , ROUND(lag3_temp_moving_avg - lag4_temp_moving_avg, 1) AS diff4_temp_moving_avg
    , ROUND(lag3_prcp_moving_avg - lag4_prcp_moving_avg, 1) AS diff4_prcp_moving_avg
    , ROUND(lag3_wdsp_moving_avg - lag4_wdsp_moving_avg, 1) AS diff4_wdsp_moving_avg
    , ROUND(lag4_temp_moving_avg, 1) AS lag4_temp_moving_avg
    , ROUND(lag4_prcp_moving_avg, 1) AS lag4_prcp_moving_avg
    , ROUND(lag4_wdsp_moving_avg, 1) AS lag4_wdsp_moving_avg
    
    , ROUND(lag4_temp_moving_avg - lag5_temp_moving_avg, 1) AS diff5_temp_moving_avg
    , ROUND(lag4_prcp_moving_avg - lag5_prcp_moving_avg, 1) AS diff5_prcp_moving_avg
    , ROUND(lag4_wdsp_moving_avg - lag5_wdsp_moving_avg, 1) AS diff5_wdsp_moving_avg
    , ROUND(lag5_temp_moving_avg, 1) AS lag5_temp_moving_avg
    , ROUND(lag5_prcp_moving_avg, 1) AS lag5_prcp_moving_avg
    , ROUND(lag5_wdsp_moving_avg, 1) AS lag5_wdsp_moving_avg
    
    , ROUND(lag5_temp_moving_avg - lag6_temp_moving_avg, 1) AS diff6_temp_moving_avg
    , ROUND(lag5_prcp_moving_avg - lag6_prcp_moving_avg, 1) AS diff6_prcp_moving_avg
    , ROUND(lag5_wdsp_moving_avg - lag6_wdsp_moving_avg, 1) AS diff6_wdsp_moving_avg
    , ROUND(lag6_temp_moving_avg, 1) AS lag6_temp_moving_avg
    , ROUND(lag6_prcp_moving_avg, 1) AS lag6_prcp_moving_avg
    , ROUND(lag6_wdsp_moving_avg, 1) AS lag6_wdsp_moving_avg
    
    , ROUND(lag6_temp_moving_avg - lag7_temp_moving_avg, 1) AS diff7_temp_moving_avg
    , ROUND(lag6_prcp_moving_avg - lag7_prcp_moving_avg, 1) AS diff7_prcp_moving_avg
    , ROUND(lag6_wdsp_moving_avg - lag7_wdsp_moving_avg, 1) AS diff7_wdsp_moving_avg
    , ROUND(lag7_temp_moving_avg, 1) AS lag7_temp_moving_avg
    , ROUND(lag7_prcp_moving_avg, 1) AS lag7_prcp_moving_avg
    , ROUND(lag7_wdsp_moving_avg, 1) AS lag7_wdsp_moving_avg
    
    , ROUND(lag7_temp_moving_avg - lag8_temp_moving_avg, 1) AS diff8_temp_moving_avg
    , ROUND(lag7_prcp_moving_avg - lag8_prcp_moving_avg, 1) AS diff8_prcp_moving_avg
    , ROUND(lag7_wdsp_moving_avg - lag8_wdsp_moving_avg, 1) AS diff8_wdsp_moving_avg
    , ROUND(lag8_temp_moving_avg, 1) AS lag8_temp_moving_avg
    , ROUND(lag8_prcp_moving_avg, 1) AS lag8_prcp_moving_avg
    , ROUND(lag8_wdsp_moving_avg, 1) AS lag8_wdsp_moving_avg
FROM lag_moving_avg
WHERE
  lag8_temp_moving_avg IS NOT NULL
ORDER BY observation_date;
-- all result rounded to 1 decimal place",,snowflake
81,bq392,noaa_gsod,"What are the top 3 dates in October 2009 with the highest average temperature for station number 723758, in the format YYYY-MM-DD?","WITH
  # FIRST CAST EACH YEAR, MONTH, DATE TO STRINGS
  T AS (
    SELECT
      *,
      CAST(year AS STRING) AS year_string,
      CAST(mo AS STRING) AS month_string,
      CAST(da AS STRING) AS day_string
    FROM
      `bigquery-public-data.noaa_gsod.gsod2009`
    WHERE
      stn = ""723758""
  ),

  # SECOND, CONCAT ALL THE STRINGS TOGETHER INTO ONE COLUMN
  TT AS (
    SELECT
      *,
      CONCAT(year_string, ""-"", month_string, ""-"", day_string) AS date_string
    FROM
      T
  ),

  # THIRD, CAST THE DATE STRING INTO A DATE FORMAT
  TTT AS (
    SELECT
      *,
      CAST(date_string AS DATE) AS date_date
    FROM
      TT
  ),

  # FOURTH, CALCULATE THE MEAN TEMPERATURE FOR EACH DATE
  Temp_Avg AS (
    SELECT
      date_date,
      AVG(temp) AS avg_temp
    FROM
      TTT
    WHERE
      date_date BETWEEN '2009-10-01' AND '2009-10-31'
    GROUP BY
      date_date
  )

# FINAL SELECTION OF TOP 3 DATES WITH HIGHEST MEAN TEMPERATURE
SELECT
  date_date AS dates
FROM
  Temp_Avg
ORDER BY
  avg_temp DESC
LIMIT 3;",,snowflake
82,sf_bq050,NEW_YORK_CITIBIKE_1,"I want to analyze bike trips in New York City for 2014 by linking trip data with weather information to understand how weather conditions (temperature, wind speed, and precipitation) affect bike trips between neighborhoods. For each combination of starting and ending neighborhoods, I need the following: 1. Total number of bike trips between the neighborhoods. 2. Average trip duration in minutes (rounded to 1 decimal). 3. Average temperature at the start of the trip (rounded to 1 decimal). 4. Average wind speed at the start (in meters per second, rounded to 1 decimal). 5. Average precipitation at the start (in centimeters, rounded to 1 decimal). 6. The month with the most trips (e.g., `4` for April). The data should be grouped by the starting and ending neighborhoods, with:`zip_codes` in `geo_us_boundaries` used to map the bike trip locations based on latitude and longitude. `zip_codes` in `cyclistic` used to obtain the borough and neighborhood names. Using weather data from the Central Park station for the trip date, covering all trips in 2014.","WITH data AS (
    SELECT
        ""ZIPSTARTNAME"".""borough"" AS ""borough_start"",
        ""ZIPSTARTNAME"".""neighborhood"" AS ""neighborhood_start"",
        ""ZIPENDNAME"".""borough"" AS ""borough_end"",
        ""ZIPENDNAME"".""neighborhood"" AS ""neighborhood_end"",
        CAST(""TRI"".""tripduration"" / 60 AS NUMERIC) AS ""trip_minutes"",
        ""WEA"".""temp"" AS ""temperature"",
        CAST(""WEA"".""wdsp"" AS NUMERIC) AS ""wind_speed"",
        ""WEA"".""prcp"" AS ""precipitation"",
        EXTRACT(MONTH FROM DATE(""TRI"".""starttime"")) AS ""start_month""
    FROM
        ""NEW_YORK_CITIBIKE_1"".""NEW_YORK_CITIBIKE"".""CITIBIKE_TRIPS"" AS ""TRI""
    INNER JOIN
        ""NEW_YORK_CITIBIKE_1"".""GEO_US_BOUNDARIES"".""ZIP_CODES"" AS ""ZIPSTART""
        ON ST_WITHIN(
            ST_POINT(""TRI"".""start_station_longitude"", ""TRI"".""start_station_latitude""),
            ST_GEOGFROMWKB(""ZIPSTART"".""zip_code_geom"")
        )
    INNER JOIN
        ""NEW_YORK_CITIBIKE_1"".""GEO_US_BOUNDARIES"".""ZIP_CODES"" AS ""ZIPEND""
        ON ST_WITHIN(
            ST_POINT(""TRI"".""end_station_longitude"", ""TRI"".""end_station_latitude""),
            ST_GEOGFROMWKB(""ZIPEND"".""zip_code_geom"")
        )
    INNER JOIN
        ""NEW_YORK_CITIBIKE_1"".""NOAA_GSOD"".""GSOD2014"" AS ""WEA""
        ON TO_DATE(CONCAT(""WEA"".""year"", LPAD(""WEA"".""mo"", 2, '0'), LPAD(""WEA"".""da"", 2, '0')), 'YYYYMMDD') = DATE(""TRI"".""starttime"")
    INNER JOIN
        ""NEW_YORK_CITIBIKE_1"".""CYCLISTIC"".""ZIP_CODES"" AS ""ZIPSTARTNAME""
        ON ""ZIPSTART"".""zip_code"" = CAST(""ZIPSTARTNAME"".""zip"" AS STRING)
    INNER JOIN
        ""NEW_YORK_CITIBIKE_1"".""CYCLISTIC"".""ZIP_CODES"" AS ""ZIPENDNAME""
        ON ""ZIPEND"".""zip_code"" = CAST(""ZIPENDNAME"".""zip"" AS STRING)
    WHERE
        ""WEA"".""wban"" = (
            SELECT ""wban"" 
            FROM ""NEW_YORK_CITIBIKE_1"".""NOAA_GSOD"".""STATIONS""
            WHERE
                ""state"" = 'NY'
                AND LOWER(""name"") LIKE LOWER('%New York Central Park%')
            LIMIT 1
        )
        AND EXTRACT(YEAR FROM DATE(""TRI"".""starttime"")) = 2014
),
agg_data AS (
    SELECT
        ""borough_start"",
        ""neighborhood_start"",
        ""borough_end"",
        ""neighborhood_end"",
        COUNT(*) AS ""num_trips"",
        ROUND(AVG(""trip_minutes""), 1) AS ""avg_trip_minutes"",
        ROUND(AVG(""temperature""), 1) AS ""avg_temperature"",
        ROUND(AVG(""wind_speed""), 1) AS ""avg_wind_speed"",
        ROUND(AVG(""precipitation""), 1) AS ""avg_precipitation""
    FROM data
    GROUP BY
        ""borough_start"",
        ""neighborhood_start"",
        ""borough_end"",
        ""neighborhood_end""
),
most_common_months AS (
    SELECT
        ""borough_start"",
        ""neighborhood_start"",
        ""borough_end"",
        ""neighborhood_end"",
        ""start_month"",
        ROW_NUMBER() OVER (
            PARTITION BY ""borough_start"", ""neighborhood_start"", ""borough_end"", ""neighborhood_end"" 
            ORDER BY COUNT(*) DESC
        ) AS ""row_num""
    FROM data
    GROUP BY
        ""borough_start"",
        ""neighborhood_start"",
        ""borough_end"",
        ""neighborhood_end"",
        ""start_month""
)

SELECT
    a.*,
    m.""start_month"" AS ""most_common_month""
FROM
    agg_data a
JOIN
    most_common_months m
    ON a.""borough_start"" = m.""borough_start"" 
    AND a.""neighborhood_start"" = m.""neighborhood_start"" 
    AND a.""borough_end"" = m.""borough_end"" 
    AND a.""neighborhood_end"" = m.""neighborhood_end"" 
    AND m.""row_num"" = 1
ORDER BY 
    a.""neighborhood_start"", 
    a.""neighborhood_end"";","Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
83,sf_bq426,NEW_YORK_CITIBIKE_1,"What user type recorded the highest average temperature for trips starting and ending in New York City's zip code 10019 during 2018? Include average precipitation, wind speed, and temperature for that user type based on weather data from the New York Central Park station.",,"Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
84,sf_bq291,NOAA_GLOBAL_FORECAST_SYSTEM,"Can you provide a daily weather summary for July 2019 within a 5 km radius of latitude 26.75 and longitude 51.5? I need the maximum, minimum, and average temperatures; total precipitation; average cloud cover between 10 AM and 5 PM; total snowfall (when average temperature is below 32°F); and total rainfall (when average temperature is 32°F or above) for each forecast date. The data should correspond to forecasts created in July 2019 for the following day.","WITH daily_forecasts AS (
    SELECT
        ""TRI"".""creation_time"",

        CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(""forecast"".value:""time"") / 1000000)) AS DATE) AS ""local_forecast_date"",
        MAX(
            CASE 
                WHEN ""forecast"".value:""temperature_2m_above_ground"" IS NOT NULL 
                THEN ""forecast"".value:""temperature_2m_above_ground"" 
                ELSE NULL 
            END
        ) AS ""max_temp"",
        MIN(
            CASE 
                WHEN ""forecast"".value:""temperature_2m_above_ground"" IS NOT NULL 
                THEN ""forecast"".value:""temperature_2m_above_ground"" 
                ELSE NULL 
            END
        ) AS ""min_temp"",
        AVG(
            CASE 
                WHEN ""forecast"".value:""temperature_2m_above_ground"" IS NOT NULL 
                THEN ""forecast"".value:""temperature_2m_above_ground"" 
                ELSE NULL 
            END
        ) AS ""avg_temp"",
        SUM(
            CASE 
                WHEN ""forecast"".value:""total_precipitation_surface"" IS NOT NULL 
                THEN ""forecast"".value:""total_precipitation_surface"" 
                ELSE 0 
            END
        ) AS ""total_precipitation"",
        AVG(
            CASE 
                WHEN CAST(DATEADD(hour, 1, TO_TIMESTAMP_NTZ(TO_NUMBER(""forecast"".value:""time"") / 1000000)    ) AS TIME) BETWEEN '10:00:00' AND '17:00:00'
                     AND ""forecast"".value:""total_cloud_cover_entire_atmosphere"" IS NOT NULL 
                THEN ""forecast"".value:""total_cloud_cover_entire_atmosphere"" 
                ELSE NULL 
            END
        ) AS ""avg_cloud_cover"",
        CASE
            WHEN AVG(""forecast"".value:""temperature_2m_above_ground"") < 32 THEN 
                SUM(
                    CASE 
                        WHEN ""forecast"".value:""total_precipitation_surface"" IS NOT NULL 
                        THEN ""forecast"".value:""total_precipitation_surface"" 
                        ELSE 0 
                    END
                )
            ELSE 0
        END AS ""total_snow"",
        CASE
            WHEN AVG(""forecast"".value:""temperature_2m_above_ground"") >= 32 THEN 
                SUM(
                    CASE 
                        WHEN ""forecast"".value:""total_precipitation_surface"" IS NOT NULL 
                        THEN ""forecast"".value:""total_precipitation_surface"" 
                        ELSE 0 
                    END
                )
            ELSE 0
        END AS ""total_rain""
    FROM
        ""NOAA_GLOBAL_FORECAST_SYSTEM"".""NOAA_GLOBAL_FORECAST_SYSTEM"".""NOAA_GFS0P25"" AS ""TRI""
    CROSS JOIN LATERAL FLATTEN(input => ""TRI"".""forecast"") AS ""forecast""
    WHERE
        TO_TIMESTAMP_NTZ(TO_NUMBER(""TRI"".""creation_time"") / 1000000) BETWEEN '2019-07-01' AND '2021-07-31'  
        AND ST_DWITHIN(
            ST_GEOGFROMWKB(""TRI"".""geography""),
            ST_POINT(26.75, 51.5),
            5000
        )
        AND CAST(TO_TIMESTAMP_NTZ(TO_NUMBER(""forecast"".value:""time"") / 1000000) AS DATE) = DATEADD(day, 1, CAST( TO_TIMESTAMP_NTZ(TO_NUMBER(""TRI"".""creation_time"") / 1000000) AS DATE))
    GROUP BY
        ""TRI"".""creation_time"",
        ""local_forecast_date""
)

SELECT
    TO_TIMESTAMP_NTZ(TO_NUMBER(""creation_time"") / 1000000),
    ""local_forecast_date"" AS ""forecast_date"",
    ""max_temp"",
    ""min_temp"",
    ""avg_temp"",
    ""total_precipitation"",
    ""avg_cloud_cover"",
    ""total_snow"",
    ""total_rain""
FROM
    daily_forecasts
ORDER BY
    ""creation_time"",
    ""forecast_date"";
","Categories: Geospatial functions


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+",snowflake
85,bq208,new_york_noaa,"Can you provide weather stations within a 20-mile radius of Chappaqua, New York (Latitude: 41.197, Longitude: -73.764), and tell me the number of valid temperature observations they have recorded from 2011 to 2020, excluding any invalid or missing temperature data?",,"Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
86,bq047,new_york_noaa,"Could you analyze the relationship between each complaint type and daily temperature in New York City, specifically using temperature data from LaGuardia (STN=725030) and JFK (STN=744860) airports for the 10 years starting in 2008, and then determine, for each complaint type that has more than 5000 total occurrences and shows a strong correlation (absolute value > 0.5) with temperature, the total number of complaints, the total number of days with valid temperature records, and the Pearson correlation coefficients (rounded to four decimals) between temperature and both the daily complaint count as well as the daily percentage of total complaints, excluding any days with missing or invalid temperature data (such as 9999.9)?",,,snowflake
87,bq048,new_york_noaa,"Which complaint types with more than 3000 total requests from 2011 to 2020 show the strongest positive and negative Pearson correlations with the daily average wind speed measured at station 744860 (JFK Airport), based on daily complaint proportions (the ratio of type-specific complaints to total daily complaints)? Please provide the complaint types and their correlation coefficients, rounded to four decimal places.",,,snowflake
88,bq293,new_york_geo,"I want to analyze New York City yellow taxi trip data specifically for January 1, 2015, using the bigquery-public-data.new_york.tlc_yellow_trips_2015 dataset. I need to join this with the geo_us_boundaries.zip_codes table to map each trip to its respective NYC zip code based on the pickup coordinates. The analysis should create a complete hour-by-hour breakdown for all zip codes by cross-joining distinct zip codes with distinct hours, including hours with zero trips. For each zip code and hour combination, calculate the total number of trips and the following time-based metrics: count of trips from 1 hour ago, 1 day (24 hours) ago, 7 days (168 hours) ago, and 14 days (336 hours) ago. Additionally, compute the 14-day and 21-day moving averages and standard deviations of trip counts, excluding the current hour. Only include trips with valid latitude and longitude coordinates. The final results should be sorted by the highest trip counts, showing only the top 5 groups with the most trips.",,"Categories: Geospatial functions


## ST_CONTAINS

Returns TRUE if a GEOGRAPHY or GEOMETRY object is completely inside another object of the same type.
More strictly, object g1 contains object g2 if and only if no points of g2 lie in the exterior of g1, and at least one point of the interior of B lies in the interior of A. There are certain subtleties in this definition that are not immediately obvious. For more details on what “contains” means, see the Dimensionally Extended 9-Intersection Model (DE-9IM).
Although ST_COVERS and ST_CONTAINS might seem similar, the two functions have subtle differences. For details on the differences between “covers” and “contains”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_WITHIN , ST_COVERS , ST_COVEREDBY


## Syntax

ST_CONTAINS( <geography_expression_1> , <geography_expression_2> )

ST_CONTAINS( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Usage notes


For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_CONTAINS function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_CONTAINS(g1, g2) 
    FROM geospatial_table_01;
+---------------------+
| ST_CONTAINS(G1, G2) |
|---------------------|
| True                |
+---------------------+



## GEOMETRY examples

The query below shows several examples of using ST_CONTAINS. Note how ST_CONTAINS determines that:

The Polygon contains itself.
The Polygon does not contain the LineString that is on its border.
SELECT ST_CONTAINS(poly, poly_inside),
      ST_CONTAINS(poly, poly),
      ST_CONTAINS(poly, line_on_boundary),
      ST_CONTAINS(poly, line_inside)
  FROM (SELECT
    TO_GEOMETRY('POLYGON((-2 0, 0 2, 2 0, -2 0))') AS poly,
    TO_GEOMETRY('POLYGON((-1 0, 0 1, 1 0, -1 0))') AS poly_inside,
    TO_GEOMETRY('LINESTRING(-1 1, 0 2, 1 1)') AS line_on_boundary,
    TO_GEOMETRY('LINESTRING(-2 0, 0 0, 0 1)') AS line_inside);

+--------------------------------+------------------------+------------------------------------+-------------------------------+
| ST_CONTAINS(POLY, POLY_INSIDE) | ST_CONTAINS(POLY,POLY) | ST_CONTAINS(POLY,LINE_ON_BOUNDARY) | ST_CONTAINS(POLY,LINE_INSIDE) |
|--------------------------------+------------------------+------------------------------------+-------------------------------|
| True                           | True                   | False                              | True                          |
+--------------------------------+------------------------+------------------------------------+-------------------------------+",snowflake
89,sf_bq017,GEO_OPENSTREETMAP,"What are the five longest types of highways within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35') by total length, analyzed through planet features?","WITH bounding_area AS (
    SELECT ""geometry"" AS geometry
    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,
    LATERAL FLATTEN(INPUT => planet_features.""all_tags"") AS ""tag""
    WHERE ""feature_type"" = 'multipolygons'
      AND ""tag"".value:""key"" = 'wikidata'
      AND ""tag"".value:""value"" = 'Q35'
),

highway_info AS (
    SELECT 
        SUM(ST_LENGTH(
                ST_GEOGRAPHYFROMWKB(planet_features.""geometry"")
            )
        ) AS highway_length,
        ""tag"".value:""value"" AS highway_type
    FROM 
        GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,
        bounding_area
    CROSS JOIN LATERAL FLATTEN(INPUT => planet_features.""all_tags"") AS ""tag""
    WHERE ""tag"".value:""key"" = 'highway'
    AND ""feature_type"" = 'lines'
    AND ST_DWITHIN(
        ST_GEOGFROMWKB(planet_features.""geometry""), 
        ST_GEOGFROMWKB(bounding_area.geometry),
        0.0
    ) 
    GROUP BY highway_type
)

SELECT 
  REPLACE(highway_type, '""', '') AS highway_type
FROM
  highway_info
ORDER BY 
  highway_length DESC
LIMIT 5;
","Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
90,sf_bq131,GEO_OPENSTREETMAP,"What is the number of bus stops for the bus network with the most stops within the multipolygon boundary of Denmark (as defined by Wikidata ID 'Q35'), analyzed through planet features?",,"Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
91,sf_bq349,GEO_OPENSTREETMAP,"Which OpenStreetMap ID from the planet features table corresponds to an administrative boundary, represented as multipolygons, whose total number of 'amenity'-tagged Points of Interest (POIs), as derived from the planet nodes table, is closest to the median count among all such boundaries?","WITH bounding_area AS (
    SELECT 
        ""osm_id"",
        ""geometry"" AS geometry,
        ST_AREA(ST_GEOGRAPHYFROMWKB(""geometry"")) AS area
    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,
    LATERAL FLATTEN(INPUT => PLANET_FEATURES.""all_tags"") AS ""tag""
    WHERE 
        ""feature_type"" = 'multipolygons'
        AND ""tag"".value:""key"" = 'boundary'
        AND ""tag"".value:""value"" = 'administrative'
),

poi AS (
    SELECT 
        nodes.""id"" AS poi_id,
        nodes.""geometry"" AS poi_geometry,
        tags.value:""value"" AS poitype
    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_NODES AS nodes,
    LATERAL FLATTEN(INPUT => nodes.""all_tags"") AS tags
    WHERE tags.value:""key"" = 'amenity'
),

poi_counts AS (
    SELECT
        ba.""osm_id"",
        COUNT(poi.poi_id) AS total_pois
    FROM bounding_area ba
    JOIN poi
    ON ST_DWITHIN(
        ST_GEOGRAPHYFROMWKB(ba.geometry), 
        ST_GEOGRAPHYFROMWKB(poi.poi_geometry), 
        0.0
    )
    GROUP BY ba.""osm_id""
),

median_value AS (
    SELECT 
        PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY total_pois) AS median_pois
    FROM poi_counts
),

closest_to_median AS (
    SELECT
        ""osm_id"",
        total_pois,
        ABS(total_pois - (SELECT median_pois FROM median_value)) AS diff_from_median
    FROM poi_counts
)

SELECT
    ""osm_id""
FROM closest_to_median
ORDER BY diff_from_median
LIMIT 1;
","Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
92,sf_bq007,CENSUS_BUREAU_ACS_2,"Identify the top 10 U.S. states with the highest vulnerable population, calculated based on a weighted sum of employment sectors using 2017 ACS 5-Year data, and determine their average median income change from 2015 to 2018 using zip code data. ",,"# Calculation of Total Vulnerable Population

## Objective
Calculate the total vulnerable population based on a weighted sum of employment in various sectors, along with the average median income change between 2015 and 2018 for U.S. states.

## Sectors and Weights
The total vulnerable population is calculated using the following sectors and their corresponding weights:

| Sector                                                                  | Weight          |
|-------------------------------------------------------------------------|-----------------|
| Wholesale Trade                                                          | 0.38423645320197042 |
| Natural Resources, Construction, and Maintenance                         | 0.48071410777129553 |
| Arts, Entertainment, Recreation, Accommodation, and Food               | 0.89455676291236841 |
| Information                                                              | 0.31315240083507306 |
| Retail Trade                                                             | 0.51            |
| Public Administration                                                    | 0.039299298394228743 |
| Services                                                                 | 0.36555534476489654 |
| Education, Health, and Social Services                                   | 0.20323178400562944 |
| Transportation, Warehousing, and Utilities                               | 0.3680506593618087  |
| Manufacturing                                                            | 0.40618955512572535 |",snowflake
93,sf_bq429,CENSUS_BUREAU_ACS_2,"Which are the top five states with the greatest average difference in median income between 2015 and 2018 at the ZIP code level, and what is the corresponding average number of vulnerable employees across wholesale trade, natural resources and construction, arts and entertainment, information, and retail trade industries in 2017 according to the ACS Five-Year Estimates and ZIP code boundaries data?","WITH median_income_diff_by_zipcode AS (
  WITH acs_2018 AS (
    SELECT
      ""geo_id"",
      ""median_income"" AS ""median_income_2018""
    FROM
      CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.""ZIP_CODES_2018_5YR""
  ),
  acs_2015 AS (
    SELECT
      ""geo_id"",
      ""median_income"" AS ""median_income_2015""
    FROM
      CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.""ZIP_CODES_2015_5YR""
  ),
  acs_diff AS (
    SELECT
      a18.""geo_id"",
      (a18.""median_income_2018"" - a15.""median_income_2015"") AS ""median_income_diff""
    FROM
      acs_2018 a18
    JOIN
      acs_2015 a15 ON a18.""geo_id"" = a15.""geo_id""
  )
  SELECT
    ""geo_id"",
    AVG(""median_income_diff"") AS ""avg_median_income_diff""
  FROM
    acs_diff
  WHERE
    ""median_income_diff"" IS NOT NULL
  GROUP BY ""geo_id""
),
base_census AS (
  SELECT
    geo.""state_name"",
    AVG(i.""avg_median_income_diff"") AS ""avg_median_income_diff"",
    AVG(
      ""employed_wholesale_trade"" * 0.38423645320197042 +
      ""occupation_natural_resources_construction_maintenance"" * 0.48071410777129553 +
      ""employed_arts_entertainment_recreation_accommodation_food"" * 0.89455676291236841 +
      ""employed_information"" * 0.31315240083507306 +
      ""employed_retail_trade"" * 0.51
    ) AS ""avg_vulnerable""
  FROM
    CENSUS_BUREAU_ACS_2.CENSUS_BUREAU_ACS.""ZIP_CODES_2017_5YR"" AS census
  JOIN
    median_income_diff_by_zipcode i ON CAST(census.""geo_id"" AS STRING) = i.""geo_id""
  JOIN
    CENSUS_BUREAU_ACS_2.GEO_US_BOUNDARIES.""ZIP_CODES"" geo ON census.""geo_id"" = geo.""zip_code""
  GROUP BY geo.""state_name""
)

SELECT 
  ""state_name"",
  ""avg_median_income_diff"",
  ""avg_vulnerable""
FROM 
  base_census
ORDER BY 
  ""avg_median_income_diff"" DESC
LIMIT 5;
","# Calculation of Average Vulnerable Population

This document outlines the method for calculating the average vulnerable population across various industries based on their employment and vulnerability weights.

## Variables

- Let:
  - wholesale_trade = Employment in Wholesale Trade
  - natural_resources_construction = Employment in Natural Resources and Construction
  - arts_entertainment_recreation = Employment in Arts, Entertainment, and Recreation
  - information = Employment in Information
  - retail = Employment in Retail Trade

## Weights

Each industry has a corresponding vulnerability weight:
- wholesale_trade = 0.38423645320197042 
- natural_resources_construction = 0.48071410777129553 
- arts_entertainment_recreation = 0.89455676291236841
- information = 0.31315240083507306 
- retail = 0.51
",snowflake
94,sf_bq073,CENSUS_BUREAU_ACS_2,"Using data on ZIP-level median income differences between 2015 and 2018, along with the 2017 ACS employment figures, list each state in descending order of total vulnerable workers, where “vulnerable” is defined as 38% of wholesale trade employees and 41% of manufacturing employees in 2017. Your results should include the state name, the number of vulnerable wholesale trade workers, the number of vulnerable manufacturing workers, and the combined total of these vulnerable workers.",,,snowflake
95,sf_bq410,CENSUS_BUREAU_ACS_2,"Find the top 3 states with the smallest adjusted non-labor force population using 2017 ACS tract-level data. Calculate the adjusted non-labor force population as (unemployed_pop + not_in_labor_force - group_quarters), clamping any negative values to zero. For each of these states, display the state abbreviation, the total median income change between 2015 and 2018 across all tracts, the total adjusted non-labor force population, and the average population-adjusted proportion (calculated as the ratio of adjusted non-labor force to total population at the tract level). Join tract data to states using FIPS code prefixes, exclude any tracts with null income differences, and sort results by the adjusted non-labor force population in ascending order.",,,snowflake
96,sf_bq348,GEO_OPENSTREETMAP,"Within the rectangular area defined by the geogpoints (31.1798246, 18.4519921), (54.3798246, 18.4519921), (54.3798246, 33.6519921), and (31.1798246, 33.6519921), which are the top three usernames responsible for the highest number of historical nodes, originally tagged with the amenities ‘hospital’, ‘clinic’, or ‘doctors’, that do not appear anymore in the current planet_nodes dataset?",,"Categories: Geospatial functions


## ST_INTERSECTS

Returns TRUE if the two GEOGRAPHY objects or the two GEOMETRY objects intersect (i.e. share any portion of space).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_DISJOINT


## Syntax

ST_INTERSECTS( <geography_expression_1> , <geography_expression_2> )

ST_INTERSECTS( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object.

geography_expression_2A GEOGRAPHY object.

geometry_expression_1A GEOMETRY object.

geometry_expression_2A GEOMETRY object.


## Returns

BOOLEAN.

## Usage notes


For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
    TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'),
    TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')
    );
+---------------------------------------------------------+
| ST_INTERSECTS(                                          |
|     TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'), |
|     TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
|     )                                                   |
|---------------------------------------------------------|
| True                                                    |
+---------------------------------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
  TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'),
  TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))') );

+------------------------------------------------------+
| ST_INTERSECTS(                                       |
|   TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'), |
|   TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
| )                                                    |
|------------------------------------------------------|
| True                                                 |
+------------------------------------------------------+




## ST_MAKEPOLYGON , ST_POLYGON

Constructs a GEOGRAPHY or GEOMETRY object that represents a Polygon without holes. The function uses the specified LineString as the outer loop.
This function corrects the orientation of the loop to prevent the creation of Polygons that span more than half of the globe. In contrast, ST_MAKEPOLYGONORIENTED does not attempt to correct the orientation of the loop.

See also:TO_GEOGRAPHY , TO_GEOMETRY , ST_MAKEPOLYGONORIENTED


## Syntax

ST_MAKEPOLYGON( <geography_or_geometry_expression> )


## Arguments


geography_or_geometry_expressionA GEOGRAPHY or GEOMETRY object that represents a LineString in which the last point is the same as the first (i.e. a loop).


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY.

## Usage notes


The lines of the Polygon must form a loop. In other words, the last Point in the sequence of Points defining the LineString must be the same Point as the first Point in the sequence.
ST_POLYGON is an alias for ST_MAKEPOLYGON.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_MAKEPOLYGON function. The sequence of points below defines a geodesic rectangular area 1 degree wide and 2 degrees high, with the lower left corner of the polygon starting at the equator (latitude) and Greenwich (longitude). The last point in the sequence is the same as the first point,
which completes the loop.

SELECT ST_MAKEPOLYGON(
   TO_GEOGRAPHY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
   ) AS polygon1;
+--------------------------------+
| POLYGON1                       |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_MAKEPOLYGON function.

SELECT ST_MAKEPOLYGON(
  TO_GEOMETRY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
  ) AS polygon;

+--------------------------------+
| POLYGON                        |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+



## ST_MAKELINE

Constructs a GEOGRAPHY or GEOMETRY object that represents a line connecting the points in the input objects.

See also:TO_GEOGRAPHY , TO_GEOMETRY


## Syntax

ST_MAKELINE( <geography_expression_1> , <geography_expression_2> )

ST_MAKELINE( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geography_expression_2A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_1A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_2A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY. The value is a LineString that connects all of the points specified by the input GEOGRAPHY or GEOMETRY objects.

## Usage notes


If an input GEOGRAPHY object contains multiple points, ST_MAKELINE connects all of the points specified in the object.
ST_MAKELINE connects the points in the order in which they are specified in the input.

For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

The examples in this section display output in WKT format:

alter session set GEOGRAPHY_OUTPUT_FORMAT='WKT';


The following example uses ST_MAKELINE to construct a LineString that connects two Points:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(37.0 45.0)'),
                   TO_GEOGRAPHY('POINT(38.5 46.5)')
                  ) AS line_between_two_points;
+-----------------------------+
| LINE_BETWEEN_TWO_POINTS     |
|-----------------------------|
| LINESTRING(37 45,38.5 46.5) |
+-----------------------------+


The following example constructs a LineString that connects a Point with the points in a MultiPoint:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(-122.306067 37.55412)'),
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))')
                  ) AS line_between_point_and_multipoint;
+-----------------------------------------------------------------------------+
| LINE_BETWEEN_POINT_AND_MULTIPOINT                                           |
|-----------------------------------------------------------------------------|
| LINESTRING(-122.306067 37.55412,-122.32328 37.561801,-122.325879 37.586852) |
+-----------------------------------------------------------------------------+


As demonstrated by the output of the example, ST_MAKELINE connects the points in the order in which they are specified in the input.
The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))'),
                   TO_GEOGRAPHY('LINESTRING(-122.306067 37.55412, -122.496691 37.495627)')
                  ) AS line_between_multipoint_and_linestring;
+---------------------------------------------------------------------------------------------------+
| LINE_BETWEEN_MULTIPOINT_AND_LINESTRING                                                            |
|---------------------------------------------------------------------------------------------------|
| LINESTRING(-122.32328 37.561801,-122.325879 37.586852,-122.306067 37.55412,-122.496691 37.495627) |
+---------------------------------------------------------------------------------------------------+



## GEOMETRY examples

The examples in this section display output in WKT format:

ALTER SESSION SET GEOMETRY_OUTPUT_FORMAT='WKT';


The first example constructs a line between two Points:

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('POINT(3.5 4.5)')) AS line_between_two_points;

+-------------------------+
| LINE_BETWEEN_TWO_POINTS |
|-------------------------|
| LINESTRING(1 2,3.5 4.5) |
+-------------------------+


The next example demonstrates creating a LineString that connects points in a MultiPoint with a Point

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_point_and_multipoint;

+---------------------------------+
| LINE_FROM_POINT_AND_MULTIPOINT  |
|---------------------------------|
| LINESTRING(1 2,3.5 4.5,6.1 7.9) |
+---------------------------------+


The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
  TO_GEOMETRY('LINESTRING(1.0 2.0, 10.1 5.5)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_linestring_and_multipoint;

+------------------------------------------+
| LINE_FROM_LINESTRING_AND_MULTIPOINT      |
|------------------------------------------|
| LINESTRING(1 2,10.1 5.5,3.5 4.5,6.1 7.9) |
+------------------------------------------+",snowflake
97,sf_bq253,GEO_OPENSTREETMAP,"Find the name of the OpenStreetMap relation that encompasses the most features within the same geographic area as the multipolygon tagged with the Wikidata item 'Q1095'. The relation should have a specified name and no 'wikidata' tag, and at least one of its included features must have a 'wikidata' tag. The analysis should be conducted using the planet_features table. Return the name of this relation.",,"Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
98,sf_bq254,GEO_OPENSTREETMAP,"Among all multipolygons located within the same geographic area as the multipolygon associated with Wikidata item Q191, but lacking a 'wikidata' tag themselves, which two rank highest by the number of points that lie within their boundaries, and what are their names?","WITH bounding_area AS (
    SELECT ""geometry"" AS geometry
    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES,
    LATERAL FLATTEN(INPUT => ""all_tags"") AS tag
    WHERE ""feature_type"" = 'multipolygons'
      AND tag.value:""key"" = 'wikidata'
      AND tag.value:""value"" = 'Q191'
),
bounding_area_features AS (
    SELECT 
        planet_features.""osm_id"", 
        planet_features.""feature_type"", 
        planet_features.""geometry"", 
        planet_features.""all_tags""
    FROM GEO_OPENSTREETMAP.GEO_OPENSTREETMAP.PLANET_FEATURES AS planet_features,
         bounding_area
    WHERE ST_DWITHIN(
        ST_GEOGFROMWKB(planet_features.""geometry""), 
        ST_GEOGFROMWKB(bounding_area.geometry), 
        0.0
    )
),
osm_id_with_wikidata AS (
    SELECT DISTINCT
        baf.""osm_id""
    FROM bounding_area_features AS baf,
         LATERAL FLATTEN(INPUT => baf.""all_tags"") AS tag
    WHERE tag.value:""key"" = 'wikidata'
),

polygons_wo_wikidata AS (
    SELECT 
        baf.""osm_id"",
        tag.value:""value"" as name,
        baf.""geometry"" as geometry
    FROM bounding_area_features AS baf
    LEFT JOIN osm_id_with_wikidata AS wd
      ON baf.""osm_id"" = wd.""osm_id"",
    LATERAL FLATTEN(INPUT => ""all_tags"") AS tag
    WHERE wd.""osm_id"" IS NULL
    AND baf.""osm_id"" IS NOT NULL
    AND baf.""feature_type"" = 'multipolygons'
    AND tag.value:""key"" = 'name'
)

SELECT 
    TRIM(pww.name) as name
FROM bounding_area_features AS baf
JOIN polygons_wo_wikidata AS pww
    ON ST_DWITHIN(
        ST_GEOGFROMWKB(baf.""geometry""), 
        ST_GEOGFROMWKB(pww.geometry), 
        0.0
    )
LEFT JOIN osm_id_with_wikidata AS wd
    ON baf.""osm_id"" = wd.""osm_id""
WHERE wd.""osm_id"" IS NOT NULL
  AND baf.""feature_type"" = 'points'
GROUP BY pww.name
ORDER BY COUNT(baf.""osm_id"") DESC
LIMIT 2



","Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
99,sf_bq056,GEO_OPENSTREETMAP_BOUNDARIES,"How many different pairs of roads classified as motorway, trunk, primary, secondary, or residential in California overlap each other without sharing nodes and do not have a bridge tag, where these roads are tagged with 'highway', analyzed through planet ways",,"Categories: Geospatial functions


## ST_INTERSECTS

Returns TRUE if the two GEOGRAPHY objects or the two GEOMETRY objects intersect (i.e. share any portion of space).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_DISJOINT


## Syntax

ST_INTERSECTS( <geography_expression_1> , <geography_expression_2> )

ST_INTERSECTS( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object.

geography_expression_2A GEOGRAPHY object.

geometry_expression_1A GEOMETRY object.

geometry_expression_2A GEOMETRY object.


## Returns

BOOLEAN.

## Usage notes


For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
    TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'),
    TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')
    );
+---------------------------------------------------------+
| ST_INTERSECTS(                                          |
|     TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'), |
|     TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
|     )                                                   |
|---------------------------------------------------------|
| True                                                    |
+---------------------------------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
  TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'),
  TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))') );

+------------------------------------------------------+
| ST_INTERSECTS(                                       |
|   TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'), |
|   TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
| )                                                    |
|------------------------------------------------------|
| True                                                 |
+------------------------------------------------------+",snowflake
100,sf_bq289,GEO_OPENSTREETMAP_CENSUS_PLACES,"Can you find the shortest distance between any two amenities (either a library, place of worship, or community center) located within Philadelphia, analyzed through pennsylvania table and planet features points?","WITH philadelphia AS (
    SELECT 
        * 
    FROM 
        GEO_OPENSTREETMAP_CENSUS_PLACES.GEO_US_CENSUS_PLACES.PLACES_PENNSYLVANIA
    WHERE 
        ""place_name"" = 'Philadelphia'
),
amenities AS (
    SELECT 
        features.*, 
        tags.value:""value"" AS amenity
    FROM 
        GEO_OPENSTREETMAP_CENSUS_PLACES.GEO_OPENSTREETMAP.PLANET_FEATURES_POINTS AS features
    CROSS JOIN philadelphia
    -- Use FLATTEN on ""all_tags"" to get the tags and filter by ""key""
    , LATERAL FLATTEN(input => features.""all_tags"") AS tags
    WHERE 
        ST_CONTAINS(ST_GEOGFROMWKB(philadelphia.""place_geom""), ST_GEOGFROMWKB(features.""geometry""))
    AND 
        tags.value:""key"" = 'amenity' 
    AND 
        tags.value:""value"" IN ('library', 'place_of_worship', 'community_centre')
),
joiin AS (
    SELECT 
        a1.*, 
        a2.""osm_id"" AS nearest_osm_id, 
        ST_DISTANCE(ST_GEOGFROMWKB(a1.""geometry""), ST_GEOGFROMWKB(a2.""geometry"")) AS distance, 
        ROW_NUMBER() OVER (PARTITION BY a1.""osm_id"" ORDER BY ST_DISTANCE(ST_GEOGFROMWKB(a1.""geometry""), ST_GEOGFROMWKB(a2.""geometry""))) AS row_num
    FROM amenities a1
    CROSS JOIN amenities a2
    WHERE a1.""osm_id"" < a2.""osm_id""
    ORDER BY a1.""osm_id"", distance
) 
SELECT distance
FROM joiin  
WHERE row_num = 1
ORDER BY distance ASC
LIMIT 1;
","Categories: Geospatial functions


## ST_CONTAINS

Returns TRUE if a GEOGRAPHY or GEOMETRY object is completely inside another object of the same type.
More strictly, object g1 contains object g2 if and only if no points of g2 lie in the exterior of g1, and at least one point of the interior of B lies in the interior of A. There are certain subtleties in this definition that are not immediately obvious. For more details on what “contains” means, see the Dimensionally Extended 9-Intersection Model (DE-9IM).
Although ST_COVERS and ST_CONTAINS might seem similar, the two functions have subtle differences. For details on the differences between “covers” and “contains”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_WITHIN , ST_COVERS , ST_COVEREDBY


## Syntax

ST_CONTAINS( <geography_expression_1> , <geography_expression_2> )

ST_CONTAINS( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Usage notes


For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_CONTAINS function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_CONTAINS(g1, g2) 
    FROM geospatial_table_01;
+---------------------+
| ST_CONTAINS(G1, G2) |
|---------------------|
| True                |
+---------------------+



## GEOMETRY examples

The query below shows several examples of using ST_CONTAINS. Note how ST_CONTAINS determines that:

The Polygon contains itself.
The Polygon does not contain the LineString that is on its border.
SELECT ST_CONTAINS(poly, poly_inside),
      ST_CONTAINS(poly, poly),
      ST_CONTAINS(poly, line_on_boundary),
      ST_CONTAINS(poly, line_inside)
  FROM (SELECT
    TO_GEOMETRY('POLYGON((-2 0, 0 2, 2 0, -2 0))') AS poly,
    TO_GEOMETRY('POLYGON((-1 0, 0 1, 1 0, -1 0))') AS poly_inside,
    TO_GEOMETRY('LINESTRING(-1 1, 0 2, 1 1)') AS line_on_boundary,
    TO_GEOMETRY('LINESTRING(-2 0, 0 0, 0 1)') AS line_inside);

+--------------------------------+------------------------+------------------------------------+-------------------------------+
| ST_CONTAINS(POLY, POLY_INSIDE) | ST_CONTAINS(POLY,POLY) | ST_CONTAINS(POLY,LINE_ON_BOUNDARY) | ST_CONTAINS(POLY,LINE_INSIDE) |
|--------------------------------+------------------------+------------------------------------+-------------------------------|
| True                           | True                   | False                              | True                          |
+--------------------------------+------------------------+------------------------------------+-------------------------------+",snowflake
101,sf_bq250,GEO_OPENSTREETMAP_WORLDPOP,"Based on the most recent 1km population grid data in Singapore before January 2023, using ST_CONVEXHULL to aggregate all population grid centroids into a bounding region and ST_INTERSECTS to identify hospitals from OpenStreetMap’s planet layer (layer_code in (2110, 2120)) that fall within this region, then calculating the distance from each grid cell to its nearest hospital, what is the total population of the grid cell that is farthest from any hospital?","WITH country_name AS (
  SELECT 'Singapore' AS value
),

last_updated AS (
  SELECT
    MAX(""last_updated"") AS value
  FROM GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop
    INNER JOIN country_name ON (pop.""country_name"" = country_name.value)
  WHERE ""last_updated"" < '2023-01-01'
),

aggregated_population AS (
  SELECT
    ""geo_id"",
    SUM(""population"") AS sum_population,
    ST_POINT(""longitude_centroid"", ""latitude_centroid"") AS centr  -- 计算每个 geo_id 的中心点
  FROM
    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop
    INNER JOIN country_name ON (pop.""country_name"" = country_name.value)
    INNER JOIN last_updated ON (pop.""last_updated"" = last_updated.value)
  GROUP BY ""geo_id"", ""longitude_centroid"", ""latitude_centroid""
),

population AS (
  SELECT
    SUM(sum_population) AS sum_population,
    ST_ENVELOPE(ST_UNION_AGG(centr)) AS boundingbox  -- 使用 ST_ENVELOPE 来代替 ST_CONVEXHULL
  FROM aggregated_population
),

hospitals AS (
  SELECT
    layer.""geometry""
  FROM
    GEO_OPENSTREETMAP_WORLDPOP.GEO_OPENSTREETMAP.PLANET_LAYERS AS layer
    INNER JOIN population ON ST_INTERSECTS(population.boundingbox, ST_GEOGFROMWKB(layer.""geometry""))
  WHERE
    layer.""layer_code"" IN (2110, 2120)
),

distances AS (
  SELECT
    pop.""geo_id"",
    pop.""population"",
    MIN(ST_DISTANCE(ST_GEOGFROMWKB(pop.""geog""), ST_GEOGFROMWKB(hospitals.""geometry""))) AS distance
  FROM
    GEO_OPENSTREETMAP_WORLDPOP.WORLDPOP.POPULATION_GRID_1KM AS pop
    INNER JOIN country_name ON pop.""country_name"" = country_name.value
    INNER JOIN last_updated ON pop.""last_updated"" = last_updated.value
    CROSS JOIN hospitals
  WHERE pop.""population"" > 0
  GROUP BY ""geo_id"", ""population""
)

SELECT
  SUM(pd.""population"") AS population
FROM
  distances pd
CROSS JOIN population p
GROUP BY distance
ORDER BY distance DESC
LIMIT 1;
","# OpenStreetMap Data in Layered GIS Format

## Point Features

### 1. Places (“places”)

Location for cities, towns, etc. Typically somewhere in the centre of the town.

Additional attributes:

| Attribute  | PostGIS Type | Description                           | OSM Tags     |
| ---------- | ------------ | ------------------------------------- | ------------ |
| population | INTEGER      | Number of people living in this place | population=* |

Note that for many places the population is not available and will be set to 0. For islands the population is always 0.

The following feature classes exist in this layer:

| code | Layer | fclass | Description                                                  | OSM Tags                  |
| ---- | ----- | ------ | ------------------------------------------------------------ | ------------------------- |
| 1000 | place |        |                                                              |                           |
| 1001 | place | city   | As defined by national/state/provincial government. Often over 100,000 people | place=city (but see 1005) |                       |
| 1002 | place | town   | As defined by national/state/provincial government. Generally smaller than a city, between 10,000 and 100,000 people | place=town                |

| code | Layer | fclass           | Description                                                  | OSM Tags                                                     |
| ---- | ----- | ---------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 1003 | place | village          | As defined by national/state/provincial government. Generally smaller than a town, below 10,000 people | place=village                                                |                                                          |
| 1004 | place | hamlet           | As defined by national/state/provincial government. Generally smaller than a village, just a few houses | place=hamlet                                                 |                                                           |
| 1005 | place | national_capital | A national capital                                           | place=city<br />- is_capital=country or<br />- admin_level=2 or<br />- capital=yes and no <br />admin_level set |                                                           |
| 1010 | place | suburb           | Named area of town or city                                   | place=suburb                                                 |
| 1020 | place | island           | Identifies an island                                         | place=island                                                 |
| 1030 | place | farm             | Named farm                                                   | place=farm                                                   |
| 1031 | place | dwelling         | Isolated dwelling (1 or 2 houses, smaller than hamlet)       | place=isolated_dwelling                                      |
| 1040 | place | region           | A region label (used in some areas only)                     | place=region                                                 |
| 1041 | place | county           | A county label (used in some areas only)                     | place=county                                                 |
| 1050 | place | locality         | Other kind of named place                                    | place=locality                                               |

### 2. Points of Interest

The following feature classes exist in this layer:

| code | layer  | fclass           | Description                                                  | OSM Tags                               |
| ---- | ------ | ---------------- | ------------------------------------------------------------ | -------------------------------------- |
| 20xx | public |                  |                                                              |                                        |
| 2001 |        | police           | A police post or station.                                    | amenity=police                         |
| 2002 |        | fire_station     | A fire station.                                              | amenity=fire_station                   |
| 2004 |        | post_box         | A post box (for letters).                                    | amenity=post_box                       |
| 2005 |        | post_office      | A post office.                                               | amenity=post_office                    |
| 2006 |        | telephone        | A public telephone booth.                                    | amenity=telephone                      |
| 2007 |        | library          | A library.                                                   | amenity=library                        |
| 2008 |        | town_hall        | A town hall.                                                 | amenity=townhall                       |
| 2009 |        | courthouse       | A court house.                                               | amenity=courthouse                     |
| 2010 |        | prison           | A prison.                                                    | amenity=prison                         |
| 2011 |        | embassy          | An embassy or consulate.                                     | amenity=embassy or office=diplomatic   |
| 2012 |        | community_centre | A public facility which is mostly used by local associations for events and festivities. | amenity=community_centre               |
| 2013 |        | nursing_home     | A home for disabled or elderly persons who need permanent care. | amenity=nursing_home                   |
| 2014 |        | arts_centre      | A venue at which a variety of arts are performed or conducted, and may well be involved with the creation of those works, and run occasional courses. | amenity=arts_centre                    |
| 2015 |        | graveyard        | A graveyard.                                                 | amenity=grave_yard or landuse=cemetery |

| code | layer   | fclass            | Description                                                  | OSM Tags                                                     |
| ---- | ------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 2016 |         | market_place      | A place where markets are held.                              | amenity=marketplace                                          |
| 2030 |         | recycling         | A place (usually a container) where you can drop waste for recycling. | amenity=recycling                                            |
| 2031 |         | recycling_glass   | A place for recycling glass.                                 | recycling:glass=yes or recycling:glass_bottles=yes           |
| 2032 |         | recycling_paper   | A place for recycling paper.                                 | recycling:paper=yes                                          |
| 2033 |         | recycling_clothes | A place for recycling clothes.                               | recycling:clothes=yes                                        |
| 2034 |         | recycling_metal   | A place for recycling metal.                                 | recycling:scrap_metal=yes                                    |
| 208x |         |                   | Education                                                    |                                                              |
| 2081 |         | university        | A university.                                                | amenity=university                                           |
| 2082 |         | school            | A school.                                                    | amenity=school                                               |
| 2083 |         | kindergarten      | A kindergarten (nursery).                                    | amenity=kindergarten                                         |
| 2084 |         | college           | A college.                                                   | amenity=college                                              |
| 2099 |         | public_building   | An unspecified public building.                              | amenity=public_building                                      |
| 21xx | health  |                   |                                                              |                                                              |
| 2101 |         | pharmacy          | A pharmacy.                                                  | amenity=pharmacy                                             |
| 2110 |         | hospital          | A hospital.                                                  | amenity=hospital                                             |
| 2111 |         | clinic            | A medical centre that does not admit inpatients.             | amenity=clinic                                               |
| 2120 |         | doctors           | A medical practice.                                          | amenity=doctors                                              |
| 2121 |         | dentist           | A dentist's practice.                                        | amenity=dentist                                              |
| 2129 |         | veterinary        | A veterinary (animal doctor).                                | amenity=veterinary                                           |
| 22xx | leisure |                   |                                                              |                                                              |
| 2201 |         | theatre           | A theatre.                                                   | amenity=theatre                                              |
| 2202 |         | nightclub         | A night club, or disco.                                      | amenity=nightclub                                            |
| 2203 |         | cinema            | A cinema.                                                    | amenity=cinema                                               |
| 2204 |         | park              | A park.                                                      | leisure=park                                                 |
| 2205 |         | playground        | A playground for children.                                   | leisure=playground                                           |
| 2206 |         | dog_park          | An area where dogs are allowed to run free without a leash.  | leisure=dog_park                                             |
| 225x |         |                   | Sports                                                       |                                                              |
| 2251 |         | sports_centre     | A facility where a range of sports activities can be pursued. | leisure=sports_centre                                        |
| 2252 |         | pitch             | An area set aside for a specific sport.                      | leisure=pitch                                                |
| 2253 |         | swimming_pool     | A swimming pool or water park.                               | amenity=swimming_pool,leisure=swimming_pool,sport=swimming, leisure=water_park |
| 2254 |         | tennis_court      | A tennis court.                                              | sport=tennis                                                 |
| 2255 |         | golf_course       | A golf course.                                               | leisure=golf_course                                          |
| 2256 |         | stadium           | A stadium. The area of the stadium may contain one or several pitches. | leisure=stadium                                              |
| 2257 |         | ice_rink          | An ice rink.                                                 | leisure=ice_rink                                             |

| code | layer         | fclass            | Description                                                  | OSM Tags                  |
| ---- | ------------- | ----------------- | ------------------------------------------------------------ | ------------------------- |
| 23xx | catering      |                   | Catering services                                            |                           |
| 2301 |               | restaurant        | A normal restaurant.                                         | amenity=restaurant        |
| 2302 |               | fast_food         | A fast-food restaurant.                                      | amenity=fast_food         |
| 2303 |               | cafe              | A cafe.                                                      | amenity=cafe              |
| 2304 |               | pub               | A pub.                                                       | amenity=pub               |
| 2305 |               | bar               | A bar. The difference between a pub and a bar is not clear but pubs tend to offer food while bars do not. | amenity=bar               |
| 2306 |               | food_court        | A common seating area with various fast-food vendors.        | amenity=food_court        |
| 2307 |               | biergarten        | An open-air area where food and drinks are served.           | amenity=biergarten        |
| 24xx | accommodation |                   | (indoor)                                                     |                           |
| 2401 |               | hotel             | A hotel.                                                     | tourism=hotel             |
| 2402 |               | motel             | A motel.                                                     | tourism=motel             |
| 2403 |               | bed_and_breakfast | A facility offering bed and breakfast.                       | tourism=bed_and_breakfast |
| 2404 |               | guesthouse        | A guesthouse. The difference between hotel, bed and breakfast, and guest houses is not a strict one and OSM tends to use whatever the facility calls itself. | tourism=guest_house       |
| 2405 |               | hostel            | A hostel (offering cheap accommodation, often bunk beds in dormitories). | tourism=hostel            |
| 2406 |               | chalet            | A detached cottage, usually self-catering.                   | tourism=chalet            |
| 2420 |               |                   | (outdoor)                                                    |                           |
| 2421 |               | shelter           | All sorts of small shelters to protect against bad weather conditions. | amenity=shelter           |
| 2422 |               | camp_site         | A camp site or camping ground.                               | tourism=camp_site         |
| 2423 |               | alpine_hut        | An alpine hut is a building typically situated in mountains providing shelter and often food and beverages to visitors. | tourism=alpine_hut        |
| 2424 |               | caravan_site      | A place where people with caravans or motorhomes can stay overnight or longer. | tourism=caravan_site      |
| 25xx | shopping      |                   |                                                              |                           |
| 2501 |               | supermarket       | A supermarket.                                               | shop=supermarket          |
| 2502 |               | bakery            | A bakery.                                                    | shop=bakery               |
| 2503 |               | kiosk             | A very small shop usually selling cigarettes, newspapers, sweets, snacks and beverages. | shop=kiosk                |
| 2504 |               | mall              | A shopping mall.                                             | shop=mall                 |
| 2505 |               | department_store  | A department store.                                          | shop=department_store     |

| code | layer | fclass            | Description                                                  | OSM Tags                         |
| ---- | ----- | ----------------- | ------------------------------------------------------------ | -------------------------------- |
| 2510 |       | general           | A general store, offering a broad range of products on a small area. Exists usually in rural and remote areas. | shop=general                     |
| 2511 |       | convenience       | A convenience store is a small shop selling a subset of items you might find at a supermarket. | shop=convenience                 |
| 2512 |       | clothes           | A clothes or fashion store.                                  | shop=clothes                     |
| 2513 |       | florist           | A store selling flowers.                                     | shop=florist                     |
| 2514 |       | chemist           | A shop selling articles of personal hygiene, cosmetics, and household cleaning products. | shop=chemist                     |
| 2515 |       | bookshop          | A book shop.                                                 | shop=books                       |
| 2516 |       | butcher           | A butcher.                                                   | shop=butcher                     |
| 2517 |       | shoe_shop         | A shoe shop.                                                 | shop=shoes                       |
| 2518 |       | beverages         | A place where you can buy alcoholic and non-alcoholic beverages. | shop=alcohol, shop=beverages     |
| 2519 |       | optician          | A place where you can buy glasses.                           | shop=optician                    |
| 2520 |       | jeweller          | A jewelry shop.                                              | shop=jewelry                     |
| 2521 |       | gift_shop         | A gift shop.                                                 | shop=gift                        |
| 2522 |       | sports_shop       | A shop selling sports equipment.                             | shop=sports                      |
| 2523 |       | stationery        | A shop selling stationery for private and office use.        | shop=stationery                  |
| 2524 |       | outdoor_shop      | A shop selling outdoor equipment.                            | shop=outdoor                     |
| 2525 |       | mobile_phone_shop | A shop for mobile phones.                                    | shop=mobile_phone                |
| 2526 |       | toy_shop          | A toy store.                                                 | shop=toys                        |
| 2527 |       | newsagent         | A shop selling mainly newspapers and magazines.              | shop=newsagent                   |
| 2528 |       | greengrocer       | A shop selling fruit and vegetables.                         | shop=greengrocer                 |
| 2529 |       | beauty_shop       | A shop that provides personal beauty services like a nail salon or tanning salon. | shop=beauty                      |
| 2530 |       | video_shop        | A place where you can buy films.                             | shop=video                       |
| 2541 |       | car_dealership    | A car dealership.                                            | shop=car                         |
| 2542 |       | bicycle_shop      | A bicycle shop.                                              | shop=bicycle                     |
| 2543 |       | doityourself      | A do-it-yourself shop where you can buy tools and building materials. | shop=doityourself, shop=hardware |
| 2544 |       | furniture_shop    | A furniture store.                                           | shop=furniture                   |
| 2546 |       | computer_shop     | A computer shop.                                             | shop=computer                    |
| 2547 |       | garden_centre     | A place selling plants and gardening goods.                  | shop=garden_centre               |
| 2561 |       | hairdresser       | A hair salon.                                                | shop=hairdresser                 |
| 2562 |       | car_repair        | A car garage.                                                | shop=car_repair                  |
| 2563 |       | car_rental        | A place where you can rent a car.                            | amenity=car_rental               |
| 2564 |       | car_wash          | A car wash.                                                  | amenity=car_wash                 |
| 2565 |       | car_sharing       | A car sharing station.                                       | amenity=car_sharing              |

| code | layer   | fclass            | Description                                                  | OSM Tags                                   |
| ---- | ------- | ----------------- | ------------------------------------------------------------ | ------------------------------------------ |
| 2566 |         | bicycle_rental    | A place where you can rent bicycles.                         | amenity=bicycle_rental                     |
| 2567 |         | travel_agent      | A travel agency.                                             | shop=travel_agency                         |
| 2568 |         | laundry           | A place where you can wash clothes or have them cleaned.     | shop=laundry, shop=dry_cleaning            |
| 2590 |         | vending_machine   | An unspecified vending machine with none of the specifics below. | amenity=vending_machine                    |
| 2591 |         | vending_cigarette | A cigarette vending machine.                                 | vending=cigarettes                         |
| 2592 |         | vending_parking   | A vending machine for parking tickets.                       | vending=parking_tickets                    |
| 2600 | money   |                   |                                                              |                                            |
| 2601 |         | bank              | A bank.                                                      | amenity=bank                               |
| 2602 |         | atm               | A machine that lets you withdraw cash from your bank account. | amenity=atm                                |
| 2700 | tourism |                   | information                                                  |                                            |
| 2701 |         | tourist_info      | Something that provides information to tourists; may or may not be manned. | tourism=information                        |
| 2704 |         | tourist_map       | A map displayed to inform tourists.                          | tourism=information, information=map       |
| 2705 |         | tourist_board     | A board with explanations aimed at tourists.                 | tourism=information, information=board     |
| 2706 |         | tourist_guidepost | A guide post.                                                | tourism=information, information=guidepost |
|      |         |                   | destinations                                                 |                                            |
| 2721 |         | attraction        | A tourist attraction.                                        | tourism=attraction                         |
| 2722 |         | museum            | A museum.                                                    | tourism=museum                             |
| 2723 |         | monument          | A monument.                                                  | historic=monument                          |
| 2724 |         | memorial          | A memorial.                                                  | historic=memorial                          |
| 2725 |         | art               | A permanent work of art.                                     | tourism=artwork                            |
| 2731 |         | castle            | A castle.                                                    | historic=castle                            |
| 2732 |         | ruins             | Ruins of historic significance.                              | historic=ruins                             |
| 2733 |         | archaeological    | An excavation site.                                          | historic=archaeological_site               |
| 2734 |         | wayside_cross     | A wayside cross, not necessarily old.                        | historic=wayside_cross                     |
| 2735 |         | wayside_shrine    | A wayside shrine.                                            | historic=wayside_shrine                    |
| 2736 |         | battlefield       | A historic battlefield.                                      | historic=battlefield                       |
| 2737 |         | fort              | A fort.                                                      | historic=fort                              |
| 2741 |         | picnic_site       | A picnic site.                                               | tourism=picnic_site                        |
| 2742 |         | viewpoint         | A viewpoint.                                                 | tourism=viewpoint                          |
| 2743 |         | zoo               | A zoo.                                                       | tourism=zoo                                |
| 2744 |         | theme_park        | A theme park.                                                | tourism=theme_park                         |
| 2900 | miscpoi |                   |                                                              |                                            |
| 2901 |         | toilet            | Public toilets.                                              | amenity=toilets                            |
| 2902 |         | bench             | A public bench.                                              | amenity=bench                              |
| 2903 |         | drinking_water    | A tap or other source of drinking water.                     | amenity=drinking_water                     |

| code | layer | fclass              | Description                                                  | OSM Tags                                       |
| ---- | ----- | ------------------- | ------------------------------------------------------------ | ---------------------------------------------- |
| 2904 |       | fountain            | A fountain for cultural, decorative, or recreational purposes. | amenity=fountain                               |
| 2905 |       | hunting_stand       | A hunting stand.                                             | amenity=hunting_stand                          |
| 2906 |       | waste_basket        | A waste basket.                                              | amenity=waste_basket                           |
| 2907 |       | camera_surveillance | A surveillance camera.                                       | man_made=surveillance                          |
| 2921 |       | emergency_phone     | An emergency telephone.                                      | amenity=emergency_phone, emergency=phone       |
| 2922 |       | fire_hydrant        | A fiery hydrant.                                             | amenity=fire_hydrant, emergency=fire_hydrant   |
| 2923 |       | emergency_access    | An emergency access point (signposted place in e.g., woods the location of which is known to emergency services). | highway=emergency_access_point                 |
| 2950 |       | tower               | A tower of some kind.                                        | man_made=tower and none of the specifics below |
| 2951 |       | tower_comms         | A communications tower.                                      | man_made=tower and tower:type=communication    |
| 2952 |       | water_tower         | A water tower.                                               | man_made=water_tower                           |
| 2953 |       | tower_observation   | An observation tower.                                        | man_made=tower and tower:type=observation      |
| 2954 |       | windmill            | A windmill.                                                  | man_made=windmill                              |
| 2955 |       | lighthouse          | A lighthouse.                                                | man_made=lighthouse                            |
| 2961 |       | wastewater_plant    | A wastewater treatment plant.                                | man_made=wastewater_plant                      |
| 2962 |       | water_well          | A facility to access underground aquifers.                   | man_made=water_well                            |
| 2963 |       | water_mill          | A mill driven by water. Often historic.                      | man_made=watermill                             |
| 2964 |       | water_works         | A place where drinking water is processed.                   | man_made=water_works                           |

### 3. Places of Worship (“pofw”)

The following feature classes exist in this layer:

| code | layer | fclass                | Description                                                  | OSM Tags                                     |
| ---- | ----- | --------------------- | ------------------------------------------------------------ | -------------------------------------------- |
| 3000 | pofw  |                       | Places of worship                                            |                                              |
| 3100 | pofw  | christian             | A christian place of worship (usually a church) without one of the denominations below. | amenity=place_of_worship, religion=christian |
| 3101 | pofw  | christian_anglican    | A christian place of worship where the denomination is known. (Note to German users: “protestant” is “evangelisch” in German; “evangelical” is “evangelikal” in German.) | + denomination=anglican                      |
| 3102 | pofw  | christian_catholic    |                                                              | + denomination=catholic                      |
| 3103 | pofw  | christian_evangelical |                                                              | + denomination=evangelical                   |
| 3104 | pofw  | christian_lutheran    |                                                              | + denomination=lutheran                      |
| 3105 | pofw  | christian_methodist   |                                                              | + denomination=methodist                     |
| 3106 | pofw  | christian_orthodox    |                                                              | + denomination=orthodox                      |
| 3107 | pofw  | christian_protestant  |                                                              | + denomination=protestant                    |
| 3108 | pofw  | christian_baptist     |                                                              | + denomination=baptist                       |
| 3109 | pofw  | christian_mormon      |                                                              | + denomination=mormon                        |

| code | layer | fclass       | Description                                                  | OSM Tags                                     |
| ---- | ----- | ------------ | ------------------------------------------------------------ | -------------------------------------------- |
| 3200 | pofw  | jewish       | A Jewish place of worship (usually a synagogue).             | amenity=place_of_worship, religion=jewish    |
| 3300 | pofw  | muslim       | A Muslim place of worship (usually a mosque) without one of the denominations below. | amenity=place_of_worship, religion=muslim    |
| 3301 | pofw  | muslim_sunni | A Sunni Muslim place of worship.                             | + denomination=sunni                         |
| 3302 | pofw  | muslim_shia  | A Shia Muslim place of worship.                              | + denomination=shia                          |
| 3400 | pofw  | buddhist     | A Buddhist place of worship.                                 | amenity=place_of_worship, religion=buddhist  |
| 3500 | pofw  | hindu        | A Hindu place of worship.                                    | amenity=place_of_worship, religion=hindu     |
| 3600 | pofw  | taoist       | A Taoist place of worship.                                   | amenity=place_of_worship, religion=taoist    |
| 3700 | pofw  | shintoist    | A Shintoist place of worship.                                | amenity=place_of_worship, religion=shintoist |
| 3800 | pofw  | sikh         | A Sikh place of worship.                                     | amenity=place_of_worship, religion=sikh      |

### 4. Natural Features (“natural”)

The following feature classes exist in this layer:

| code | layer   | fclass        | Description                                                  | OSM Tags              |
| ---- | ------- | ------------- | ------------------------------------------------------------ | --------------------- |
| 4101 | natural | spring        | A spring, possibly source of a stream.                       | natural=spring        |
| 4103 | natural | glacier       | A glacier.                                                   | natural=glacier       |
| 4111 | natural | peak          | A mountain peak.                                             | natural=peak          |
| 4112 | natural | cliff         | A cliff.                                                     | natural=cliff         |
| 4113 | natural | volcano       | A volcano.                                                   | natural=volcano       |
| 4121 | natural | tree          | A tree.                                                      | natural=tree          |
| 4131 | natural | mine          | A mine.                                                      | natural=mine          |
| 4132 | natural | cave_entrance | A cave entrance.                                             | natural=cave_entrance |
| 4141 | natural | beach         | A beach. (Note that beaches are only rarely mapped as point features.) | natural=beach         |

### 5. Traffic Related (“traffic”)

The following feature classes exist in this layer:

| code | layer   | fclass          | Description                                                  | OSM Tags                                 |
| ---- | ------- | --------------- | ------------------------------------------------------------ | ---------------------------------------- |
| 5201 | traffic | traffic_signals | Traffic lights.                                              | highway=traffic_signals                  |
| 5202 | traffic | mini_roundabout | A small roundabout without physical structure, usually just painted onto the road surface. | highway=mini_roundabout                  |
| 5203 | traffic | stop            | A stop sign.                                                 | highway=stop                             |
| 5204 | traffic | crossing        | A place where the street is crossed by pedestrians or a railway. | highway=crossing, railway=level_crossing |

| code | layer   | fclass              | Description                                             | OSM Tags                      |
| ---- | ------- | ------------------- | ------------------------------------------------------- | ----------------------------- |
| 5205 | traffic | ford                | A place where the road runs through a river or stream.  | highway=ford                  |
| 5206 | traffic | motorway_junction   | The place where a slipway enters or leaves a motorway.  | highway=motorway_junction     |
| 5207 | traffic | turning_circle      | An area at the end of a street where vehicles can turn. | highway=turning_circle        |
| 5208 | traffic | speed_camera        | A camera that photographs speeding vehicles.            | highway=speed_camera          |
| 5209 | traffic | street_lamp         | A lamp illuminating the road.                           | highway=street_lamp           |
|      |         |            |Fuel and Parking||
| 5250 | traffic | fuel                | A gas station.                                          | amenity=fuel                  |
| 5251 | traffic | service             | A service area, usually along motorways.                | highway=services              |
| 5260 | traffic | parking             | A car park of unknown type.                             | amenity=parking               |
| 5261 | traffic | parking_site        | A surface car park.                                     | amenity=parking, parking=site |
| 5262 | traffic | parking_multistorey | A multi-storey car park.                                | parking=multi-storey          |
| 5263 | traffic | parking_underground | An underground car park.                                | parking=underground           |
| 5270 | traffic | parking_bicycle     | A place to park your bicycle.                           | amenity=bicycle_parking       |
|      |         |                     |Water Traffic||
| 5301 | traffic | slipway             | A slipway.                                              | leisure=slipway               |
| 5302 | traffic | marina              | A marina.                                               | leisure=marina                |
| 5303 | traffic | pier                | A pier.                                                 | man_made=pier                 |
| 5311 | traffic | dam                 | A dam.                                                  | waterway=dam                  |
| 5321 | traffic | waterfall           | A waterfall.                                            | waterway=waterfall            |
| 5331 | traffic | lock_gate           | A lock gate.                                            | waterway=lock_gate            |
| 5332 | traffic | weir                | A barrier built across a river or stream.               | waterway=weir                 |

Note: Most of the 53xx type objects do sometimes appear as linear features in OSM as well but those are not yet available in the shape files.

### 6. Transport Infrastructure (“transport”)

The following feature classes exist in this layer:

| code | layer     | fclass          | Description                                          | OSM Tags                                                     |
| ---- | --------- | --------------- | ---------------------------------------------------- | ------------------------------------------------------------ |
| 5601 | transport | railway_station | A larger railway station of mainline rail services.  | railway=station                                              |
| 5602 | transport | railway_halt    | A smaller, local railway station, or subway station. | railway=halt, or public_transport=stop_position + train=yes  |
| 5603 | transport | tram_stop       | A tram stop.                                         | railway=tram_stop, or public_transport=stop_position + tram=yes |
| 5621 | transport | bus_stop        | A bus stop.                                          | highway=bus_stop, or public_transport=stop_position + bus=yes |

| code | layer     | fclass            | Description                                  | OSM Tags                                                     |
| ---- | --------- | ----------------- | -------------------------------------------- | ------------------------------------------------------------ |
| 5622 | transport | bus_station       | A large bus station with multiple platforms. | amenity=bus_station                                          |
| 5641 | transport | taxi_rank         | A taxi rank.                                 | amenity=taxi                                                 |
| 565x |           |                   | Air Traffic                                  |                                                              |
| 5651 | transport | airport           | A large airport.                             | amenity=airport or aeroway=aerodrome unless type=airstrip    |
| 5652 | transport | airfield          | A small airport or airfield.                 | aeroway=airfield, military=airfield, aeroway=aeroway with type=airstrip |
| 5655 | transport | helipad           | A place for landing helicopters.             | aeroway=helipad                                              |
| 5656 | transport | apron             | An apron (area where aircraft are parked)    | aeroway=apron                                                |
| 566x |           |                   | Water Traffic                                |                                                              |
| 5661 | transport | ferry_terminal    | A ferry terminal.                            | amenity=ferry_terminal                                       |
| 567x |           |                   | Other Traffic                                |                                                              |
| 5671 | transport | aerialway_station | A station where cable cars or lifts alight.  | aerialway=station                                            |",snowflake
102,sf_bq083,CRYPTO,"Can you calculate the daily change in the market value of USDC tokens (address `0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48`) for 2023, based on Ethereum transactions? The change should be computed from minting (input pattern `0x40c10f19%`) and burning (input pattern `0x42966c68%`) operations. For each transaction, minting should be positive and burning negative. Extract the relevant amount from the 'input' field as a hexadecimal, convert it to millions, express it in USD format. Group the results by date and order them in descending order.","SELECT 
  TO_DATE(TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000)) AS ""Date"",  -- 将时间戳转换为日期格式，除以1000000
  TO_CHAR(SUM(
      CASE
          WHEN ""input"" LIKE '0x40c10f19%' THEN 1
          ELSE -1
      END * 
      CAST(CONCAT('0x', LTRIM(SUBSTRING(""input"", 
                                       CASE 
                                           WHEN ""input"" LIKE '0x40c10f19%' THEN 75
                                           ELSE 11
                                       END, 64), '0')) AS FLOAT) / 1000000)
  , '$999,999,999,999') AS ""Δ Total Market Value""
FROM 
  ""CRYPTO"".""CRYPTO_ETHEREUM"".""TRANSACTIONS""
WHERE 
  TO_DATE(TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000)) BETWEEN '2023-01-01' AND '2023-12-31'
  AND ""to_address"" = '0xa0b86991c6218b36c1d19d4a2e9eb0ce3606eb48'  -- USDC Token
  AND (""input"" LIKE '0x42966c68%' -- Burn
       OR ""input"" LIKE '0x40c10f19%' -- Mint
  )
GROUP BY 
  TO_DATE(TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000))
ORDER BY 
  ""Date"" DESC;
","## Total Market Value Change

1. **`USD(...)`:**

   \- **Purpose:** This is a user-defined function that formats a floating-point number as a USD currency string.

   \- **Effect:** It wraps the entire sum calculation to produce a string formatted as currency for display purposes.



2. **`SUM(...)`:**

   \- **Purpose:** Aggregates the calculated values for each transaction to provide a total market value change per day.

   

3. **`IFMINT(input, 1, -1)`:**

   \- **Function:** This temporary function checks if the transaction input indicates a mint operation (by checking if it starts with `0x40c10f19`).

   \- **Return Value:** Returns `1` for mint operations and `-1` for non-mint operations (such as burn), effectively applying a positive or negative sign to the calculated value.

   

4. **`CAST(CONCAT(""0x"", LTRIM(SUBSTRING(input, IFMINT(input, 75, 11), 64), ""0"")) AS FLOAT64)`:**

   \- **Process:**

​     \- **`SUBSTRING(input, IFMINT(input, 75, 11), 64)`:** Extracts a portion of the transaction input string based on the operation type. It uses `75` for mint operations and `11` for non-mint operations (e.g., burn).

​     \- **`LTRIM(...,""0"")`:** Removes leading zeros from the extracted substring.

​     \- **`CONCAT(""0x"", ...)`:** Prepends ""0x"" to the adjusted string segment, creating a complete hexadecimal string.

​     \- **`CAST(... AS FLOAT64)`:** Converts the hexadecimal string to a floating-point number, interpreting it as a value in the smallest token unit.

​     

5. **`/ 1000000`:**

   \- **Purpose:** Scales down the number from the smallest token unit to a standard unit (e.g., from wei to ether), assuming USDC has six decimal places.



6. **Alias `AS `Δ Total Market Value``:**

   \- **Result:** Names the final output column as ""Δ Total Market Value"" to clarify the calculation purpose—representing the net change in total market value due to mint and burn operations on that specific day.",snowflake
103,sf_bq184,CRYPTO,"Using only the traces, can you calculate daily cumulative counts of smart contracts created by external addresses (where the trace_address is NULL) versus those created by other contracts (where the trace_address is NOT NULL) for each date from 2017-01-01 through 2021-12-31, ensuring that all dates in this range are included even if no new contracts were created on some days, and showing monotonically increasing cumulative totals for both categories?",,,snowflake
104,sf_bq195,CRYPTO,"What are the top 10 Ethereum addresses by balance, considering both value transactions and gas fees, before September 1, 2021? Only keep successful transactions with no call type or where the call type is 'call'.",,,snowflake
105,sf_bq256,CRYPTO,"Determine the final Ether balance of the Ethereum address that initiated the highest number of successful transactions prior to September 1, 2021 (UTC), excluding calls of type delegatecall, callcode, or staticcall and including all relevant incoming and outgoing transfers, miner rewards, and gas fee deductions, with the final balance presented in Ether after converting from the native unit.",,,snowflake
106,sf_bq080,CRYPTO,"Using only the Ethereum traces table, can you provide a daily cumulative count of smart contracts created by external users (where trace_address is null) versus contracts created by other contracts (where trace_address is not null) between August 30, 2018, and September 30, 2018? Ensure results include every date in this range, even if no new contracts were created, and show strictly increasing cumulative totals.",,,snowflake
107,sf_bq342,CRYPTO,"What is the difference between the average hourly changes in transaction values for the Ethereum token 0x68e54af74b22acaccffa04ccaad13be16ed14eac, specifically considering only transactions where the address 0x8babf0ba311aab914c00e8fda7e8558a8b66de5d was the sender or the address 0xfbd6c6b112214d949dcdfb1217153bc0a742862f was the receiver, between January 1, 2019, and December 31, 2020, when comparing 2019 to 2020?",,,snowflake
108,sf_bq341,CRYPTO,"Which Ethereum address has the top 3 smallest positive balance from transactions involving the token at address ""0xa92a861fc11b99b24296af880011b47f9cafb5ab""?","WITH transaction_addresses AS (
    SELECT 
        ""from_address"", 
        ""to_address"", 
        CAST(""value"" AS NUMERIC) / 1000000 AS ""value""
    FROM 
        ""CRYPTO"".""CRYPTO_ETHEREUM"".""TOKEN_TRANSFERS""
    WHERE 
        ""token_address"" = '0xa92a861fc11b99b24296af880011b47f9cafb5ab'
),

out_addresses AS (
    SELECT 
        ""from_address"", 
        SUM(-1 * ""value"") AS ""total_value""
    FROM 
        transaction_addresses
    GROUP BY 
        ""from_address""
),

in_addresses AS (
    SELECT 
        ""to_address"", 
        SUM(""value"") AS ""total_value""
    FROM 
        transaction_addresses
    GROUP BY 
        ""to_address""
),

all_addresses AS (
    SELECT 
        ""from_address"" AS ""address"", 
        ""total_value""
    FROM 
        out_addresses

    UNION ALL

    SELECT 
        ""to_address"" AS ""address"", 
        ""total_value""
    FROM 
        in_addresses
)

SELECT 
    ""address""
FROM 
    all_addresses
GROUP BY 
    ""address""
HAVING 
    SUM(""total_value"") > 0
ORDER BY 
    SUM(""total_value"") ASC
LIMIT 3;
",,snowflake
109,sf_bq444,CRYPTO,"Can you pull the blockchain timestamp, block number, and transaction hash for the first five mint and burn events from Ethereum logs for the address '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'? Please include mint events identified by the topic '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde' and burn events by '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c', and order them by block timestamp from the oldest to the newest.","WITH parsed_burn_logs AS (
  SELECT
    logs.""block_timestamp"" AS block_timestamp,
    logs.""block_number"" AS block_number,
    logs.""transaction_hash"" AS transaction_hash,
    logs.""log_index"" AS log_index,
    PARSE_JSON(logs.""data"") AS data,
    logs.""topics""
  FROM CRYPTO.CRYPTO_ETHEREUM.LOGS AS logs
  WHERE logs.""address"" = '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'
    AND logs.""topics""[0] = '0x0c396cd989a39f4459b5fa1aed6a9a8dcdbc45908acfd67e028cd568da98982c'
),
parsed_mint_logs AS (
  SELECT
    logs.""block_timestamp"" AS block_timestamp,
    logs.""block_number"" AS block_number,
    logs.""transaction_hash"" AS transaction_hash,
    logs.""log_index"" AS log_index,
    PARSE_JSON(logs.""data"") AS data,
    logs.""topics""
  FROM CRYPTO.CRYPTO_ETHEREUM.LOGS AS logs
  WHERE logs.""address"" = '0x8ad599c3a0ff1de082011efddc58f1908eb6e6d8'
    AND logs.""topics""[0] = '0x7a53080ba414158be7ec69b987b5fb7d07dee101fe85488f0853ae16239d0bde'
)

SELECT
    block_timestamp,
    block_number,
    transaction_hash
FROM parsed_mint_logs

UNION ALL

SELECT
    block_timestamp,
    block_number,
    transaction_hash
FROM parsed_burn_logs

ORDER BY block_timestamp
LIMIT 5;
","Ethereum Logs Overview: Ethereum logs represent activities recorded by smart contracts on the Ethereum blockchain. These logs can be filtered using specific topics associated with contract events, such as minting or burning tokens.

Understanding Mint and Burn Events: Mint events generally refer to the creation of tokens, whereas burn events refer to their removal from circulation. These processes are tracked and verified on the blockchain, allowing transparent transaction history and token supply management.

Event Topics in Ethereum: Each event in Ethereum is identified by a topic, which is typically the hash of the event's signature. This allows for efficient filtering and retrieval of events related to specific actions or contracts.

JavaScript Functions in SQL Queries: SQL queries might use JavaScript functions to parse complex data structures from blockchain logs. These functions typically utilize libraries to decode the data according to the Ethereum ABI, which defines how data is structured in blockchain interactions.

Use of External Libraries for Data Decoding: External libraries can be employed within SQL queries to handle specific data decoding tasks related to blockchain data. These libraries facilitate the interpretation of raw data encoded according to the blockchain’s standards.
",snowflake
110,sf_bq340,CRYPTO,"Which six Ethereum addresses, excluding '0x0000000000000000000000000000000000000000', have the largest absolute differences between their previous and current balances from the tokens at addresses '0x0d8775f648430679a709e98d2b0cb6250d2887ef0' and '0x1e15c05cbad367f044cbfbafda3d9a1510db5513'?",,,snowflake
111,sf_bq005,CRYPTO,"Calculate the daily average Bitcoin block interval (in seconds) for 2023 by joining consecutive blocks via row-numbered self-joins (including cross-day intervals), excluding the genesis block, and list the first 10 dates with their unadjusted averages.",,,snowflake
112,sf_bq334,CRYPTO,"Calculate the annual differences in Bitcoin output value averages between two methods: Merged input/output records: Combine the inputs and outputs tables, filter to only output records, and calculate yearly averages. Transactions table: Directly use the output_value field from the transactions table for yearly averages. Show the difference (merged outputs average minus transactions average) only for years with data in both methods.","WITH all_transactions AS (
    SELECT 
        TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000) AS ""timestamp"",  -- 将时间戳转换为日期时间格式
        ""value"",
        'input' AS ""type""
    FROM 
        ""CRYPTO"".""CRYPTO_BITCOIN"".""INPUTS""
    UNION ALL
    SELECT 
        TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000) AS ""timestamp"",  -- 将时间戳转换为日期时间格式
        ""value"",
        'output' AS ""type""
    FROM 
        ""CRYPTO"".""CRYPTO_BITCOIN"".""OUTPUTS""
),
filtered_transactions AS (
    SELECT
        EXTRACT(YEAR FROM ""timestamp"") AS ""year"",
        ""value""
    FROM 
        all_transactions
    WHERE ""type"" = 'output'
),
average_output_values AS (
    SELECT
        ""year"",
        AVG(""value"") AS ""avg_value""
    FROM 
        filtered_transactions
    GROUP BY ""year""
),
average_transaction_values AS (
    SELECT 
        EXTRACT(YEAR FROM TO_TIMESTAMP_NTZ(""block_timestamp"" / 1000000)) AS ""year"",  -- 同样转换时间戳
        AVG(""output_value"") AS ""avg_transaction_value"" 
    FROM 
        ""CRYPTO"".""CRYPTO_BITCOIN"".""TRANSACTIONS"" 
    GROUP BY ""year"" 
    ORDER BY ""year""
),
common_years AS (
    SELECT
        ao.""year"",
        ao.""avg_value"" AS ""avg_output_value"",
        atv.""avg_transaction_value""
    FROM
        average_output_values ao
    JOIN
        average_transaction_values atv 
        ON ao.""year"" = atv.""year""
)

SELECT
    ""year"",
    ""avg_transaction_value"" - ""avg_output_value"" AS ""difference""
FROM
    common_years
ORDER BY
    ""year"";
",,snowflake
113,sf_bq335,CRYPTO,"Among all Bitcoin addresses that have at least one transaction in October 2017 (combining both inputs and outputs), which address conducted its final transaction on the latest date in that month, and, among any addresses sharing that same latest date, which one has the highest sum of transaction values?",,,snowflake
114,sf_bq057,CRYPTO,"Which month (e.g., 3 for March) in 2021 witnessed the highest percentage of Bitcoin transaction volume occurring in CoinJoin transactions (defined as transactions with >2 outputs, output value ≤ input value, and having multiple equal-value outputs)? Also provide the percentage of all Bitcoin transactions that were CoinJoins, the percentage of UTXOs involved in CoinJoin transactions (average of input and output percentages), and the percentage of total Bitcoin volume that occurred in CoinJoin transactions for that month. Round all percentages to 1 decimal place.","WITH totals AS (
    -- Aggregate monthly totals for Bitcoin txs, input/output UTXOs,
    -- and input/output values (UTXO stands for Unspent Transaction Output)
    SELECT
        ""txs_tot"".""block_timestamp_month"" AS tx_month,
        COUNT(""txs_tot"".""hash"") AS tx_count,
        SUM(""txs_tot"".""input_count"") AS tx_inputs,
        SUM(""txs_tot"".""output_count"") AS tx_outputs,
        SUM(""txs_tot"".""input_value"") / 100000000 AS tx_input_val,
        SUM(""txs_tot"".""output_value"") / 100000000 AS tx_output_val
    FROM CRYPTO.CRYPTO_BITCOIN.TRANSACTIONS AS ""txs_tot""
    WHERE ""txs_tot"".""block_timestamp_month"" BETWEEN CAST('2021-01-01' AS DATE) AND CAST('2021-12-31' AS DATE)
    GROUP BY ""txs_tot"".""block_timestamp_month""
    ORDER BY ""txs_tot"".""block_timestamp_month"" DESC
),
coinjoinOuts AS (
    -- Builds a table where each row represents an output of a 
    -- potential CoinJoin tx, defined as a tx that had more 
    -- than two outputs and had a total output value less than its
    -- input value, per Adam Fiscor's description in this article: 
    SELECT 
        ""txs"".""hash"",
        ""txs"".""block_number"",
        ""txs"".""block_timestamp_month"",
        ""txs"".""input_count"",
        ""txs"".""output_count"",
        ""txs"".""input_value"",
        ""txs"".""output_value"",
        ""o"".value:""value"" AS ""outputs_val""
    FROM CRYPTO.CRYPTO_BITCOIN.TRANSACTIONS AS ""txs"", 
         LATERAL FLATTEN(INPUT => ""txs"".""outputs"") AS ""o""
    WHERE ""txs"".""output_count"" > 2 
      AND ""txs"".""output_value"" <= ""txs"".""input_value""
      AND ""txs"".""block_timestamp_month"" BETWEEN CAST('2021-01-01' AS DATE) AND CAST('2021-12-31' AS DATE)
    ORDER BY ""txs"".""block_number"", ""txs"".""hash"" DESC
),
coinjoinTxs AS (
    -- Builds a table of just the distinct CoinJoin tx hashes
    -- which had more than one equal-value output.
    SELECT 
        ""coinjoinouts"".""hash"" AS ""cjhash"",
        ""coinjoinouts"".""outputs_val"" AS outputVal,
        COUNT(*) AS cjOuts
    FROM coinjoinOuts AS ""coinjoinouts""
    GROUP BY ""coinjoinouts"".""hash"", ""coinjoinouts"".""outputs_val""
    HAVING COUNT(*) > 1
),
coinjoinsD AS (
    -- Filter out all potential CoinJoin txs that did not have
    -- more than one equal-value output. Do not list the
    -- outputs themselves, only the distinct tx hashes and
    -- their input/output counts and values.
    SELECT DISTINCT 
        ""coinjoinouts"".""hash"", 
        ""coinjoinouts"".""block_number"", 
        ""coinjoinouts"".""block_timestamp_month"",
        ""coinjoinouts"".""input_count"",
        ""coinjoinouts"".""output_count"",
        ""coinjoinouts"".""input_value"",
        ""coinjoinouts"".""output_value""
    FROM coinjoinOuts AS ""coinjoinouts""
    INNER JOIN coinjoinTxs AS ""coinjointxs"" 
        ON ""coinjoinouts"".""hash"" = ""coinjointxs"".""cjhash""
),
coinjoins AS (
    -- Aggregate monthly totals for CoinJoin txs, input/output UTXOs,
    -- and input/output values
    SELECT 
        ""cjs"".""block_timestamp_month"" AS cjs_month,
        COUNT(""cjs"".""hash"") AS cjs_count,
        SUM(""cjs"".""input_count"") AS cjs_inputs,
        SUM(""cjs"".""output_count"") AS cjs_outputs,
        SUM(""cjs"".""input_value"") / 100000000 AS cjs_input_val,
        SUM(""cjs"".""output_value"") / 100000000 AS cjs_output_val
    FROM coinjoinsD AS ""cjs""
    GROUP BY ""cjs"".""block_timestamp_month""
    ORDER BY ""cjs"".""block_timestamp_month"" DESC
)
SELECT EXTRACT(MONTH FROM tx_month) AS month,
    -- Calculate resulting CoinJoin percentages:
    -- tx_percent = percent of monthly Bitcoin txs that were CoinJoins
    ROUND(coinjoins.cjs_count / totals.tx_count * 100, 1) AS tx_percent,
    
    -- utxos_percent = percent of monthly Bitcoin utxos that were CoinJoins
    ROUND((coinjoins.cjs_inputs / totals.tx_inputs + coinjoins.cjs_outputs / totals.tx_outputs) / 2 * 100, 1) AS utxos_percent,
    
    -- value_percent = percent of monthly Bitcoin volume that took place
    -- in CoinJoined transactions
    ROUND(coinjoins.cjs_input_val / totals.tx_input_val * 100, 1) AS value_percent
FROM totals
INNER JOIN coinjoins
    ON totals.tx_month = coinjoins.cjs_month
ORDER BY value_percent DESC
LIMIT 1;
",,snowflake
115,sf_bq068,CRYPTO,"Using double-entry bookkeeping principles by treating transaction inputs as debits (negative values) and outputs as credits (positive values) for all Bitcoin Cash transactions between 2014-03-01 and 2014-04-01, how can we calculate the maximum and minimum final balances grouped by address type from these transactions?","WITH double_entry_book AS (
    -- debits
    SELECT
        ARRAY_TO_STRING(""inputs"".value:addresses, ',') AS ""address"",  -- Use the correct JSON path notation
        ""inputs"".value:type AS ""type"",
        - ""inputs"".value:value AS ""value""
    FROM CRYPTO.CRYPTO_BITCOIN_CASH.TRANSACTIONS,
         LATERAL FLATTEN(INPUT => ""inputs"") AS ""inputs""
    WHERE TO_TIMESTAMP(""block_timestamp"" / 1000000) >= '2014-03-01' 
      AND TO_TIMESTAMP(""block_timestamp"" / 1000000) < '2014-04-01'

    UNION ALL
 
    -- credits
    SELECT
        ARRAY_TO_STRING(""outputs"".value:addresses, ',') AS ""address"",  -- Use the correct JSON path notation
        ""outputs"".value:type AS ""type"",
        ""outputs"".value:value AS ""value""
    FROM CRYPTO.CRYPTO_BITCOIN_CASH.TRANSACTIONS, 
         LATERAL FLATTEN(INPUT => ""outputs"") AS ""outputs""
    WHERE TO_TIMESTAMP(""block_timestamp"" / 1000000) >= '2014-03-01' 
      AND TO_TIMESTAMP(""block_timestamp"" / 1000000) < '2014-04-01'
),
address_balances AS (
    SELECT 
        ""address"",
        ""type"",
        SUM(""value"") AS ""balance""
    FROM double_entry_book
    GROUP BY ""address"", ""type""
),
max_min_balances AS (
    SELECT
        ""type"",
        MAX(""balance"") AS max_balance,
        MIN(""balance"") AS min_balance
    FROM address_balances
    GROUP BY ""type""
)
SELECT
    REPLACE(""type"", '""', '') AS ""type"",  -- Replace double quotes with nothing
    max_balance,
    min_balance
FROM max_min_balances
ORDER BY ""type"";
",,snowflake
116,sf_bq092,CRYPTO,"In April 2023, what are the highest and lowest balances across all Dash addresses when calculating the net balance for each address using double-entry bookkeeping (where inputs are treated as debits/negative values and outputs as credits/positive values)? Consider all transactions filtered by block_timestamp_month='2023-04-01', and when an address appears as an array in the data, concatenate the array elements into a comma-separated string. For each address and type combination, sum all the values to determine the balance.",,,snowflake
117,sf_bq093,CRYPTO,"What were the maximum and minimum net balance changes for Ethereum Classic addresses on October 14, 2016? Calculate these by summing all transactions where addresses received funds (debits), sent funds (credits), and paid or received gas fees. Only include successful status transactions and exclude internal calls of types. For gas fees, consider both the fees paid by transaction senders and received by miners, calculated as multiplied by the gas price for both miners and senders","WITH double_entry_book AS (
    -- Debits
    SELECT 
        ""to_address"" AS ""address"", 
        ""value"" AS ""value""
    FROM 
        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRACES
    WHERE 
        ""to_address"" IS NOT NULL
        AND ""status"" = 1
        AND (""call_type"" NOT IN ('delegatecall', 'callcode', 'staticcall') OR ""call_type"" IS NULL)
        AND TO_DATE(TO_TIMESTAMP(""block_timestamp"" / 1000000)) = '2016-10-14'

    UNION ALL
    
    -- Credits
    SELECT 
        ""from_address"" AS ""address"", 
        - ""value"" AS ""value""
    FROM 
        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRACES
    WHERE 
        ""from_address"" IS NOT NULL
        AND ""status"" = 1
        AND (""call_type"" NOT IN ('delegatecall', 'callcode', 'staticcall') OR ""call_type"" IS NULL)
        AND TO_DATE(TO_TIMESTAMP(""block_timestamp"" / 1000000)) = '2016-10-14'

    UNION ALL

    -- Transaction Fees Debits
    SELECT 
        ""miner"" AS ""address"", 
        SUM(CAST(""receipt_gas_used"" AS NUMERIC) * CAST(""gas_price"" AS NUMERIC)) AS ""value""
    FROM 
        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRANSACTIONS AS ""transactions""
    JOIN 
        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.BLOCKS AS ""blocks"" 
        ON ""blocks"".""number"" = ""transactions"".""block_number""
    WHERE 
        TO_DATE(TO_TIMESTAMP(""block_timestamp"" / 1000000)) = '2016-10-14'
    GROUP BY 
        ""blocks"".""miner""

    UNION ALL
    
    -- Transaction Fees Credits
    SELECT 
        ""from_address"" AS ""address"", 
        -(CAST(""receipt_gas_used"" AS NUMERIC) * CAST(""gas_price"" AS NUMERIC)) AS ""value""
    FROM 
        CRYPTO.CRYPTO_ETHEREUM_CLASSIC.TRANSACTIONS
    WHERE 
        TO_DATE(TO_TIMESTAMP(""block_timestamp"" / 1000000)) = '2016-10-14'
),
net_changes AS (
    SELECT 
        ""address"",
        SUM(""value"") AS ""net_change""
    FROM 
        double_entry_book
    GROUP BY 
        ""address""
)
SELECT 
    MAX(""net_change"") AS ""max_net_change"",
    MIN(""net_change"") AS ""min_net_change""
FROM
    net_changes;",,snowflake
118,sf_bq292,CRYPTO,"Analyze Bitcoin transactions since July 2023 to determine monthly percentages of: (1)Transactions classified as CoinJoins (defined by >2 outputs, output value ≤ input value, and multiple identical-value outputs), (2) UTXOs involved in CoinJoins (calculated as the average of CoinJoin input/output ratios against total network UTXOs), (3) Transaction volume (based on input value) attributed to CoinJoins. Provide results in a table with monthly metrics for transactions, UTXOs, and volume.",,,snowflake
119,sf_bq135,CRYPTO,Which date before 2022 had the highest total transaction amount in the Zilliqa blockchain data?,,,snowflake
120,sf_bq136,CRYPTO,"Find all exactly 2-hop transaction paths on Zilliqa blockchain between the source address 'zil1jrpjd8pjuv50cfkfr7eu6yrm3rn5u8rulqhqpz' and destination address 'zil19nmxkh020jnequql9kvqkf3pkwm0j0spqtd26e', considering both regular transactions and contract transitions. A 2-hop path means there must be an intermediate address between source and destination. Exclude paths where any intermediate address has more than 50 outgoing transactions to filter out exchanges and high-activity wallets. Ensure transactions in each path follow chronological order (earlier transaction timestamps first). Display results in the format: '<source> --(tx ABCDE..)--> <intermediate> --(tx FGHIJ..)--> <destination>' where the transaction IDs are truncated to the first 5 characters. Include only confirmed on-chain transactions in both steps of the path.",,,snowflake
121,sf_bq065,CRYPTO,"From the oracle requests table, retrieve the 10 most recent oracle requests with script ID 3. For each request, extract all symbol-rate pairs by matching each symbol in the ""symbols"" array with its corresponding rate at the same position in the ""rates"" array from the decoded result. Adjust each rate by dividing it by the request's multiplier value. Return the block timestamp, oracle request ID, symbol, and the adjusted rate for each symbol-rate pair. Sort the results in chronological order with the newest records first.",,,snowflake
122,sf_bq037,HUMAN_GENOME_VARIANTS,"About the refined human genetic variations collected in phase 3 on 2015-02-20, I want to know the minimum and maximum start positions as well as the proportions of these two respectively for reference bases 'AT' and 'TA'.","WITH A AS (
    SELECT
        ""reference_bases"",
        ""start_position""
    FROM
        ""HUMAN_GENOME_VARIANTS"".""HUMAN_GENOME_VARIANTS"".""_1000_GENOMES_PHASE_3_OPTIMIZED_SCHEMA_VARIANTS_20150220""
    WHERE
        ""reference_bases"" IN ('AT', 'TA')
),
B AS (
    SELECT
        ""reference_bases"",
        MIN(""start_position"") AS ""min_start_position"",
        MAX(""start_position"") AS ""max_start_position"",
        COUNT(1) AS ""total_count""
    FROM
        A
    GROUP BY
        ""reference_bases""
),
min_counts AS (
    SELECT
        A.""reference_bases"",  -- Explicitly referencing the column from table A
        A.""start_position"" AS ""min_start_position"",
        COUNT(1) AS ""min_count""
    FROM
        A
    INNER JOIN B 
        ON A.""reference_bases"" = B.""reference_bases""
    WHERE
        A.""start_position"" = B.""min_start_position""
    GROUP BY
        A.""reference_bases"", A.""start_position""
),
max_counts AS (
    SELECT
        A.""reference_bases"",  -- Explicitly referencing the column from table A
        A.""start_position"" AS ""max_start_position"",
        COUNT(1) AS ""max_count""
    FROM
        A
    INNER JOIN B
        ON A.""reference_bases"" = B.""reference_bases""
    WHERE
        A.""start_position"" = B.""max_start_position""
    GROUP BY
        A.""reference_bases"", A.""start_position""
)
SELECT
    B.""reference_bases"",  -- Explicitly referencing the column from table B
    B.""min_start_position"",
    CAST(min_counts.""min_count"" AS FLOAT) / B.""total_count"" AS ""min_position_ratio"",
    B.""max_start_position"",
    CAST(max_counts.""max_count"" AS FLOAT) / B.""total_count"" AS ""max_position_ratio""
FROM
    B
LEFT JOIN
    min_counts ON B.""reference_bases"" = min_counts.""reference_bases"" AND B.""min_start_position"" = min_counts.""min_start_position""
LEFT JOIN
    max_counts ON B.""reference_bases"" = max_counts.""reference_bases"" AND B.""max_start_position"" = max_counts.""max_start_position""
ORDER BY
    B.""reference_bases"";
",,snowflake
123,sf_bq012,ETHEREUM_BLOCKCHAIN,"Calculate the average balance (in quadrillions, 10^15) of the top 10 Ethereum addresses by net balance, including incoming and outgoing transfers from traces (only successful transactions and excluding call types like delegatecall, callcode, and staticcall), miner rewards (sum of gas fees per block), and sender gas fee deductions. Exclude null addresses and round the result to two decimal places.","WITH double_entry_book AS (
  -- Debits
  SELECT 
    ""to_address"" AS ""address"",
    ""value"" AS ""value""
  FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TRACES""
  WHERE ""to_address"" IS NOT NULL
    AND ""status"" = 1
    AND (""call_type"" NOT IN ('delegatecall', 'callcode', 'staticcall') OR ""call_type"" IS NULL)
  
  UNION ALL
  
  -- Credits
  SELECT 
    ""from_address"" AS ""address"",
    - ""value"" AS ""value""
  FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TRACES""
  WHERE ""from_address"" IS NOT NULL
    AND ""status"" = 1
    AND (""call_type"" NOT IN ('delegatecall', 'callcode', 'staticcall') OR ""call_type"" IS NULL)
  
  UNION ALL
  
  -- Transaction fees debits
  SELECT 
    ""miner"" AS ""address"",
    SUM(CAST(""receipt_gas_used"" AS NUMBER) * CAST(""gas_price"" AS NUMBER)) AS ""value""
  FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TRANSACTIONS"" AS ""transactions""
  JOIN ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""BLOCKS"" AS ""blocks""
    ON ""blocks"".""number"" = ""transactions"".""block_number""
  GROUP BY ""blocks"".""miner""
  
  UNION ALL
  
  -- Transaction fees credits
  SELECT 
    ""from_address"" AS ""address"",
    -(CAST(""receipt_gas_used"" AS NUMBER) * CAST(""gas_price"" AS NUMBER)) AS ""value""
  FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TRANSACTIONS""
),
top_10_balances AS (
  SELECT
    ""address"",
    SUM(""value"") AS ""balance""
  FROM double_entry_book
  GROUP BY ""address""
  ORDER BY ""balance"" DESC
  LIMIT 10
)
SELECT 
    ROUND(AVG(""balance"") / 1e15, 2) AS ""average_balance_trillion""
FROM top_10_balances;
",,snowflake
124,sf_bq187,ETHEREUM_BLOCKCHAIN,"Calculate the total circulating supply of 'BNB' tokens (in units divided by 10^18) by summing balances of all non-zero addresses, where each address’s balance equals its total received BNB minus sent BNB. Exclude transactions involving the zero address (0x000...) for both senders and receivers.","WITH tokenInfo AS (
    SELECT ""address""
    FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TOKENS""
    WHERE ""name"" = 'BNB'
),

receivedTx AS (
    SELECT ""tx"".""to_address"" AS ""addr"", 
           ""tokens"".""name"" AS ""name"", 
           SUM(CAST(""tx"".""value"" AS FLOAT) / POWER(10, 18)) AS ""amount_received""
    FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TOKEN_TRANSFERS"" AS ""tx""
    JOIN tokenInfo ON ""tx"".""token_address"" = tokenInfo.""address""
    JOIN ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TOKENS"" AS ""tokens""
      ON ""tx"".""token_address"" = ""tokens"".""address""
    WHERE ""tx"".""to_address"" <> '0x0000000000000000000000000000000000000000'
    GROUP BY ""tx"".""to_address"", ""tokens"".""name""
),

sentTx AS (
    SELECT ""tx"".""from_address"" AS ""addr"", 
           ""tokens"".""name"" AS ""name"", 
           SUM(CAST(""tx"".""value"" AS FLOAT) / POWER(10, 18)) AS ""amount_sent""
    FROM ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TOKEN_TRANSFERS"" AS ""tx""
    JOIN tokenInfo ON ""tx"".""token_address"" = tokenInfo.""address""
    JOIN ""ETHEREUM_BLOCKCHAIN"".""ETHEREUM_BLOCKCHAIN"".""TOKENS"" AS ""tokens""
      ON ""tx"".""token_address"" = ""tokens"".""address""
    WHERE ""tx"".""from_address"" <> '0x0000000000000000000000000000000000000000'
    GROUP BY ""tx"".""from_address"", ""tokens"".""name""
),

walletBalances AS (
    SELECT r.""addr"",
           COALESCE(SUM(r.""amount_received""), 0) - COALESCE(SUM(s.""amount_sent""), 0) AS ""balance""
    FROM receivedTx AS r
    LEFT JOIN sentTx AS s
      ON r.""addr"" = s.""addr""
    GROUP BY r.""addr""
)

SELECT 
    SUM(""balance"") AS ""circulating_supply""
FROM walletBalances;
",,snowflake
125,sf_bq450,ETHEREUM_BLOCKCHAIN,"Generate a comprehensive report of all Ethereum addresses active before January 1, 2017, calculating their net balances (adjusted for transaction fees and excluding delegatecall/callcode/staticcall transactions), hourly activity patterns, active days, incoming/outgoing transaction metrics (counts, unique counterparties, average ETH transfers), ERC20 token interactions (in/out counts, unique tokens, counterparties), mining rewards, contract creation frequency, failed transaction counts, and contract bytecode sizes, with all ETH values converted to standard units (divided by 10^18) and excluding addresses with no transaction history.",,"### Revised Task Requirement Document

#### Overview
This document outlines the data requirements and analytical objectives for evaluating Ethereum blockchain data. The purpose is to assess various aspects of address activity including transaction behaviors, balances, and contract interactions to derive comprehensive insights into Ethereum address dynamics.

#### Data Integration and Analysis Requirements
- **Address Activity Analysis**:
  - **Objective**: Analyze the activity of each address until January 1, 2017.
  - **Methodology**: For addresses with significant activity (more than 24 activities), calculate the hourly activity consistency using trigonometric functions to gauge uniformity in activity throughout the day. This involves computing the root mean square of the sum of cosines and sines of the transaction hours, normalized by the number of transactions, to measure activity concentration.
  - **Metrics**:
    - `R_active_hour`: Consistency of hourly activity.
    - `active_days`: Number of active days for each address.

- **Balance Analysis**:
  - **Objective**: Compute the net balance for each address by considering tokens received, tokens sent, and transaction fees.
  - **Methodology**: Include successful transactions only. Exclude transactions of specific Ethereum call types (`delegatecall`, `callcode`, `staticcall`) unless they are null. Calculate the balance by subtracting the total values of tokens sent from tokens received and adjusting for transaction fees.
  - **Metrics**:
    - `balance`: Net balance of each address, adjusted for scale by dividing by \(10^{18}\) to convert from Wei to Ether.

- **Token Transaction Analysis**:
  - **Objective**: Quantify both incoming and outgoing token transactions to and from each address.
  - **Methodology**: Count the number of transactions involving token transfers, distinct types of tokens, and distinct counterparty addresses involved in these transactions.
  - **Metrics**:
    - `token_in_tnx`: Number of incoming token transactions.
    - `token_out_tnx`: Number of outgoing token transactions.
    - `token_in_type`: Types of tokens received.
    - `token_out_type`: Types of tokens sent.

- **General Transaction Analysis**:
  - **Objective**: Analyze all transactions, both incoming and outgoing, for each address.
  - **Methodology**: Calculate the total number of transactions, identify unique counterpart addresses, and measure transactions with non-zero values. For incoming transactions, specifically compute the average and standard deviation of gas used for ""call"" type transactions.
  - **Metrics**:
    - `in_trace_count`/`out_trace_count`: Number of incoming/outgoing transactions.
    - `in_addr_count`/`out_addr_count`: Number of unique incoming/outgoing addresses.
    - `in_transfer_count`/`out_transfer_count`: Number of incoming/outgoing transactions with non-zero value.
    - `in_avg_amount`/`out_avg_amount`: Average amount of incoming/outgoing transactions, scaled by \(10^{18}\).

- **Mining Rewards and Contract Creation Analysis**:
  - **Objective**: Summarize total mining rewards and count the number of contracts created by each address, excluding transaction fees from the rewards.
  - **Metrics**:
    - `reward_amount`: Total mining rewards received, adjusted for Wei to Ether.
    - `contract_create_count`: Number of contracts created by the address.

- **Failure and Bytecode Analysis**:
  - **Objective**: Record failures and bytecode analysis for contracts.
  - **Methodology**: Count the number of failed transactions and calculate the bytecode length for contracts created by the address.
  - **Metrics**:
    - `failure_count`: Number of failed transactions initiated by the address.
    - `bytecode_size`: Length of the bytecode for contracts associated with the address.

#### Final Report Format and Content
The final report will present a detailed analysis with the following columns:

- **Address (`address`)**: Ethereum address of the user.
- **Balance (`balance`)**: Net balance of the address, adjusted for scale by dividing by \(10^{18}\) to convert from Wei to Ether.
- **Hourly Activity Consistency (`R_active_hour`)**: Calculated value representing the uniformity of activity across different hours using trigonometric measures.
- **Active Days (`active_days`)**: Total number of days the address showed activity.
- **Incoming Trace Count (`in_trace_count`)**: Number of incoming transactions to the address.
- **Unique Incoming Addresses (`in_addr_count`)**: Number of unique addresses that have sent transactions to the address.
- **Incoming Transfers (`in_transfer_count`)**: Count of incoming transactions with non-zero value.
- **Average Incoming Amount (`in_avg_amount`)**: Average amount of incoming transactions, scaled by \(10^{18}\) to convert from Wei to Ether.
- **Average Gas Used (`avg_gas_used`)**: Average gas used in incoming ""call"" transactions.
- **Standard Deviation of Gas Used (`std_gas_used`)**: Standard deviation of gas used in incoming ""call"" transactions.
- **Outgoing Trace Count (`out_trace_count`)**: Number of outgoing transactions from the address.
- **Unique Outgoing Addresses (`out_addr_count`)**: Number of unique addresses that have received transactions from the address.
- **Outgoing Transfers (`out_transfer_count`)**: Count of outgoing transactions with non-zero value.
- **Average Outgoing Amount (`out_avg_amount`)**: Average amount of outgoing transactions, scaled by \(10^{18}\) to convert from Wei to Ether.
- **Incoming Token Transactions (`token_in_tnx`)**: Total number of token transactions received by the address.
- **Incoming Token Types (`token_in_type`)**: Number of different types of tokens received.
- **Distinct Incoming Token Senders (`token_from_addr`)**: Number of distinct addresses from which tokens were received.
- **Outgoing Token Transactions (`token_out_tnx`)**: Total number of token transactions sent by the address.
- **Outgoing Token Types (`token_out_type`)**: Number of different types of tokens sent.
- **Distinct Outgoing Token Receivers (`token_to_addr`)**: Number of distinct addresses to which tokens were sent.
- **Reward Amount (`reward_amount`)**: Total mining rewards received by the address, scaled by \(10^{18}\) to convert from Wei to Ether.
- **Contract Creation Count (`contract_create_count`)**: Number of contracts created by the address.
- **Failure Count (`failure_count`)**: Number of failed transactions initiated by the address.
- **Bytecode Size (`bytecode_size`)**: Length of the bytecode for contracts associated with the address.

#### Additional Notes
- All data operations should prioritize performance and accuracy, particularly in processing large datasets.
- Ensure integrity and precision in the data to facilitate reliable analytics and decision-making. 

This document provides the necessary information for an experienced SQL engineer to design and implement the queries needed to compile the final report as specified, focusing on clarity and detail in data representation.


















",snowflake
126,bq034,ghcn_d,"I want to know the IDs, names of weather stations within a 50 km straight-line distance from the center of Chicago (41.8319°N, 87.6847°W)","WITH params AS (
  SELECT ST_GeogPoint(-87.6847, 41.8319) AS center,
         50 AS maxdist_km
),
distance_from_center AS (
  SELECT
    id,
    name,
    state,
    ST_GeogPoint(longitude, latitude) AS loc,
    ST_Distance(ST_GeogPoint(longitude, latitude), params.center) AS dist_meters
  FROM
    `bigquery-public-data.ghcn_d.ghcnd_stations`,
    params
  WHERE ST_DWithin(ST_GeogPoint(longitude, latitude), params.center, params.maxdist_km*1000)
),
nearest_stations AS (
  SELECT 
    *, 
    RANK() OVER (ORDER BY dist_meters ASC) AS rank
  FROM 
    distance_from_center
),
nearest_nstations AS (
  SELECT 
    station.* 
  FROM 
    nearest_stations AS station, params
)
SELECT * from nearest_nstations",,snowflake
127,bq383,ghcn_d,"Could you provide the highest recorded precipitation, minimum temperature, and maximum temperature from the last 15 days of each year from 2013 to 2016 at weather station USW00094846? Ensure each value represents the peak measurement for that period, with precipitation in millimeters and temperatures in degrees Celsius, using only validated data (non-null values and no quality flags)","WITH data AS (
  SELECT
    EXTRACT(YEAR FROM wx.date) AS year,
    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,
    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,
    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax
  FROM
    `bigquery-public-data.ghcn_d.ghcnd_2013` AS wx
  WHERE
    wx.id = 'USW00094846' AND
    wx.qflag IS NULL AND
    wx.value IS NOT NULL AND
    DATE_DIFF(DATE('2013-12-31'), wx.date, DAY) < 15
  GROUP BY
    year

  UNION ALL

  SELECT
    EXTRACT(YEAR FROM wx.date) AS year,
    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,
    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,
    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax
  FROM
    `bigquery-public-data.ghcn_d.ghcnd_2014` AS wx
  WHERE
    wx.id = 'USW00094846' AND
    wx.qflag IS NULL AND
    wx.value IS NOT NULL AND
    DATE_DIFF(DATE('2014-12-31'), wx.date, DAY) < 15
  GROUP BY
    year

  UNION ALL

  SELECT
    EXTRACT(YEAR FROM wx.date) AS year,
    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,
    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,
    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax
  FROM
    `bigquery-public-data.ghcn_d.ghcnd_2015` AS wx
  WHERE
    wx.id = 'USW00094846' AND
    wx.qflag IS NULL AND
    wx.value IS NOT NULL AND
    DATE_DIFF(DATE('2015-12-31'), wx.date, DAY) < 15
  GROUP BY
    year

  UNION ALL

  SELECT
    EXTRACT(YEAR FROM wx.date) AS year,
    MAX(IF(wx.element = 'PRCP', wx.value/10, NULL)) AS max_prcp,
    MAX(IF(wx.element = 'TMIN', wx.value/10, NULL)) AS max_tmin,
    MAX(IF(wx.element = 'TMAX', wx.value/10, NULL)) AS max_tmax
  FROM
    `bigquery-public-data.ghcn_d.ghcnd_2016` AS wx
  WHERE
    wx.id = 'USW00094846' AND
    wx.qflag IS NULL AND
    wx.value IS NOT NULL AND
    DATE_DIFF(DATE('2016-12-31'), wx.date, DAY) < 15
  GROUP BY
    year
)

SELECT
  year,
  MAX(max_prcp) AS annual_max_prcp,
  MAX(max_tmin) AS annual_max_tmin,
  MAX(max_tmax) AS annual_max_tmax
FROM data
GROUP BY year
ORDER BY year ASC;
",,snowflake
128,bq051,new_york_ghcn,"Calculate the average daily number of Citibike trips in New York City during 2016, categorizing days as rainy if the total precipitation exceeds 5 millimeters (obtained by dividing the raw precipitation value by 10), and non-rainy otherwise. Use data from the nearest GHCN station located within 50 km of (40.7128, -74.0060) that has valid, unflagged measurements, then compare the resulting average Citibike trips on rainy days versus non-rainy days.",,,snowflake
129,bq038,new_york,"Identify the top 10 Citibike stations by highest proportion of group rides, defined as trips starting and ending at the same station where multiple riders departed/arrived within the same 2-minute time window. Calculate the proportion as the number of trips that are part of a group divided by the total number of trips ending at that station.",,,snowflake
130,bq053,new_york,"Calculate the change in the number of living trees of each fall color in New York City from 1995 to 2015 by computing, for each tree species, the difference between the number of trees not marked as dead in 1995 and the number of trees alive in 2015, matching species by the uppercase form of their scientific names from the tree_species table. Then, group the species by their fall color and sum these differences to determine the total change in the number of trees for each fall color.","SELECT
  c.fall_color,
  SUM(d.count_growth) AS change
FROM (
  SELECT
    fall_color,
    UPPER(species_scientific_name) AS latin
  FROM
    `bigquery-public-data.new_york.tree_species`)c
JOIN (
  SELECT
    IFNULL(a.upper_latin,
      b.upper_latin) AS latin,
    (IFNULL(count_2015,
        0)-IFNULL(count_1995,
        0)) AS count_growth
  FROM (
    SELECT
      UPPER(spc_latin) AS upper_latin,
      spc_common,
      COUNT(*) AS count_2015
    FROM
      `bigquery-public-data.new_york.tree_census_2015`
    WHERE
      status=""Alive""
    GROUP BY
      spc_latin,
      spc_common)a
  FULL OUTER JOIN (
    SELECT
      UPPER(spc_latin) AS upper_latin,
      COUNT(*) AS count_1995
    FROM
      `bigquery-public-data.new_york.tree_census_1995`
    WHERE
      status !=""Dead""
    GROUP BY
      spc_latin)b
  ON
    a.upper_latin=b.upper_latin
  ORDER BY
    count_growth DESC)d
ON
  d.latin=c.latin
GROUP BY
  fall_color
ORDER BY
  change DESC",,snowflake
131,bq054,new_york,"Please provide the top 10 tree species in New York, using their uppercase Latin names where the Latin name is not empty and including their common names, showing the total number of trees, the counts of alive and dead trees for each year, and the corresponding growth in these counts from 1995 to 2015, then order by the difference in total tree counts between these years.",,,snowflake
132,bq021,new_york,"For the top 20 Citi Bike routes in 2016, which route is faster than yellow taxis and among those, which one has the longest average bike duration? Please provide the start station name of this route. The coordinates are rounded to three decimals.","WITH top20route AS (
SELECT
  start_station_name, end_station_name, avg_bike_duration, avg_taxi_duration
FROM (
  SELECT
    start_station_name,
    end_station_name,
    ROUND(start_station_latitude, 3) AS ss_lat,
    ROUND(start_station_longitude, 3) AS ss_long,
    ROUND(end_station_latitude, 3) AS es_lat,
    ROUND(end_station_longitude, 3) AS es_long,
    AVG(tripduration) AS avg_bike_duration,
    COUNT(*) AS bike_trips
  FROM
    `bigquery-public-data.new_york.citibike_trips`
  WHERE 
  EXTRACT(YEAR from starttime) = 2016 AND
    start_station_name != end_station_name
  GROUP BY
    start_station_name, end_station_name, ss_lat, ss_long, es_lat, es_long
  ORDER BY
    bike_trips DESC
  LIMIT
    20
) a
JOIN (
  SELECT
    ROUND(pickup_latitude, 3) AS pu_lat,
    ROUND(pickup_longitude, 3) AS pu_long,
    ROUND(dropoff_latitude, 3) AS do_lat,
    ROUND(dropoff_longitude, 3) AS do_long,
    AVG(UNIX_SECONDS(dropoff_datetime)-UNIX_SECONDS(pickup_datetime)) AS avg_taxi_duration,
    COUNT(*) AS taxi_trips
  FROM
    `bigquery-public-data.new_york.tlc_yellow_trips_2016`
  GROUP BY
    pu_lat, pu_long, do_lat, do_long
) b
ON
  a.ss_lat = b.pu_lat AND
  a.es_lat = b.do_lat AND
  a.ss_long = b.pu_long AND
  a.es_long = b.do_long
)

SELECT start_station_name
FROM top20route
WHERE avg_bike_duration < avg_taxi_duration
ORDER BY
avg_bike_duration
DESC
LIMIT 1
",,snowflake
133,bq202,new_york_plus,"For the station that had the highest number of Citibike trips starting there in 2018, which numeric day of the week and which hour of the day had the greatest number of trips based on the start time of those trips?",,,snowflake
134,bq185,new_york_plus,"What is the average trip duration in minutes for all valid Yellow taxi trips that took place between February 1, 2016, and February 7, 2016 (inclusive), with a positive trip duration, more than three passengers, and a trip distance of at least ten miles, where both the pickup and dropoff locations are in Brooklyn?","SELECT 
    AVG(TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) / 60.0) AS average_trip_duration_in_minutes
FROM
(
    SELECT *
    FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016` t
    WHERE 
        pickup_datetime BETWEEN '2016-02-01' AND '2016-02-07' AND 
        dropoff_datetime BETWEEN '2016-02-01' AND '2016-02-07' AND
        TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) > 0 AND 
        passenger_count > 3 AND 
        trip_distance >= 10
) t
INNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz
ON t.pickup_location_id = tz.zone_id
INNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz1
ON t.dropoff_location_id = tz1.zone_id
WHERE 
    tz.borough = ""Brooklyn"" AND
    tz1.borough = ""Brooklyn"";
",,snowflake
135,bq040,new_york_plus,"For NYC yellow taxi trips between January 1 and January 7, 2016, excluding any trips picked up in ‘EWR’ or ‘Staten Island,’ determine the proportion of rides that fall into each tip category in each pickup borough. Only include trips where the dropoff time is after the pickup time, the passenger count is greater than zero, and trip_distance, tip_amount, tolls_amount, mta_tax, fare_amount, and total_amount are all non-negative. Classify the tip percentage as follows: 0% (no tip), up to 5%, 5% to 10%, 10% to 15%, 15% to 20%, 20% to 25%, and more than 25%.",,"## Tip Rate Calculation and Categorization

The `tip_rate` for each trip is calculated based on the total trip amount and the tip amount. The formula for calculating the tip rate is as follows:



After calculating the tip rate, the values are categorized into the following ranges:

- `no tip`: if `tip_rate = 0`
- `Less than 5%`: if `tip_rate <= 5`
- `5% to 10%`: if `tip_rate > 5` and `tip_rate <= 10`
- `10% to 15%`: if `tip_rate > 10` and `tip_rate <= 15`
- `15% to 20%`: if `tip_rate > 15` and `tip_rate <= 20`
- `20% to 25%`: if `tip_rate > 20` and `tip_rate <= 25`
- `More than 25%`: if `tip_rate > 25`
",snowflake
136,bq098,new_york_plus,"For NYC yellow taxi trips where both the pickup and dropoff occurred between January 1 and 7, 2016, inclusive, calculate the percentage of trips with no tip in each pickup borough, ensuring that only trips where the dropoff occurs after the pickup are included, the passenger count is greater than zero, and the trip distance, tip amount, tolls amount, MTA tax, fare amount, and total amount are non-negative; define ""no tip"" trips as those where the tip rate is zero, with the tip rate calculated as (tip_amount × 100) divided by total_amount (and considered zero when total_amount is zero).","WITH t2 AS
(
SELECT 
    t.*,
    t.pickup_location_id as pickup_zone_id,
    tz.borough as pickup_borough
FROM
(
SELECT *,
    TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) as time_duration_in_secs,
    (CASE WHEN total_amount=0 THEN 0
    ELSE (tip_amount*100/total_amount) END) as tip_rate
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`
) t
INNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz
ON t.pickup_location_id = tz.zone_id
WHERE 
    pickup_datetime BETWEEN '2016-01-01' AND '2016-01-07' 
    AND dropoff_datetime BETWEEN '2016-01-01' AND '2016-01-07'
    AND TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) > 0
    AND passenger_count > 0
    AND trip_distance >= 0 
    AND tip_amount >= 0 
    AND tolls_amount >= 0 
    AND mta_tax >= 0 
    AND fare_amount >= 0
    AND total_amount >= 0
),
t3 AS
(SELECT 
pickup_borough,
(CASE 
    WHEN tip_rate = 0 THEN 'no tip'
    WHEN tip_rate <= 5 THEN 'Less than 5%'
    WHEN tip_rate <= 10 THEN '5% to 10%'
    WHEN tip_rate <= 15 THEN '10% to 15%'
    WHEN tip_rate <= 20 THEN '15% to 20%'
    WHEN tip_rate <= 25 THEN '20% to 25%'
    ELSE 'More than 25%' END)as tip_category,
COUNT(*) as no_of_trips
FROM t2
GROUP BY 1,2
ORDER BY pickup_borough ASC),
INFO AS (
SELECT pickup_borough
     , tip_category
     , Sum(no_of_trips) as no_of_trips,
     (CASE 
          WHEN pickup_borough is null THEN (select sum(no_of_trips)
          FROM t3)
          
          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)
          FROM t3)
          
          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)
          FROM t3
          WHERE pickup_borough = m.pickup_borough)
          END) as parent_sum,
       (
          Sum(no_of_trips)
            /
          (
            CASE 
          WHEN pickup_borough is null THEN (select sum(no_of_trips)
          FROM t3)
          
          WHEN pickup_borough is not null and tip_category is null THEN (select sum(no_of_trips)
          FROM t3)
          
          WHEN pickup_borough is not null and tip_category is not null THEN (select sum(no_of_trips)
          FROM t3
          WHERE pickup_borough = m.pickup_borough)
          END
          )
        ) as percentage
FROM t3 m
GROUP BY ROLLUP(pickup_borough, tip_category)
order by 1, 2
)

SELECT 
    pickup_borough,
    (SUM(CASE WHEN tip_category = 'no tip' THEN no_of_trips ELSE 0 END) * 100.0 / SUM(no_of_trips)) AS percentage_no_tip
FROM t3
GROUP BY pickup_borough
ORDER BY pickup_borough;","## Tip Rate Calculation and Categorization

The `tip_rate` for each trip is calculated based on the total trip amount and the tip amount. The formula for calculating the tip rate is as follows:



After calculating the tip rate, the values are categorized into the following ranges:

- `no tip`: if `tip_rate = 0`
- `Less than 5%`: if `tip_rate <= 5`
- `5% to 10%`: if `tip_rate > 5` and `tip_rate <= 10`
- `10% to 15%`: if `tip_rate > 10` and `tip_rate <= 15`
- `15% to 20%`: if `tip_rate > 15` and `tip_rate <= 20`
- `20% to 25%`: if `tip_rate > 20` and `tip_rate <= 25`
- `More than 25%`: if `tip_rate > 25`
",snowflake
137,bq039,new_york_plus,"Find the top 10 taxi trips in New York City between July 1 and July 7, 2016 (ensuring both pickup and dropoff times fall within these dates) where the passenger count is greater than five, the trip distance is at least ten miles, and there are no negative fare-related amounts (including tip, tolls, mta tax, fare, and total costs). Exclude any trips where the dropoff time is not strictly after the pickup time, then sort the results by total fare amount in descending order. Finally, display each trip’s pickup zone, dropoff zone, trip duration in seconds, driving speed in miles per hour, and tip rate as a percentage of the total fare amount.","SELECT 
    tz.zone_name AS pickup_zone,
    tz1.zone_name AS dropoff_zone, 
    time_duration_in_secs,
    driving_speed_miles_per_hour,
    tip_rate
FROM
(
SELECT *,
    TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) as time_duration_in_secs,
    ROUND(trip_distance / (TIMESTAMP_DIFF(dropoff_datetime, pickup_datetime, SECOND) / 3600), 2) AS driving_speed_miles_per_hour,
    (CASE WHEN total_amount=0 THEN 0
          ELSE (tip_amount*100/total_amount) END) as tip_rate
FROM `bigquery-public-data.new_york_taxi_trips.tlc_yellow_trips_2016`
) t
INNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz
ON t.pickup_location_id = tz.zone_id
INNER JOIN `bigquery-public-data.new_york_taxi_trips.taxi_zone_geom` tz1
ON t.dropoff_location_id = tz1.zone_id
WHERE 
    pickup_datetime BETWEEN '2016-07-01' AND '2016-07-07' 
    AND dropoff_datetime BETWEEN '2016-07-01' AND '2016-07-07'
    AND TIMESTAMP_DIFF(dropoff_datetime,pickup_datetime,SECOND) > 0
    AND passenger_count > 5
    AND trip_distance >= 10
    AND tip_amount >= 0 
    AND tolls_amount >= 0 
    AND mta_tax >= 0 
    AND fare_amount >= 0
    AND total_amount >= 0
ORDER BY total_amount DESC
LIMIT 10;
",,snowflake
138,bq203,new_york_plus,"For each New York City borough, how many subway stations are there in total, how many have at least one entrance that is marked both as an actual entry and as ADA-compliant, and what percentage of the total stations in each borough does this represent, listing boroughs from the highest to the lowest percentage?","WITH stations_n_entrances AS (
      SELECT borough_name,s.station_name,entry,ada_compliant
      FROM `bigquery-public-data.new_york_subway.stations` s
      JOIN `bigquery-public-data.new_york_subway.station_entrances` se
      ON s.station_name = se.station_name
      )

SELECT se.borough_name, COUNT(DISTINCT se.station_name) num_stations,
      COUNT(DISTINCT adas.station_name) num_stations_w_compliant_entrance, 
      (100*COUNT(DISTINCT adas.station_name))/(COUNT(DISTINCT se.station_name)) percent_compliant_stations
FROM `stations_n_entrances` se
LEFT JOIN `stations_n_entrances` adas
ON se.station_name = adas.station_name
AND adas.entry AND adas.ada_compliant
GROUP BY 1
ORDER BY 4 DESC",,snowflake
139,bq035,san_francisco,"What is the total distance traveled by each bike in the San Francisco Bikeshare program, measured in meters? Use data from bikeshare trips and stations to calculate this.","SELECT
  bike_number, 
  AVG(dist_in_m) AS avg_dist_m, 
  SUM(dist_in_m) AS total_dist_m
FROM (
  SELECT
    ST_DISTANCE(
      ST_GEOGPOINT(start_lon, start_lat),
      ST_GEOGPOINT(end_lon, end_lat)
    ) AS dist_in_m,
    starts.bike_number
  FROM (
    SELECT 
      latitude AS start_lat,
      longitude AS start_lon,
      bike_number,
      trip_id
    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips
    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations
      ON trips.start_station_id = stations.station_id
  ) starts
  LEFT JOIN (
    SELECT 
      latitude AS end_lat,
      longitude AS end_lon,
      bike_number,
      trip_id
    FROM `bigquery-public-data.san_francisco.bikeshare_trips` trips
    LEFT JOIN `bigquery-public-data.san_francisco.bikeshare_stations` stations
      ON trips.end_station_id = stations.station_id
  ) ends ON ends.trip_id = starts.trip_id
)
GROUP BY bike_number
ORDER BY total_dist_m DESC",,snowflake
140,bq186,san_francisco,"Please find, for each year-month combination (in the format YYYYMM) derived from the start date of bike share trips in San Francisco, the first trip duration in minutes, the last trip duration in minutes, the highest trip duration in minutes, and the lowest trip duration in minutes, where ‘first’ and ‘last’ are determined by the chronological order of the trip start date, then group your results by this year-month and sort them by the same year-month key.",,,snowflake
141,bq081,san_francisco_plus,"Find the latest ride data for each region between 2014 and 2017. I want to know the name of each region, the trip ID of this ride, the ride duration, the start time, the starting station, and the gender of the rider.","SELECT t1.*
  FROM 
  (SELECT Trips.trip_id TripId,
               Trips.duration_sec TripDuration,
               Trips.start_date TripStartDate,
               Trips.start_station_name TripStartStation,
               Trips.member_gender Gender,
               Regions.name RegionName
          FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` Trips
         INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` StationInfo
            ON CAST(Trips.start_station_id AS STRING) = CAST(StationInfo.station_id AS STRING)
         INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` Regions
            ON StationInfo.region_id = Regions.region_id
         WHERE (EXTRACT(YEAR from Trips.start_date)) BETWEEN 2014 AND 2017
           ) 
           t1
 RIGHT JOIN (SELECT MAX(start_date) TripStartDate,
                   Regions.name RegionName
              FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` StationInfo
             INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` Trips
                ON CAST(StationInfo.station_id AS STRING) = CAST(Trips.start_station_id AS STRING)
             INNER JOIN `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` Regions
                ON Regions.region_id = StationInfo.region_id
                 WHERE (EXTRACT(YEAR from Trips.start_date) BETWEEN 2014 AND 2017
           AND Regions.name IS NOT NULL)
             GROUP BY RegionName) 
             t2
    ON t1.RegionName = t2.RegionName AND t1.TripStartDate = t2.TripStartDate",,snowflake
142,sf_bq294,SAN_FRANCISCO_PLUS,"Could you provide the details of the top 5 longest bike share trips that started between July 1, 2017, and December 31, 2017, including the trip ID, duration in seconds, start date, start station name, route (derived from start station name to end station name), bike number, subscriber type, member's birth year, the member's current age (calculated using the current year), an age classification based on whether the member is younger than 40, between 40 and 60, or older than 60, the member's gender, and the name of the region of the start station? Please exclude any trips where the start station name, member's birth year, or member's gender is not specified.","SELECT
  ""trip_id"",
  ""duration_sec"",
  DATE(TO_TIMESTAMP_LTZ(""start_date"" / 1000000)) AS ""star_date"", -- 将微秒转换为日期
  ""start_station_name"",
  CONCAT(""start_station_name"", ' - ', ""end_station_name"") AS ""route"",
  ""bike_number"",
  ""subscriber_type"",
  ""member_birth_year"",
  (EXTRACT(YEAR FROM CURRENT_DATE()) - ""member_birth_year"") AS ""age"",
  CASE
    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - ""member_birth_year"") < 40 THEN 'Young (<40 Y.O)'
    WHEN (EXTRACT(YEAR FROM CURRENT_DATE()) - ""member_birth_year"") BETWEEN 40 AND 60 THEN 'Adult (40-60 Y.O)'
    ELSE 'Senior Adult (>60 Y.O)'
  END AS ""age_class"",
  ""member_gender"",
  c.""name"" AS ""region_name""
FROM ""SAN_FRANCISCO_PLUS"".""SAN_FRANCISCO_BIKESHARE"".""BIKESHARE_TRIPS"" a
LEFT JOIN ""SAN_FRANCISCO_PLUS"".""SAN_FRANCISCO_BIKESHARE"".""BIKESHARE_STATION_INFO"" b 
  ON a.""start_station_name"" = b.""name""
LEFT JOIN ""SAN_FRANCISCO_PLUS"".""SAN_FRANCISCO_BIKESHARE"".""BIKESHARE_REGIONS"" c 
  ON b.""region_id"" = c.""region_id""
WHERE TO_TIMESTAMP_LTZ(""start_date"" / 1000000) BETWEEN '2017-07-01' AND '2017-12-31'
  AND b.""name"" IS NOT NULL
  AND ""member_birth_year"" IS NOT NULL
  AND ""member_gender"" IS NOT NULL
ORDER BY ""duration_sec"" DESC
LIMIT 5;
","# Details of bike share trips

- trip ID
- duration (s)
- start date
- start station name
  - Not null
- route : `start_station_name` - `end_station_name` (there is "" - "" between two strings of names)
- bike number
- subscriber type
- member birth year
  - Not null
- age := current year - birth year
  - Not null
- age class
  - age < 40 : 'Young (<40 Y.O)'
  - 40 <= age <= 60 : 'Adult (40-60 Y.O)'
  - age > 60 : 'Senior Adult (>60 Y.O)'
- member gender
  - Not null
- region name",snowflake
143,bq339,san_francisco_plus,"Which month in 2017 had the largest absolute difference between cumulative bike usage minutes for customers and subscribers?	Which month (in number) in 2017 had the largest absolute difference between cumulative bike usage minutes (in thousands) for customers and subscribers, based on the trip end dates in the San Francisco bikeshare data?","WITH monthly_totals AS (
  SELECT
    SUM(CASE WHEN subscriber_type = 'Customer' THEN duration_sec / 60 ELSE NULL END) AS customer_minutes_sum,
    SUM(CASE WHEN subscriber_type = 'Subscriber' THEN duration_sec / 60 ELSE NULL END) AS subscriber_minutes_sum,
    EXTRACT(MONTH FROM end_date) AS end_month
  FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips`
  WHERE
    EXTRACT(YEAR FROM end_date) = 2017
  GROUP BY
    end_month
),

cumulative_totals AS (
  SELECT
    end_month,
    SUM(customer_minutes_sum) OVER (ORDER BY end_month ROWS UNBOUNDED PRECEDING) / 1000 AS cumulative_minutes_cust,
    SUM(subscriber_minutes_sum) OVER (ORDER BY end_month ROWS UNBOUNDED PRECEDING) / 1000 AS cumulative_minutes_sub
  FROM
    monthly_totals
),

differences AS (
  SELECT
    end_month,
    ABS(cumulative_minutes_cust - cumulative_minutes_sub) AS abs_diff
  FROM
    cumulative_totals
)

SELECT
  end_month
FROM
  differences
ORDER BY
  abs_diff DESC
LIMIT 1;",,snowflake
144,bq400,san_francisco_plus,"For trips where 'Clay St & Drumm St' occurs before 'Sacramento St & Davis St' in the stop sequence (one direction only), what are the earliest departure times from 'Clay St & Drumm St' and the latest arrival times at 'Sacramento St & Davis St' in the format HH:MM:SS? Please provide the trip headsign for each route.","WITH SelectedStops AS (
  SELECT 
      stop_id,
      stop_name
  FROM 
      `bigquery-public-data.san_francisco_transit_muni.stops`
  WHERE 
      stop_name IN ('Clay St & Drumm St', 'Sacramento St & Davis St')
),
FilteredStopTimes AS (
  SELECT 
      st.trip_id, 
      st.stop_id, 
      st.arrival_time, 
      st.departure_time, 
      st.stop_sequence, 
      ss.stop_name
  FROM 
      `bigquery-public-data.san_francisco_transit_muni.stop_times` st
  JOIN 
      SelectedStops ss ON CAST(st.stop_id AS STRING) = ss.stop_id
)
SELECT
    t.trip_headsign,
    MIN(st1.departure_time) AS start_time,
    MAX(st2.arrival_time) AS end_time
FROM 
    `bigquery-public-data.san_francisco_transit_muni.trips` t
JOIN FilteredStopTimes st1 ON t.trip_id = CAST(st1.trip_id AS STRING) AND st1.stop_name = 'Clay St & Drumm St'
JOIN FilteredStopTimes st2 ON t.trip_id = CAST(st2.trip_id AS STRING) AND st2.stop_name = 'Sacramento St & Davis St'
WHERE 
    st1.stop_sequence < st2.stop_sequence
GROUP BY 
    t.trip_headsign;",,snowflake
145,bq059,san_francisco_plus,"What is the highest average speed (rounded to 1 decimal, in metric m/s) for bike trips in Berkeley with trip distance greater than 1000 meters?","WITH stations AS (
  SELECT station_id
  FROM
    `bigquery-public-data.san_francisco_bikeshare.bikeshare_station_info` AS stainfo
  WHERE stainfo.region_id = (
    SELECT region.region_id
    FROM `bigquery-public-data.san_francisco_bikeshare.bikeshare_regions` AS region
    WHERE region.name = ""Berkeley""
  )
),
meta_data AS (
    SELECT
        round(st_distance(start_station_geom, end_station_geom), 1) as distancia_metros,
        round(st_distance(start_station_geom, end_station_geom) / duration_sec, 1) as velocidade_media
    FROM
        `bigquery-public-data.san_francisco_bikeshare.bikeshare_trips` AS trips
    WHERE
        cast(trips.start_station_id as string) IN (SELECT station_id FROM stations)
        AND cast(trips.end_station_id as string) IN (SELECT station_id FROM stations)
        AND start_station_latitude IS NOT NULL
        AND start_station_longitude IS NOT NULL
        AND end_station_latitude IS NOT NULL
        AND end_station_longitude IS NOT NULL
        AND st_distance(start_station_geom, end_station_geom) > 1000
    ORDER BY
        velocidade_media DESC
    LIMIT 1
)

SELECT velocidade_media as max_velocity
FROM meta_data;",,snowflake
146,bq376,san_francisco_plus,"For each neighborhood in San Francisco where at least one bike share station and at least one crime incident are located, provide the neighborhood name along with the total count of bike share stations and the total number of crime incidents in that neighborhood.","WITH station_neighborhoods AS (
   SELECT
       bs.station_id,
       bs.name AS station_name,
       nb.neighborhood
   FROM `bigquery-public-data.san_francisco.bikeshare_stations` bs
   JOIN
       bigquery-public-data.san_francisco_neighborhoods.boundaries nb
   ON 
       ST_Intersects(ST_GeogPoint(bs.longitude, bs.latitude), nb.neighborhood_geom)
),

neighborhood_crime_counts AS (
   SELECT
       neighborhood,
       COUNT(*) AS crime_count
   FROM (
       SELECT
           n.neighborhood
       FROM
           bigquery-public-data.san_francisco.sfpd_incidents i
       JOIN
           bigquery-public-data.san_francisco_neighborhoods.boundaries n
       ON
           ST_Intersects(ST_GeogPoint(i.longitude, i.latitude), n.neighborhood_geom)
   ) AS incident_neighborhoods
   GROUP BY
       neighborhood
)

SELECT
  sn.neighborhood,
  COUNT(station_name) AS station_number,
  ANY_VALUE(ncc.crime_count) AS crime_number
FROM
  station_neighborhoods sn
JOIN
  neighborhood_crime_counts ncc
ON
  sn.neighborhood = ncc.neighborhood
GROUP BY sn.neighborhood
ORDER BY
  crime_number ASC

",,snowflake
147,sf_bq014,THELOOK_ECOMMERCE,Can you help me figure out the revenue for the product category that has the highest number of customers making a purchase in their first non-cancelled and non-returned order?,,,snowflake
148,sf_bq188,THELOOK_ECOMMERCE,"Among all product categories in the dataset, identify the category with the highest total purchase quantity (based on order_items table), and for that specific category, what is the average time in minutes that users spend on each product page visit? The average time should be calculated as the difference between the timestamp when a user views a product page and the timestamp of the next event within the same session",,,snowflake
149,sf_bq258,THELOOK_ECOMMERCE,"Generate a monthly report for each product category , where each row corresponds to orders that have a status of 'Complete' and were delivered before the year 2022, grouping by the month and year of delivery. For each category, calculate the total revenue (the sum of sale_price), the total number of completed orders, and compute the month-over-month percentage growth for both revenue and orders by comparing each month’s totals to the previous month’s. Then, for the same orders, aggregate and show the total cost (from product costs), total profit (revenue minus total cost), and finally the profit-to-cost ratio for each month.",,,snowflake
150,sf_bq259,THELOOK_ECOMMERCE,"Using data up to the end of 2022 and organized by the month of each user's first purchase, can you provide the percentage of users who made a purchase in each of the first, second, third, and fourth months since their initial purchase, where the ""first month"" refers to the month of their initial purchase?",,,snowflake
151,sf_bq189,THELOOK_ECOMMERCE,"Based solely on completed orders, calculate the average monthly percentage growth rate in the number of unique orders (counting distinct order IDs) for each product category by comparing each month's count to the previous month within the same category. Identify the product category with the highest average of these monthly order growth rates. Then, for that specific product category, compute the average monthly revenue growth rate by calculating the percentage change in total revenue (sum of sale prices) from month to month and averaging these values over the entire period.",,,snowflake
152,sf_bq260,THELOOK_ECOMMERCE,"From January 1, 2019, to April 30, 2022, how many users are at the youngest age and how many users are at the oldest age for each gender in the e-commerce platform, counting both youngest and oldest users separately for each gender?","WITH filtered_users AS (
    SELECT 
        ""first_name"", 
        ""last_name"", 
        ""gender"", 
        ""age"",
        CAST(TO_TIMESTAMP(""created_at"" / 1000000.0) AS DATE) AS ""created_at""
    FROM 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS""
    WHERE 
        CAST(TO_TIMESTAMP(""created_at"" / 1000000.0) AS DATE) BETWEEN '2019-01-01' AND '2022-04-30'
),
youngest_ages AS (
    SELECT 
        ""gender"", 
        MIN(""age"") AS ""age""
    FROM 
        filtered_users
    GROUP BY 
        ""gender""
),
oldest_ages AS (
    SELECT 
        ""gender"", 
        MAX(""age"") AS ""age""
    FROM 
        filtered_users
    GROUP BY 
        ""gender""
),
youngest_oldest AS (
    SELECT 
        u.""first_name"", 
        u.""last_name"", 
        u.""gender"", 
        u.""age"", 
        'youngest' AS ""tag""
    FROM 
        filtered_users u
    JOIN 
        youngest_ages y
    ON 
        u.""gender"" = y.""gender"" AND u.""age"" = y.""age""
    
    UNION ALL
    
    SELECT 
        u.""first_name"", 
        u.""last_name"", 
        u.""gender"", 
        u.""age"", 
        'oldest' AS ""tag""
    FROM 
        filtered_users u
    JOIN 
        oldest_ages o
    ON 
        u.""gender"" = o.""gender"" AND u.""age"" = o.""age""
)
SELECT 
    ""tag"", 
    ""gender"", 
    COUNT(*) AS ""num""
FROM 
    youngest_oldest
GROUP BY 
    ""tag"", ""gender""
ORDER BY 
    ""tag"", ""gender"";
",,snowflake
153,sf_bq261,THELOOK_ECOMMERCE,"For each month prior to January 2024, identify the product that achieved the highest total profit (calculated as the sum of sale_price minus the product’s cost) across all order items, then report the total cost and total profit for that top product per month, including all order items regardless of their status, and present the results chronologically by month.",,,snowflake
154,sf_bq262,THELOOK_ECOMMERCE,"Generate a monthly analysis report for e-commerce sales from June 2019 to December 2019 that includes, for each product category and each month, the total number of orders, total revenue, and total profit, along with their month-over-month growth rates using the data from June 2019 as the basis for calculating growth starting from July 2019. Ensure that all orders are included regardless of their status, and present the results sorted in ascending order by month (formatted as ""2019-07"") and then by product category. Omitting June 2019 from the final output but using it for the growth calculations.",,,snowflake
155,sf_bq190,THELOOK_ECOMMERCE,"Determine the number of users who are the youngest and oldest for each gender (male and female) separately, among those who signed up between January 1, 2019, and April 30, 2022. For each gender, identify the minimum and maximum ages within this date range, and count how many users fall into these respective age groups.",,,snowflake
156,sf_bq263,THELOOK_ECOMMERCE,"Please create a month-by-month report for the year 2023 that focuses on the 'Sleep & Lounge' category, showing for each month the total sales, total cost, number of complete orders, total profit, and the profit-to-cost ratio, ensuring that the order is marked as 'Complete,' the creation date is between January 1, 2023, and December 31, 2023, and the cost data is accurately associated with the corresponding product through the order items. ","WITH d AS (
    SELECT
        a.""order_id"", 
        TO_CHAR(TO_TIMESTAMP(a.""created_at"" / 1000000.0), 'YYYY-MM') AS ""month"",  -- 格式化为年月
        TO_CHAR(TO_TIMESTAMP(a.""created_at"" / 1000000.0), 'YYYY') AS ""year"",  -- 格式化为年份
        b.""product_id"", b.""sale_price"", c.""category"", c.""cost""
    FROM 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDERS"" AS a
    JOIN 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDER_ITEMS"" AS b
        ON a.""order_id"" = b.""order_id""
    JOIN 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""PRODUCTS"" AS c
        ON b.""product_id"" = c.""id""
    WHERE 
        a.""status"" = 'Complete'
        AND TO_TIMESTAMP(a.""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2023-01-01') AND TO_TIMESTAMP('2023-12-31')
        AND c.""category"" = 'Sleep & Lounge'
),

e AS (
    SELECT 
        ""month"", 
        ""year"", 
        ""sale_price"", 
        ""category"", 
        ""cost"",
        SUM(""sale_price"") OVER (PARTITION BY ""month"", ""category"") AS ""TPV"",
        SUM(""cost"") OVER (PARTITION BY ""month"", ""category"") AS ""total_cost"",
        COUNT(DISTINCT ""order_id"") OVER (PARTITION BY ""month"", ""category"") AS ""TPO"",
        SUM(""sale_price"" - ""cost"") OVER (PARTITION BY ""month"", ""category"") AS ""total_profit"",
        SUM((""sale_price"" - ""cost"") / ""cost"") OVER (PARTITION BY ""month"", ""category"") AS ""Profit_to_cost_ratio""
    FROM 
        d
)

SELECT DISTINCT 
    ""month"", 
    ""category"", 
    ""TPV"", 
    ""total_cost"", 
    ""TPO"", 
    ""total_profit"", 
    ""Profit_to_cost_ratio""
FROM 
    e
ORDER BY 
    ""month"";
",,snowflake
157,sf_bq264,THELOOK_ECOMMERCE,"Identify the difference in the number of the oldest and youngest users registered between January 1, 2019, and April 30, 2022, from our e-commerce platform data.","WITH youngest AS (
    SELECT
        ""gender"", 
        ""id"", 
        ""first_name"", 
        ""last_name"", 
        ""age"", 
        'youngest' AS ""tag""
    FROM 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS""
    WHERE 
        ""age"" = (SELECT MIN(""age"") FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS"")
        AND TO_TIMESTAMP(""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')
    GROUP BY 
        ""gender"", ""id"", ""first_name"", ""last_name"", ""age""
    ORDER BY 
        ""gender""
),

oldest AS (
    SELECT
        ""gender"", 
        ""id"", 
        ""first_name"", 
        ""last_name"", 
        ""age"", 
        'oldest' AS ""tag""
    FROM 
        ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS""
    WHERE 
        ""age"" = (SELECT MAX(""age"") FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS"")
        AND TO_TIMESTAMP(""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2022-04-30')
    GROUP BY 
        ""gender"", ""id"", ""first_name"", ""last_name"", ""age""
    ORDER BY 
        ""gender""
),

TEMP_record AS (
    SELECT * FROM youngest
    UNION ALL
    SELECT * FROM oldest
)

SELECT 
    SUM(CASE WHEN ""age"" = (SELECT MAX(""age"") FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS"") THEN 1 END) - 
    SUM(CASE WHEN ""age"" = (SELECT MIN(""age"") FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS"") THEN 1 END) AS ""diff""
FROM 
    TEMP_record;
",,snowflake
158,sf_bq197,THELOOK_ECOMMERCE,"For each month prior to July 2024, identify the single best-selling product (determined by highest sales volume, with total revenue as a tiebreaker) among all orders with a 'Complete' status and products with non-null brands. Return a report showing the month, product name, brand, category, total sales, rounded total revenue, and order status for these monthly top performers.",,,snowflake
159,sf_bq265,THELOOK_ECOMMERCE,"Can you list the email addresses of the top 10 users who registered in 2019 and made purchases in 2019, ranking them by their highest average order value, where average order value is calculated by multiplying the number of items in each order by the sale price, summing this total across all orders for each user, and then dividing by the total number of orders?","WITH
  main AS (
    SELECT
      ""id"" AS ""user_id"",
      ""email"",
      ""gender"",
      ""country"",
      ""traffic_source""
    FROM
      ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS""
    WHERE
      TO_TIMESTAMP(""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')
  ),

  daate AS (
    SELECT
      ""user_id"",
      ""order_id"",
      CAST(TO_TIMESTAMP(""created_at"" / 1000000.0) AS DATE) AS ""order_date"",
      ""num_of_item""
    FROM
      ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDERS""
    WHERE
      TO_TIMESTAMP(""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')
  ),

  orders AS (
    SELECT
      ""user_id"",
      ""order_id"",
      ""product_id"",
      ""sale_price"",
      ""status""
    FROM
      ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDER_ITEMS""
    WHERE
      TO_TIMESTAMP(""created_at"" / 1000000.0) BETWEEN TO_TIMESTAMP('2019-01-01') AND TO_TIMESTAMP('2019-12-31')
  ),

  nest AS (
    SELECT
      o.""user_id"",
      o.""order_id"",
      o.""product_id"",
      d.""order_date"",
      d.""num_of_item"",
      ROUND(o.""sale_price"", 2) AS ""sale_price"",
      ROUND(d.""num_of_item"" * o.""sale_price"", 2) AS ""total_sale""
    FROM
      orders o
    INNER JOIN
      daate d
    ON
      o.""order_id"" = d.""order_id""
    ORDER BY
      o.""user_id""
  ),

  type AS (
    SELECT
      ""user_id"",
      MIN(nest.""order_date"") AS ""cohort_date"",
      MAX(nest.""order_date"") AS ""latest_shopping_date"",
      DATEDIFF(MONTH, MIN(nest.""order_date""), MAX(nest.""order_date"")) AS ""lifespan_months"",
      ROUND(SUM(""total_sale""), 2) AS ""ltv"",
      COUNT(""order_id"") AS ""no_of_order""
    FROM
      nest
    GROUP BY
      ""user_id""
  ),

  kite AS (
    SELECT
      m.""user_id"",
      m.""email"",
      m.""gender"",
      m.""country"",
      m.""traffic_source"",
      EXTRACT(YEAR FROM n.""cohort_date"") AS ""cohort_year"",
      n.""latest_shopping_date"",
      n.""lifespan_months"",
      n.""ltv"",
      n.""no_of_order"",
      ROUND(n.""ltv"" / n.""no_of_order"", 2) AS ""avg_order_value""
    FROM
      main m
    INNER JOIN
      type n
    ON
      m.""user_id"" = n.""user_id""
  )

SELECT
  ""email""
FROM
  kite
ORDER BY
  ""avg_order_value"" DESC
LIMIT 10;
",,snowflake
160,sf_bq266,THELOOK_ECOMMERCE,"Please provide the names of the products that had sales in each month of 2020 and had the lowest profit, calculated as the difference between their retail price and cost from the products data. Exclude any months where this data isn't available. Please list the products in chronological order based on the month.",,,snowflake
161,sf_bq333,THELOOK_ECOMMERCE,"Which three browsers have the shortest average session duration—calculated by the difference in seconds between the earliest and latest timestamps for each user’s session—while only including browsers that have more than 10 total sessions, and what are their respective average session durations?",,,snowflake
162,sf_bq361,THELOOK_ECOMMERCE,"For the user cohort with a first purchase date in January 2020, what proportion of users returned in the subsequent months of 2020?",,,snowflake
163,sf_bq271,THELOOK_ECOMMERCE,"Please generate a report that, for each month in 2021, provides the number of orders, the number of unique purchasers, and the profit (calculated as the sum of product retail prices minus the sum of product costs), where the orders were placed during 2021 by users who registered in 2021 for inventory items created in 2021, and group the results by the users' country, product department, and product category.","WITH
orders_x_order_items AS (
  SELECT orders.*,
         order_items.""inventory_item_id"",
         order_items.""sale_price""
  FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDERS"" AS orders
  LEFT JOIN ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDER_ITEMS"" AS order_items
  ON orders.""order_id"" = order_items.""order_id""
  WHERE TO_TIMESTAMP_NTZ(orders.""created_at"" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')
),

orders_x_inventory AS (
  SELECT orders_x_order_items.*,
         inventory_items.""product_category"",
         inventory_items.""product_department"",
         inventory_items.""product_retail_price"",
         inventory_items.""product_distribution_center_id"",
         inventory_items.""cost"",
         distribution_centers.""name""
  FROM orders_x_order_items
  LEFT JOIN ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""INVENTORY_ITEMS"" AS inventory_items
  ON orders_x_order_items.""inventory_item_id"" = inventory_items.""id""
  LEFT JOIN ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""DISTRIBUTION_CENTERS"" AS distribution_centers
  ON inventory_items.""product_distribution_center_id"" = distribution_centers.""id""
  WHERE TO_TIMESTAMP_NTZ(inventory_items.""created_at"" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')
),

orders_x_users AS (
  SELECT orders_x_inventory.*,
         users.""country"" AS ""users_country""
  FROM orders_x_inventory
  LEFT JOIN ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS"" AS users
  ON orders_x_inventory.""user_id"" = users.""id""
  WHERE TO_TIMESTAMP_NTZ(users.""created_at"" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2021-01-01') AND TO_TIMESTAMP_NTZ('2021-12-31')
)

SELECT 
  DATE_TRUNC('MONTH', TO_DATE(TO_TIMESTAMP_NTZ(orders_x_users.""created_at"" / 1000000))) AS ""reporting_month"",
  orders_x_users.""users_country"",
  orders_x_users.""product_department"",
  orders_x_users.""product_category"",
  COUNT(DISTINCT orders_x_users.""order_id"") AS ""n_order"",
  COUNT(DISTINCT orders_x_users.""user_id"") AS ""n_purchasers"",
  SUM(orders_x_users.""product_retail_price"") - SUM(orders_x_users.""cost"") AS ""profit""
FROM orders_x_users
GROUP BY 1, 2, 3, 4
ORDER BY ""reporting_month"";
",,snowflake
164,sf_bq272,THELOOK_ECOMMERCE,"Please provide the names of the top three most profitable products for each month from January 2019 through August 2022, excluding any products associated with orders that were canceled or returned. For each product in each month, the profit should be calculated as the sum of the sale prices of all order items minus the sum of the costs of those sold items in that month.",,,snowflake
165,sf_bq273,THELOOK_ECOMMERCE,"Can you list the top 5 months from August 2022 to November 2023 where the profit from Facebook-sourced completed orders showed the largest month-over-month increase? Calculate profit as sales minus costs, group by delivery month, and include only orders created between August 2022 and November 2023. Compare each month's profit to its previous month to find the largest increases.","WITH 
orders AS (
  SELECT
    ""order_id"", 
    ""user_id"", 
    ""created_at"",
    DATE_TRUNC('MONTH', TO_TIMESTAMP_NTZ(""delivered_at"" / 1000000)) AS ""delivery_month"",  -- Converting to timestamp
    ""status"" 
  FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDERS""
),

order_items AS (
  SELECT 
    ""order_id"", 
    ""product_id"", 
    ""sale_price"" 
  FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""ORDER_ITEMS""
),

products AS (
  SELECT 
    ""id"", 
    ""cost""
  FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""PRODUCTS""
),

users AS (
  SELECT
    ""id"", 
    ""traffic_source"" 
  FROM ""THELOOK_ECOMMERCE"".""THELOOK_ECOMMERCE"".""USERS""
),

filter_join AS (
  SELECT 
    orders.""order_id"",
    orders.""user_id"",
    order_items.""product_id"",
    orders.""delivery_month"",
    orders.""status"",
    order_items.""sale_price"",
    products.""cost"",
    users.""traffic_source""
  FROM orders
  JOIN order_items ON orders.""order_id"" = order_items.""order_id""
  JOIN products ON order_items.""product_id"" = products.""id""
  JOIN users ON orders.""user_id"" = users.""id""
  WHERE orders.""status"" = 'Complete' 
    AND users.""traffic_source"" = 'Facebook'
    AND TO_TIMESTAMP_NTZ(orders.""created_at"" / 1000000) BETWEEN TO_TIMESTAMP_NTZ('2022-07-01') AND TO_TIMESTAMP_NTZ('2023-11-30')  -- Include July for calculation
),

monthly_sales AS (
  SELECT 
    ""delivery_month"",
    ""traffic_source"",
    SUM(""sale_price"") AS ""total_revenue"",
    SUM(""sale_price"") - SUM(""cost"") AS ""total_profit"",
    COUNT(DISTINCT ""product_id"") AS ""product_quantity"",
    COUNT(DISTINCT ""order_id"") AS ""orders_quantity"",
    COUNT(DISTINCT ""user_id"") AS ""users_quantity""
  FROM filter_join
  GROUP BY ""delivery_month"", ""traffic_source""
)

-- Filter to show only 8th month and onwards, but calculate using July
SELECT 
  current_month.""delivery_month"",
  COALESCE(
    current_month.""total_profit"" - previous_month.""total_profit"", 
    0  -- If there is no previous month (i.e. for 8月), return 0
  ) AS ""profit_vs_prior_month""
FROM monthly_sales AS current_month
LEFT JOIN monthly_sales AS previous_month
  ON current_month.""traffic_source"" = previous_month.""traffic_source""
  AND current_month.""delivery_month"" = DATEADD(MONTH, -1, previous_month.""delivery_month"")  -- Correctly join to previous month
WHERE current_month.""delivery_month"" >= '2022-08-01'  -- Only show August and later data, but use July for calculation
ORDER BY ""profit_vs_prior_month"" DESC
LIMIT 5;
",,snowflake
166,sf_bq020,GENOMICS_CANNABIS,What is the name of the reference sequence with the highest variant density in the given cannabis genome dataset?,,,snowflake
167,sf_bq107,GENOMICS_CANNABIS,What is the variant density of the cannabis reference with the longest reference length? Pay attention that a variant is present if there is at least one variant call with a genotype greater than 0.,,,snowflake
168,bq025,census_bureau_international,"Provide a list of the top 10 countries for the year 2020, ordered by the highest percentage of their population under 20 years old. For each country, include the total population under 20 years old, the total midyear population, and the percentage of the population that is under 20 years old.","SELECT
  age.country_name,
  SUM(age.population) AS under_25,
  pop.midyear_population AS total,
  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25
FROM (
  SELECT
    country_name,
    population,
    country_code
  FROM
    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`
  WHERE
    year =2020
    AND age < 20) age
INNER JOIN (
  SELECT
    midyear_population,
    country_code
  FROM
    `bigquery-public-data.census_bureau_international.midyear_population`
  WHERE
    year = 2020) pop
ON
  age.country_code = pop.country_code
GROUP BY
  1,
  3
ORDER BY
  4 DESC
/* Remove limit for visualization */
LIMIT
  10
",,snowflake
169,bq115,census_bureau_international,Which country has the highest percentage of population under the age of 25 in 2017?,"SELECT
country_name
FROM
(SELECT
  age.country_name,
  SUM(age.population) AS under_25,
  pop.midyear_population AS total,
  ROUND((SUM(age.population) / pop.midyear_population) * 100,2) AS pct_under_25
FROM (
  SELECT
    country_name,
    population,
    country_code
  FROM
    `bigquery-public-data.census_bureau_international.midyear_population_agespecific`
  WHERE
    year =2017
    AND age < 25) age
INNER JOIN (
  SELECT
    midyear_population,
    country_code
  FROM
    `bigquery-public-data.census_bureau_international.midyear_population`
  WHERE
    year = 2017) pop
ON
  age.country_code = pop.country_code
GROUP BY
  1,
  3
ORDER BY
  4 DESC
)
LIMIT
1",,snowflake
170,bq030,covid19_open_data,"As of May 10, 2020, among all countries that had more than 50,000 confirmed COVID-19 cases, which three countries had the highest recovery rates based on the total number of recovered cases relative to their total confirmed cases, and what were their respective recovery rates expressed as percentages?",,,snowflake
171,bq018,covid19_open_data,Which day in March and April had the highest COVID-19 confirmed case growth rate in the United States? The format is MM-DD.,"WITH us_cases_by_date AS (
  SELECT
    date,
    SUM( cumulative_confirmed ) AS cases
  FROM
    `bigquery-public-data.covid19_open_data.covid19_open_data`
  WHERE
    country_name=""United States of America""
    AND date between '2020-03-01' and '2020-04-30'
  GROUP BY
    date
  ORDER BY
    date ASC
 )

, us_previous_day_comparison AS
(SELECT
  date,
  cases,
  LAG(cases) OVER(ORDER BY date) AS previous_day,
  cases - LAG(cases) OVER(ORDER BY date) AS net_new_cases,
  (cases - LAG(cases) OVER(ORDER BY date))*100/LAG(cases) OVER(ORDER BY date) AS percentage_increase
FROM us_cases_by_date
)
SELECT
  FORMAT_DATE('%m-%d', Date) 
FROM
  us_previous_day_comparison
ORDER BY  
  percentage_increase
DESC
LIMIT 1",,snowflake
172,bq086,covid19_open_world_bank,"You need to calculate the percentage of each country's population that had been confirmed with COVID-19 by June 30, 2020. The population data for 2018 can be found in the World Bank dataset, and the cumulative COVID-19 confirmed cases data is available in the COVID-19 Open Data dataset. Calculate the percentage of each country's population, that was cumulatively confirmed to have COVID-19","WITH
  country_pop AS (
  SELECT
    country_code AS iso_3166_1_alpha_3,
    year_2018 AS population_2018
  FROM
    `bigquery-public-data.world_bank_global_population.population_by_country`)
SELECT
  country_code,
  country_name,
  cumulative_confirmed AS june_confirmed_cases,
  population_2018,
  ROUND(cumulative_confirmed/population_2018 * 100,2) AS case_percent
FROM
  `bigquery-public-data.covid19_open_data.covid19_open_data`
JOIN
  country_pop
USING
  (iso_3166_1_alpha_3)
WHERE
  date = '2020-06-30'
  AND aggregation_level = 0
ORDER BY
  case_percent DESC",,snowflake
173,bq085,covid19_jhu_world_bank,"Could you provide, for the United States, France, China, Italy, Spain, Germany, and Iran, the total number of confirmed COVID-19 cases as of April 20, 2020, along with the number of cases per 100,000 people based on their total 2020 populations calculated by summing all relevant population entries from the World Bank data","SELECT
  c.country,
  c.total_confirmed_cases,
  (c.total_confirmed_cases / p.population) * 100000 AS cases_per_100k
FROM
  (
    SELECT
      CASE
        WHEN country_region = 'US' THEN 'United States'
        WHEN country_region = 'Iran' THEN 'Iran, Islamic Rep.'
        ELSE country_region
      END AS country,
      SUM(confirmed) AS total_confirmed_cases
    FROM
      `bigquery-public-data.covid19_jhu_csse.summary`
    WHERE
      date = '2020-04-20'
      AND country_region IN ('US', 'France', 'China', 'Italy', 'Spain', 'Germany', 'Iran')
    GROUP BY
      country
  ) AS c
JOIN
  (
    SELECT
      country_name AS country,
      SUM(value) AS population
    FROM
      `bigquery-public-data.world_bank_wdi.indicators_data`
    WHERE
      indicator_code = 'SP.POP.TOTL'
      AND year = 2020
    GROUP BY
      country_name
  ) AS p
ON
  c.country = p.country
ORDER BY
  cases_per_100k DESC",,snowflake
174,bq130,covid19_nyt,"Analyze daily new COVID-19 case counts from March to May 2020, identifying the top five states by daily increases. Please compile a ranking based on how often each state appears in these daily top fives. Then, examine the state that ranks fourth overall and identify its top five counties based on their frequency of appearing in the daily top five new case counts.","WITH StateCases AS (
    SELECT
        b.state_name,
        b.date,
        b.confirmed_cases - a.confirmed_cases AS daily_new_cases
    FROM 
        (SELECT
            state_name,
            state_fips_code,
            confirmed_cases,
            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift
        FROM
            `bigquery-public-data.covid19_nyt.us_states`
        WHERE
            date >= '2020-02-29' AND date <= '2020-05-30'
        ) a
    JOIN
        `bigquery-public-data.covid19_nyt.us_states` b 
        ON a.state_fips_code = b.state_fips_code AND a.date_shift = b.date
    WHERE
        b.date >= '2020-03-01' AND b.date <= '2020-05-31'
),
RankedStatesPerDay AS (
    SELECT
        state_name,
        date,
        daily_new_cases,
        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank
    FROM
        StateCases
),
TopStates AS (
    SELECT
        state_name,
        COUNT(*) AS appearance_count
    FROM
        RankedStatesPerDay
    WHERE
        rank <= 5
    GROUP BY
        state_name
    ORDER BY
        appearance_count DESC
),
FourthState AS (
    SELECT
        state_name
    FROM
        TopStates
    LIMIT 1
    OFFSET 3
),
CountyCases AS (
    SELECT
        b.county,
        b.date,
        b.confirmed_cases - a.confirmed_cases AS daily_new_cases
    FROM 
        (SELECT
            county,
            county_fips_code,
            confirmed_cases,
            DATE_ADD(date, INTERVAL 1 DAY) AS date_shift
        FROM
            `bigquery-public-data.covid19_nyt.us_counties`
        WHERE
            date >= '2020-02-29' AND date <= '2020-05-30'
        ) a
    JOIN
        `bigquery-public-data.covid19_nyt.us_counties` b 
        ON a.county_fips_code = b.county_fips_code AND a.date_shift = b.date
    WHERE
        b.date >= '2020-03-01' AND b.date <= '2020-05-31'
        AND b.state_name = (SELECT state_name FROM FourthState)
),
RankedCountiesPerDay AS (
    SELECT
        county,
        date,
        daily_new_cases,
        RANK() OVER (PARTITION BY date ORDER BY daily_new_cases DESC) as rank
    FROM
        CountyCases
),
TopCounties AS (
    SELECT
        county,
        COUNT(*) AS appearance_count
    FROM
        RankedCountiesPerDay
    WHERE
        rank <= 5
    GROUP BY
        county
    ORDER BY
        appearance_count DESC
    LIMIT 5
)
SELECT
    county
FROM
    TopCounties;
",,snowflake
175,bq087,covid19_symptom_search,"Please calculate the overall percentage change in the average weekly search frequency for the symptom 'Anosmia' across the five New York City counties—Bronx County, Queens County, Kings County, New York County, and Richmond County—by comparing the combined data from January 1, 2019, through December 31, 2019, with the combined data from January 1, 2020, through December 31, 2020.","SELECT
  table_2019.avg_symptom_Anosmia_2019,
  table_2020.avg_symptom_Anosmia_2020,
  ((table_2020.avg_symptom_Anosmia_2020 - table_2019.avg_symptom_Anosmia_2019) / table_2019.avg_symptom_Anosmia_2019) * 100 AS avg_increase
FROM (
  SELECT
    AVG(SAFE_CAST(symptom_Anosmia AS FLOAT64)) AS avg_symptom_Anosmia_2020
  FROM
    `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_weekly`
  WHERE
    sub_region_1 = ""New York""
    AND sub_region_2 IN (""Bronx County"", ""Queens County"", ""Kings County"", ""New York County"", ""Richmond County"")
    AND date >= '2020-01-01'
    AND date < '2021-01-01'
) AS table_2020,
(
  SELECT
    AVG(SAFE_CAST(symptom_Anosmia AS FLOAT64)) AS avg_symptom_Anosmia_2019
  FROM
    `bigquery-public-data.covid19_symptom_search.symptom_search_sub_region_2_weekly`
  WHERE
    sub_region_1 = ""New York""
    AND sub_region_2 IN (""Bronx County"", ""Queens County"", ""Kings County"", ""New York County"", ""Richmond County"")
    AND date >= '2019-01-01'
    AND date < '2020-01-01'
) AS table_2019
",,snowflake
176,bq088,covid19_symptom_search,"Please calculate the average levels of anxiety and depression symptoms from the weekly country data for the United States during the periods from January 1, 2019, to January 1, 2020, and from January 1, 2020, to January 1, 2021. Then, compute the percentage increase in these average symptom levels from the 2019 period to the 2020 period.","SELECT
  table_2019.avg_symptom_Anxiety_2019,
  table_2020.avg_symptom_Anxiety_2020,
  ((table_2020.avg_symptom_Anxiety_2020 - table_2019.avg_symptom_Anxiety_2019)/table_2019.avg_symptom_Anxiety_2019) * 100 AS percent_increase_anxiety,
  table_2019.avg_symptom_Depression_2019,
  table_2020.avg_symptom_Depression_2020,
  ((table_2020.avg_symptom_Depression_2020 - table_2019.avg_symptom_Depression_2019)/table_2019.avg_symptom_Depression_2019) * 100 AS percent_increase_depression
FROM (
  SELECT
    AVG(CAST(symptom_Anxiety AS FLOAT64)) AS avg_symptom_Anxiety_2020,
    AVG(CAST(symptom_Depression AS FLOAT64)) AS avg_symptom_Depression_2020,
  FROM
    `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly`
  WHERE
    country_region_code = ""US""
    AND date >= '2020-01-01'
    AND date <'2021-01-01') AS table_2020,
  (
  SELECT
    AVG(CAST(symptom_Anxiety AS FLOAT64)) AS avg_symptom_Anxiety_2019,
    AVG(CAST(symptom_Depression AS FLOAT64)) AS avg_symptom_Depression_2019,
  FROM
    `bigquery-public-data.covid19_symptom_search.symptom_search_country_weekly`
  WHERE
    country_region_code = ""US""
    AND date >= '2019-01-01'
    AND date <'2020-01-01') AS table_2019",,snowflake
177,bq089,covid19_usa,"Given the latest population estimates from the 2018 five-year American Community Survey, what is the number of vaccine sites per 1000 people for counties in California?","WITH
  num_vaccine_sites_per_county AS (
  SELECT
    facility_sub_region_1 AS us_state,
    facility_sub_region_2 AS us_county,
    facility_sub_region_2_code AS us_county_fips,
    COUNT(DISTINCT facility_place_id) AS num_vaccine_sites
  FROM
    bigquery-public-data.covid19_vaccination_access.facility_boundary_us_all
  WHERE
    STARTS_WITH(facility_sub_region_2_code, ""06"")
  GROUP BY
    facility_sub_region_1,
    facility_sub_region_2,
    facility_sub_region_2_code ),
  total_population_per_county AS (
  SELECT
    LEFT(geo_id, 5) AS us_county_fips,
    ROUND(SUM(total_pop)) AS total_population
  FROM
    bigquery-public-data.census_bureau_acs.censustract_2018_5yr
  WHERE
    STARTS_WITH(LEFT(geo_id, 5), ""06"")
  GROUP BY
    LEFT(geo_id, 5) )
SELECT
  * EXCEPT(us_county_fips),
  ROUND((num_vaccine_sites * 1000) / total_population, 2) AS sites_per_1k_ppl
FROM
  num_vaccine_sites_per_county
INNER JOIN
  total_population_per_county
USING
  (us_county_fips)
ORDER BY
  sites_per_1k_ppl ASC
LIMIT
  100;",,snowflake
178,bq407,covid19_usa,"Find the top three counties with populations over 50,000, using the 2020 5-year census data, that had the highest COVID-19 case fatality rates on August 27, 2020. For these counties, provide the name, state, median age, total population, number of confirmed COVID-19 cases per 100,000 people, number of deaths per 100,000 people, and the case fatality rate as a percentage","WITH population_data AS (
  SELECT
    geo_id,
    median_age,
    total_pop
  FROM
    `bigquery-public-data.census_bureau_acs.county_2020_5yr`
  WHERE
    total_pop > 50000
),
covid_data AS (
  SELECT
    county_fips_code,
    county_name,
    state,
    SUM(confirmed_cases) AS total_cases,
    SUM(deaths) AS total_deaths
  FROM
    `bigquery-public-data.covid19_usafacts.summary`
  WHERE
    date = '2020-08-27'
  GROUP BY
    county_fips_code, county_name, state
)
SELECT
  covid.county_name,
  covid.state,
  pop.median_age,
  pop.total_pop,
  (covid.total_cases / pop.total_pop * 100000) AS confirmed_cases_per_100000,
  (covid.total_deaths / pop.total_pop * 100000) AS deaths_per_100000,
  (covid.total_deaths / covid.total_cases * 100) AS case_fatality_rate
FROM
  covid_data covid
JOIN
  population_data pop ON covid.county_fips_code = pop.geo_id
ORDER BY
  case_fatality_rate DESC
LIMIT 3;",,snowflake
179,bq137,census_bureau_usa,"Please find all zip code areas located within 10 kilometers of the coordinates (-122.3321, 47.6062) by joining the 2010 census population data (summing only male and female populations with no age constraints) and the zip code area information, and return each area’s polygon, land and water area in meters, latitude and longitude, state code, state name, city, county, and total population.",,"Categories: Geospatial functions


## ST_DWITHIN

Returns TRUE if the minimum geodesic distance between two points (two GEOGRAPHY objects) is within the specified distance. Otherwise, returns FALSE.
If the parameters are GEOGRAPHY values that are not points (e.g. lines or polygons), this returns TRUE or FALSE based on the minimum geodesic distance between the two closest points of the two values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

## Syntax

ST_DWITHIN( <geography_expression_1> , <geography_expression_2> , <distance_in_meters> )


## Arguments


geography_expression_1The argument must be an expression of type GEOGRAPHY.

geography_expression_2The argument must be an expression of type GEOGRAPHY.

distance_in_metersThe argument must be an expression of type REAL. The distance is in meters.


## Returns

Returns a BOOLEAN.

## Usage notes


Returns NULL if any input is NULL.


## Examples

This returns TRUE because the distance in meters between two points 1 degree apart along the equator is less than 150,000 meters:

SELECT ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000);
+-------------------------------------------------------------+
| ST_DWITHIN (ST_MAKEPOINT(0, 0), ST_MAKEPOINT(1, 0), 150000) |
|-------------------------------------------------------------|
| True                                                        |
+-------------------------------------------------------------+",snowflake
180,bq060,census_bureau_international,Which top 3 countries had the highest net migration in 2017 among those with an area greater than 500 square kilometers? And what are their migration rates?,"WITH results AS (
    SELECT
        growth.country_name,
        growth.net_migration,
        CAST(area.country_area as INT64) as country_area
    FROM (
        SELECT
            country_name,
            net_migration,
            country_code
        FROM
            `bigquery-public-data.census_bureau_international.birth_death_growth_rates`
        WHERE
            year = 2017
    ) growth
    INNER JOIN (
        SELECT
            country_area,
            country_code
        FROM
            `bigquery-public-data.census_bureau_international.country_names_area`
        WHERE
            country_area > 500
    ) area
    ON
        growth.country_code = area.country_code
    ORDER BY
        net_migration DESC
    LIMIT 3
)
SELECT country_name, net_migration
FROM results;",,snowflake
181,bq338,census_bureau_acs_1,"Can you find the census tracts in the 36047 area that are among the top 20 for the largest percentage increases in population from 2011 to 2018, are also among the top 20 for the largest absolute increases in median income during the same period, and had over 1,000 residents in each of those years?","WITH population_change AS (
    SELECT
        a.geo_id,
        a.total_pop AS pop_2011,
        b.total_pop AS pop_2018,
        ((b.total_pop - a.total_pop) / a.total_pop) * 100 AS population_change_percentage
    FROM
        bigquery-public-data.census_bureau_acs.censustract_2011_5yr a
    JOIN
        bigquery-public-data.census_bureau_acs.censustract_2018_5yr b
    ON
        a.geo_id = b.geo_id
    WHERE 
        a.total_pop > 1000
        AND b.total_pop > 1000
        AND a.geo_id LIKE '36047%'
        AND b.geo_id LIKE '36047%'
    ORDER BY 
        population_change_percentage DESC
    LIMIT 20
),

acs_2018 AS (
    SELECT 
        geo_id, 
        median_income AS median_income_2018
    FROM 
        bigquery-public-data.census_bureau_acs.censustract_2018_5yr  
    WHERE 
        geo_id LIKE '36047%'
        AND total_pop > 1000
),

acs_2011 AS (
    SELECT 
        geo_id, 
        median_income AS median_income_2011
    FROM 
        bigquery-public-data.census_bureau_acs.censustract_2011_5yr 
    WHERE 
        geo_id LIKE '36047%'
    AND total_pop > 1000
),

acs_diff AS (
    SELECT
        a18.geo_id, 
        a18.median_income_2018, 
        a11.median_income_2011,
        (a18.median_income_2018 - a11.median_income_2011) AS median_income_diff
    FROM 
        acs_2018 a18
    JOIN 
        acs_2011 a11
        ON a18.geo_id = a11.geo_id
    WHERE 
        (a18.median_income_2018 - a11.median_income_2011) IS NOT NULL
    ORDER BY 
        (a18.median_income_2018 - a11.median_income_2011) DESC
    LIMIT 20
),

common_geoids AS (
    SELECT population_change.geo_id
    FROM population_change
    JOIN acs_diff ON population_change.geo_id = acs_diff.geo_id
)

SELECT geo_id FROM common_geoids;
",,snowflake
182,bq061,census_bureau_acs_1,Which census tract has witnessed the largest increase in median income between 2015 and 2018 in California? Tell me the tract code.,"WITH acs_2018 AS (
    SELECT
      geo_id,
      median_income AS median_income_2018
    FROM
      `bigquery-public-data.census_bureau_acs.censustract_2018_5yr` 
),
acs_2015 AS (
    SELECT
      geo_id,
      median_income AS median_income_2015
    FROM
      `bigquery-public-data.census_bureau_acs.censustract_2015_5yr` ),
acs_diff AS (
    SELECT
      a18.geo_id,
      a18.median_income_2018,
      a15.median_income_2015,
      (a18.median_income_2018 - a15.median_income_2015) AS median_income_diff,
    FROM
      acs_2018 a18
    JOIN
      acs_2015 a15
    ON
      a18.geo_id = a15.geo_id
),
max_geo_id AS (
    SELECT
      geo_id
    FROM
      acs_diff
    WHERE
      median_income_diff IS NOT NULL
      AND acs_diff.geo_id in (
        SELECT
          geo_id
        FROM
          `bigquery-public-data.geo_census_tracts.census_tracts_california`
      )
    ORDER BY
      median_income_diff DESC
    LIMIT 1
)
SELECT
    tracts.tract_ce as tract_code
FROM
    max_geo_id
JOIN
    `bigquery-public-data.geo_census_tracts.census_tracts_california` AS tracts
ON
    max_geo_id.geo_id = tracts.geo_id;",,snowflake
183,bq064,census_bureau_acs_1,"Using the 2017 U.S. Census Tract data from the BigQuery public datasets, you need to proportionally allocate each tract's population and income to the zip codes based on the overlapping area between their geographic boundaries. Then, filter the results to include only those zip codes located within a 5-mile radius of a specific point in Washington State, with coordinates at latitude 47.685833°N and longitude -122.191667°W. Finally, calculate the total population and the average individual income for each zip code (rounded to one decimal place) and sort the results by the average individual income in descending order.","WITH all_zip_tract_join AS (
  SELECT 
    zips.zip_code, 
    zips.functional_status as zip_functional_status,
    tracts.tract_ce, 
    tracts.geo_id as tract_geo_id, 
    tracts.functional_status as tract_functional_status,
    ST_Area(ST_Intersection(tracts.tract_geom, zips.zip_code_geom))
        / ST_Area(tracts.tract_geom) as tract_pct_in_zip_code
  FROM  
    `bigquery-public-data.geo_census_tracts.us_census_tracts_national` tracts,
    `bigquery-public-data.geo_us_boundaries.zip_codes` zips
  WHERE 
    ST_Intersects(tracts.tract_geom, zips.zip_code_geom)
),
zip_tract_join AS (
  SELECT * FROM all_zip_tract_join WHERE tract_pct_in_zip_code > 0
),
census_totals AS (
  -- convert averages to additive totals
  SELECT 
    geo_id,
    total_pop,
    total_pop * income_per_capita AS total_income 
  FROM 
    `bigquery-public-data.census_bureau_acs.censustract_2017_5yr` 
),
joined AS ( 
  -- join with precomputed census/zip pairs,
  -- compute zip's share of tract
  SELECT 
    zip_code, 
    total_pop * tract_pct_in_zip_code    AS zip_pop,
    total_income * tract_pct_in_zip_code AS zip_income
  FROM census_totals c
  JOIN zip_tract_join ztj
  ON c.geo_id = ztj.tract_geo_id
),
sums AS ( 
  -- aggregate all ""pieces"" of zip code
  SELECT
    zip_code, 
    SUM(zip_pop) AS zip_pop,
    SUM(zip_income) AS zip_total_inc
  FROM joined 
  GROUP BY zip_code
),
zip_pop_income AS (
    SELECT 
        zip_code, zip_pop, 
        -- convert to averages
        zip_total_inc / zip_pop AS income_per_capita
    FROM sums
),
zipcodes_within_distance as (
    SELECT 
        zip_code, zip_code_geom
    FROM 
        `bigquery-public-data.geo_us_boundaries.zip_codes`
    WHERE
        state_code = 'WA'  -- Washington state code
        AND
        ST_DWithin(
            ST_GeogPoint(-122.191667, 47.685833),
            zip_code_geom,
            8046.72
        )
)
select 
  stats.zip_code,
  ROUND(stats.zip_pop, 1) as zip_population,
  ROUND(stats.income_per_capita, 1) as average_income
from 
  zipcodes_within_distance area
join 
  zip_pop_income stats
on area.zip_code = stats.zip_code
ORDER BY
    average_income DESC;","Categories: Geospatial functions


## ST_INTERSECTS

Returns TRUE if the two GEOGRAPHY objects or the two GEOMETRY objects intersect (i.e. share any portion of space).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_DISJOINT


## Syntax

ST_INTERSECTS( <geography_expression_1> , <geography_expression_2> )

ST_INTERSECTS( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object.

geography_expression_2A GEOGRAPHY object.

geometry_expression_1A GEOMETRY object.

geometry_expression_2A GEOMETRY object.


## Returns

BOOLEAN.

## Usage notes


For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
    TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'),
    TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')
    );
+---------------------------------------------------------+
| ST_INTERSECTS(                                          |
|     TO_GEOGRAPHY('POLYGON((0 0, 2 0, 2 2, 0 2, 0 0))'), |
|     TO_GEOGRAPHY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
|     )                                                   |
|---------------------------------------------------------|
| True                                                    |
+---------------------------------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_INTERSECTS function:

SELECT ST_INTERSECTS(
  TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'),
  TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))') );

+------------------------------------------------------+
| ST_INTERSECTS(                                       |
|   TO_GEOMETRY('POLYGON((0 0, 0 2, 2 2, 2 0, 0 0))'), |
|   TO_GEOMETRY('POLYGON((1 1, 3 1, 3 3, 1 3, 1 1))')  |
| )                                                    |
|------------------------------------------------------|
| True                                                 |
+------------------------------------------------------+




## ST_AREA

Returns the area of the Polygon(s) in a GEOGRAPHY or GEOMETRY object.

## Syntax

ST_AREA( <geography_or_geometry_expression> )


## Arguments


geography_or_geometry_expressionThe argument must be of type GEOGRAPHY or GEOMETRY.


## Returns

Returns a REAL value, which represents the area:

For GEOGRAPHY input values, the area is in square meters.
For GEOMETRY input values, the area is computed with the same units used to define the input coordinates.


## Usage notes


If geography_expression is not a Polygon, MultiPolygon, or GeometryCollection containing polygons, ST_AREA returns 0.
If geography_expression is a GeometryCollection, ST_AREA returns the sum of the areas of the polygons in the collection.


## Examples


## GEOGRAPHY examples

This uses the ST_AREA function with GEOGRAPHY objects to calculate the area of Earth’s surface 1 degree on each side with the bottom of the area on the equator:

SELECT ST_AREA(TO_GEOGRAPHY('POLYGON((0 0, 1 0, 1 1, 0 1, 0 0))')) AS area;
+------------------+
|             AREA |
|------------------|
| 12364036567.0764 |
+------------------+



## GEOMETRY examples

The following example calls the ST_AREA function with GEOMETRY objects that represent a Point, LineString, and Polygon.

SELECT ST_AREA(g), ST_ASWKT(g) FROM (SELECT TO_GEOMETRY(column1) as g
  from values ('POINT(1 1)'),
              ('LINESTRING(0 0, 1 1)'),
              ('POLYGON((0 0, 0 1, 1 1, 1 0, 0 0))'));

+------------+--------------------------------+
| ST_AREA(G) | ST_ASWKT(G)                    |
|------------+--------------------------------|
|          0 | POINT(1 1)                     |
|          0 | LINESTRING(0 0,1 1)            |
|          1 | POLYGON((0 0,0 1,1 1,1 0,0 0)) |
+------------+--------------------------------+",snowflake
184,bq461,ncaa_basketball,"Please provide a chronological summary of all scoring plays from the 2014 season game where the Wildcats were the home team and the Fighting Irish were the away team. Include for each scoring event the game clock, cumulative scores for both teams (Wildcats and Fighting Irish), the team that scored, and a description of the event.",,,snowflake
185,bq198,ncaa_basketball,"List the top 5 universities with the most seasons where they achieved the maximum wins in their respective NCAA basketball seasons between 1900-2000, showing each team's total number of such peak-performance seasons, while excluding entries with missing team names.","SELECT
  team_name,
  COUNT(*) AS top_performer_count
FROM (
  SELECT
    DISTINCT c2.season,
    c2.market AS team_name
  FROM (
    SELECT
      season AS a,
      MAX(wins) AS win_max
    FROM
      `bigquery-public-data.ncaa_basketball.mbb_historical_teams_seasons`
    WHERE
      season<=2000
      AND season >=1900
    GROUP BY
      season ),
    `bigquery-public-data.ncaa_basketball.mbb_historical_teams_seasons` c2
  WHERE
    win_max = c2.wins
    AND a = c2.season
    AND c2.market IS NOT NULL
  ORDER BY
    c2.season)
GROUP BY
  team_name
ORDER BY
  top_performer_count DESC,
  team_name
LIMIT
  5",,snowflake
186,bq462,ncaa_basketball,"Please generate a table from the NCAA basketball dataset that lists the top five records in each of these four categories: (1) Top Venues - the largest venues by seating capacity with Date shown as 'N/A'; (2) Biggest Championship Margins - National Championship games since the 2016 season (season > 2015) with the biggest point margin victories; (3) Highest Scoring Games - games since the 2011 season (season > 2010) with the highest total points scored by both teams combined; and (4) Total Threes - games since the 2011 season (season > 2010) with the highest total three-pointers made by both teams combined. The final table should be organized with columns for Category, Date, Matchup or Venue, and Key Metric, with each category's 5 records presented in descending order of their key metric.",,,snowflake
187,bq427,ncaa_basketball,"Could you determine, for each shot type, the average x and y coordinates (adjusted to ensure consistency regarding the left or right basket), the average number of shot attempts, and the average number of successful shots, considering only shots taken before March 15, 2018, excluding those with null shot types or coordinates, ensuring the shots are on the correct side of the court based on the team's basket.",,"# Score Intervals and Coordinates Logic

## Score Delta Intervals

To categorize the score deltas into intervals, the following conditions are used:

- **<-20**: When `score_delta < -20`
- **-20 — -11**: When `score_delta` is between -20 (inclusive) and -10 (exclusive)
- **-10 — -1**: When `score_delta` is between -10 (inclusive) and 0 (exclusive)
- **0**: When `score_delta` equals 0
- **1 — 10**: When `score_delta` is between 1 (inclusive) and 10 (inclusive)
- **11 — 20**: When `score_delta` is between 11 (exclusive) and 20 (inclusive)
- **>20**: When `score_delta > 20`

These intervals help in analyzing the performance based on the difference in team scores.

## X and Y Coordinates Calculation

Coordinates are adjusted based on the `event_coord_x` and `event_coord_y` values as follows:

- **X Coordinate**: 
  - If `event_coord_x < 564`: Use `event_coord_x` directly.
  - Otherwise: Calculate as `1128 - event_coord_x`.

- **Y Coordinate**: 
  - If `event_coord_x < 564`: Calculate as `600 - event_coord_y`.
  - Otherwise: Use `event_coord_y` directly.
",snowflake
188,bq428,ncaa_basketball,"For the top five team markets with the highest number of distinct players who scored at least 15 points during the second period of games between 2010 and 2018, provide details of each game they played in NCAA basketball historical tournament matches during the same period, as specified in the data model document.","WITH top_teams AS (
  SELECT
    team_market
  FROM (
    SELECT
      team_market,
      player_id AS id,
      SUM(points_scored)
    FROM
      `bigquery-public-data.ncaa_basketball.mbb_pbp_sr`
    WHERE
      season >= 2010 AND season <=2018 AND period = 2
    GROUP BY
      game_id,
      team_market,
      player_id
    HAVING
      SUM(points_scored) >= 15) C
  GROUP BY
    team_market
  HAVING
    COUNT(DISTINCT id) > 5
  ORDER BY
    COUNT(DISTINCT id) DESC
  LIMIT 5
)


SELECT
  season,
  round,
  days_from_epoch,
  game_date,
  day,
  'win' AS label,
  win_seed AS seed,
  win_market AS market,
  win_name AS name,
  win_alias AS alias,
  win_school_ncaa AS school_ncaa,
  lose_seed AS opponent_seed,
  lose_market AS opponent_market,
  lose_name AS opponent_name,
  lose_alias AS opponent_alias,
  lose_school_ncaa AS opponent_school_ncaa
FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
JOIN top_teams ON top_teams.team_market = win_market
WHERE season >= 2010 AND season <=2018

UNION ALL

SELECT
  season,
  round,
  days_from_epoch,
  game_date,
  day,
  'loss' AS label,
  lose_seed AS seed,
  lose_market AS market,
  lose_name AS name,
  lose_alias AS alias,
  lose_school_ncaa AS school_ncaa,
  win_seed AS opponent_seed,
  win_market AS opponent_market,
  win_name AS opponent_name,
  win_alias AS opponent_alias,
  win_school_ncaa AS opponent_school_ncaa
FROM `bigquery-public-data.ncaa_basketball.mbb_historical_tournament_games`
JOIN top_teams ON top_teams.team_market = lose_market
WHERE season >= 2010 AND season <=2018
","# NCAA Basketball Tournament Data Model Documentation

This documentation outlines the dataset that combines information about games where teams have either won or lost in the NCAA basketball tournaments, detailing each column's data format:

- **season**: The NCAA tournament season year.
- **round**: The tournament round during which the game took place.
- **days_from_epoch**: The number of days from a fixed epoch to the game date.
- **game_date**: The actual date when the game was played.
- **day**: The day of the week on which the game occurred.
- **label**: A string indicating whether the team won ('win') or lost ('loss') the game.
- **seed**: The tournament seed of the team (winning or losing).
- **market**: The market or region associated with the team.
- **name**: The name of the team.
- **alias**: The alias or abbreviation of the team name.
- **school_ncaa**: The NCAA identifier for the school.
- **opponent_seed**: The seed of the opposing team.
- **opponent_market**: The market or region associated with the opposing team.
- **opponent_name**: The name of the opposing team.
- **opponent_alias**: The alias or abbreviation of the opposing team's name.
- **opponent_school_ncaa**: The NCAA identifier for the opposing school.",snowflake
189,bq144,ncaa_insights,"Create a dataset by combining NCAA men's basketball tournament game outcomes from the 2014 season onwards, including both the historical tournament games and the 2018 tournament results, with the corresponding pace and efficiency performance metrics for each team and their opponents from the feature_engineering data. The dataset should include the season, game outcome labels (win or loss), team and opponent seeds, school names, pace and efficiency rankings, statistical values, and the differences between the team's and the opponent's metrics to enable a comprehensive analysis of team and opponent dynamics.","WITH outcomes AS (
SELECT

  season, # 1994
  ""win"" AS label, # our label
  win_seed AS seed, # ranking # this time without seed even
  win_school_ncaa AS school_ncaa,
  lose_seed AS opponent_seed, # ranking
  lose_school_ncaa AS opponent_school_ncaa
FROM `data-to-insights.ncaa.mbb_historical_tournament_games` t
WHERE season >= 2014
UNION ALL

SELECT

  season, # 1994
  ""loss"" AS label, # our label
  lose_seed AS seed, # ranking
  lose_school_ncaa AS school_ncaa,
  win_seed AS opponent_seed, # ranking
  win_school_ncaa AS opponent_school_ncaa
FROM
`data-to-insights.ncaa.mbb_historical_tournament_games` t
WHERE season >= 2014
UNION ALL

SELECT
  season,
  label,
  seed,
  school_ncaa,
  opponent_seed,
  opponent_school_ncaa
FROM
  `data-to-insights.ncaa.2018_tournament_results`
)
SELECT
o.season,
label,
  seed,
  school_ncaa,
  team.pace_rank,
  team.poss_40min,
  team.pace_rating,
  team.efficiency_rank,
  team.pts_100poss,
  team.efficiency_rating,
  opponent_seed,
  opponent_school_ncaa,
  opp.pace_rank AS opp_pace_rank,
  opp.poss_40min AS opp_poss_40min,
  opp.pace_rating AS opp_pace_rating,
  opp.efficiency_rank AS opp_efficiency_rank,
  opp.pts_100poss AS opp_pts_100poss,
  opp.efficiency_rating AS opp_efficiency_rating,
  opp.pace_rank - team.pace_rank AS pace_rank_diff,
  opp.poss_40min - team.poss_40min AS pace_stat_diff,
  opp.pace_rating - team.pace_rating AS pace_rating_diff,
  opp.efficiency_rank - team.efficiency_rank AS eff_rank_diff,
  opp.pts_100poss - team.pts_100poss AS eff_stat_diff,
  opp.efficiency_rating - team.efficiency_rating AS eff_rating_diff
FROM outcomes AS o
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS team
ON o.school_ncaa = team.team AND o.season = team.season
LEFT JOIN `data-to-insights.ncaa.feature_engineering` AS opp
ON o.opponent_school_ncaa = opp.team AND o.season = opp.season","#### NCAA Basketball Tournament SQL Query Variable Guide



This document provides a detailed explanation of the variables used and extracted in the SQL query focused on NCAA basketball tournament game outcomes. The purpose of this guide is to share insights into each variable's meaning and describe how they are derived within the query.



\## Variables and Their Meanings



\### Features and Labels



1. **`season`**:

   \- Represents the year of the NCAA tournament season being analyzed.

   \- **Source**: Directly selected from the datasets.



2. **`label`**:

   \- Indicates the outcome of the game for the team in focus: either ""win"" or ""loss"".

   \- **Source**: Manually assigned in the `SELECT` statement, depending on whether the team won or lost.



3. **`seed`**:

   \- The seed ranking of the team associated with the outcome.

   \- **Source**: Selected from `win_seed` or `lose_seed` as applicable from the dataset.



4. **`school_ncaa`**:

   \- The NCAA school or team involved in the game.

   \- **Source**: Selected from `win_school_ncaa` or `lose_school_ncaa` based on the context of the row (win/loss).



5. **`opponent_seed`**:

   \- The seed ranking of the opposing team.

   \- **Source**: Selected oppositely to `seed`.



6. **`opponent_school_ncaa`**:

   \- The opposing NCAA school or team.

   \- **Source**: Selected oppositely to `school_ncaa`.



\### Team Metrics



\#### New Pace Metrics



7. **`pace_rank`**:

   \- Reflects the team's rank in pace, measuring possessions over time.

   \- **Source**: Joined from `team.pace_rank` in the `feature_engineering` dataset.



8. **`poss_40min`**:

   \- The average number of possessions over a 40-minute game.

   \- **Source**: Joined from `team.poss_40min`.



9. **`pace_rating`**:

   \- Rating of the team's pace performance.

   \- **Source**: Joined from `team.pace_rating`.



\#### New Efficiency Metrics



10. **`efficiency_rank`**:

​    \- Rank based on scoring efficiency over time.

​    \- **Source**: Joined from `team.efficiency_rank`.



11. **`pts_100poss`**:

​    \- Points scored per 100 possessions.

​    \- **Source**: Joined from `team.pts_100poss`.



12. **`efficiency_rating`**:

​    \- Overall efficiency rating.

​    \- **Source**: Joined from `team.efficiency_rating`.



\### Opponent Metrics



13. **`opp_pace_rank`**, **`opp_poss_40min`**, **`opp_pace_rating`**, **`opp_efficiency_rank`**, **`opp_pts_100poss`**, **`opp_efficiency_rating`**:

​    \- Corresponding pace and efficiency metrics for the opponent team.

​    \- **Source**: Joined parallel fields from `opp` (opposing team's data) in the `feature_engineering` dataset.



\### Feature Engineering: Differences



14. **`pace_rank_diff`**, **`pace_stat_diff`**, **`pace_rating_diff`**, **`eff_rank_diff`**, **`eff_stat_diff`**, **`eff_rating_diff`**:

​    \- Differences calculated between the metrics of the opposing team and the team in focus.

​    \- **Source**: Derived by subtracting the team's metric from the opponent's metric using calculated fields in the `SELECT` statement.",snowflake
190,bq113,bls,"Which county in Utah experienced the greatest percentage increase in construction employment from 2000 to 2018, calculated by averaging the employment levels during the third month of each quarter in those years? What is the corresponding percentage increase?","WITH utah_code AS (
  SELECT DISTINCT geo_id
  FROM bigquery-public-data.geo_us_boundaries.states
  WHERE state_name = 'Utah'
),
e2000 as(
  SELECT
    AVG(month3_emplvl_23_construction) AS construction_employees_2000,
    geoid
  FROM
    `bigquery-public-data.bls_qcew.2000_*`
  WHERE
    geoid LIKE CONCAT((SELECT geo_id FROM utah_code), '%')
  GROUP BY
    geoid),

e2018 AS (
  SELECT
    AVG(month3_emplvl_23_construction) AS construction_employees_2018,
    geoid,
  FROM
    `bigquery-public-data.bls_qcew.2018_*` e2018
  WHERE
    geoid LIKE CONCAT((SELECT geo_id FROM utah_code), '%')
  GROUP BY
    geoid)

SELECT
  c.county_name AS county,
  (construction_employees_2018 - construction_employees_2000) / construction_employees_2000 * 100 AS increase_rate
FROM
  e2000
JOIN
  e2018 USING (geoid)
JOIN 
  `bigquery-public-data.geo_us_boundaries.counties` c ON c.geo_id = e2018.geoid
WHERE
  c.state_fips_code = (SELECT geo_id FROM utah_code)
ORDER BY
  increase_rate desc
LIMIT 1",,snowflake
191,bq112,bls,"Between 1998 and 2017, for Allegheny County in the Pittsburgh area, did the average annual wages for all industries keep pace with the inflation of all consumer items, and what were the respective percentage growth rates (to two decimal places) for wages and the CPI over that period?","WITH geo AS (
  SELECT DISTINCT geo_id
  FROM `bigquery-public-data.geo_us_boundaries.counties`
  WHERE county_name = ""Allegheny"" 
),
avg_wage_1998 AS(
  SELECT
    ROUND(AVG(avg_wkly_wage_10_total_all_industries) * 52, 2) AS wages_1998
  FROM
    `bigquery-public-data.bls_qcew.1998*`
  WHERE
    geoid = (SELECT geo_id FROM geo) --Selecting Allgeheny County
),
    
avg_wage_2017 AS (
  SELECT
    ROUND(AVG(avg_wkly_wage_10_total_all_industries) * 52, 2) AS wages_2017
  FROM
    `bigquery-public-data.bls_qcew.2017*`
  WHERE
    geoid = (SELECT geo_id FROM geo) --Selecting Allgeheny County
),

avg_cpi_1998 AS (
  SELECT
    AVG(value) AS cpi_1998
  FROM
    `bigquery-public-data.bls.cpi_u` c
  WHERE
    year = 1998
    AND item_code in (
      SELECT DISTINCT item_code FROM `bigquery-public-data.bls.cpi_u` WHERE LOWER(item_name) = ""all items""
    )
    AND area_code = (
      SELECT DISTINCT area_code FROM `bigquery-public-data.bls.cpi_u` WHERE area_name LIKE '%Pittsburgh%'
    )
), 
-- A104 is the code for Pittsburgh, PA
-- SA0 is the code for all items
    
avg_cpi_2017 AS(
  SELECT
    AVG(value) AS cpi_2017
  FROM
    `bigquery-public-data.bls.cpi_u` c
  WHERE
    year = 2017
    AND item_code in (
      SELECT DISTINCT item_code FROM `bigquery-public-data.bls.cpi_u` WHERE LOWER(item_name) = ""all items""
    )
    AND area_code = (
      SELECT DISTINCT area_code FROM `bigquery-public-data.bls.cpi_u` WHERE area_name LIKE '%Pittsburgh%'
    )
)
-- A104 is the code for Pittsburgh, PA
-- SA0 is the code for all items

SELECT
  ROUND((wages_2017 - wages_1998) / wages_1998 * 100, 2) AS wages_percent_change,
  ROUND((cpi_2017 - cpi_1998) / cpi_1998 * 100, 2) AS cpi_percent_change
FROM
  avg_wage_2017,
  avg_wage_1998,
  avg_cpi_2017,
  avg_cpi_1998",,snowflake
192,bq055,google_dei,"Can you provide the top three races with the largest percentage differences between Google's 2021 overall hiring data from dar non intersectional hiring and the average percentages in the 2021 BLS data for the technology sectors specifically defined as 'Internet publishing and broadcasting and web search portals,' 'Software publishers,' 'Data processing, hosting, and related services,' or the industry group 'Computer systems design and related services,' along with their respective differences?",,,snowflake
193,bq075,google_dei,"Could you provide a combined 2021 report comparing racial (Asian, Black, Hispanic/Latinx, White) and gender (U.S. Women, U.S. Men) distributions across Google’s overall workforce hiring, Google’s overall workforce representation, and the BLS data specifically for the technology sectors defined as Internet publishing and broadcasting and web search portals or Computer systems design and related services?",,,snowflake
194,bq406,google_dei,"Please calculate the growth rates for Asians, Black people, Latinx people, Native Americans, White people, US women, US men, global women, and global men from 2014 to 2024 concerning the overall workforce.","CREATE TEMP FUNCTION GrowthRate(end_value FLOAT64, begin_value FLOAT64)
RETURNS FLOAT64
AS ((end_value - begin_value) / begin_value);

SELECT
  GrowthRate(SUM(IF(report_year=2024, race_asian, 0)),
             SUM(IF(report_year=2014, race_asian, 0))) AS race_asian_growth,
  GrowthRate(SUM(IF(report_year=2024, race_black, 0)),
             SUM(IF(report_year=2014, race_black, 0))) AS race_black_growth,
  GrowthRate(SUM(IF(report_year=2024, race_hispanic_latinx, 0)),
             SUM(IF(report_year=2014, race_hispanic_latinx, 0))) AS race_hispanic_growth,
  GrowthRate(SUM(IF(report_year=2024, race_native_american, 0)),
             SUM(IF(report_year=2014, race_native_american, 0))) AS race_native_american_growth,
  GrowthRate(SUM(IF(report_year=2024, race_white, 0)),
             SUM(IF(report_year=2014, race_white, 0))) AS race_white_growth,
  GrowthRate(SUM(IF(report_year=2024, gender_us_women, 0)),
             SUM(IF(report_year=2014, gender_us_women, 0))) AS gender_us_women_growth,
  GrowthRate(SUM(IF(report_year=2024, gender_us_men, 0)),
             SUM(IF(report_year=2014, gender_us_men, 0))) AS gender_us_men_growth,
  GrowthRate(SUM(IF(report_year=2024, gender_global_women, 0)),
             SUM(IF(report_year=2014, gender_global_women, 0))) AS gender_global_women_growth,
  GrowthRate(SUM(IF(report_year=2024, gender_global_men, 0)),
             SUM(IF(report_year=2014, gender_global_men, 0))) AS gender_global_men_growth
FROM `bigquery-public-data.google_dei.dar_non_intersectional_representation`
WHERE report_year IN (2014, 2024)
  AND workforce = 'overall';",,snowflake
195,sf_bq084,GOOG_BLOCKCHAIN,"For each month in the year 2023, how many total transactions occurred (counting all transaction records without removing duplicates of transaction hashes), and how many transactions per second were processed each month, where the transactions-per-second value is calculated by dividing the monthly total count by the exact number of seconds in that month, including the correct leap-year logic if applicable based on the extracted year from the transaction timestamp? Show the monthly transaction count, the computed transactions per second, the year, and the month, and present the rows in descending order of the monthly transaction count.",,,snowflake
196,sf_bq058,GOOG_BLOCKCHAIN,"Retrieve all finalized deposits into Optimism at block 29815485 using the Optimism Standard Bridge, including transaction hash, an Etherscan link (the complete URL), L1 and L2 token addresses, sender and receiver addresses (with leading zeroes stripped), and the deposited amount (converted from hex to decimal). Ensure data is properly formatted and parsed according to Optimism's address and token standards, and remove the prefix '0x' except transaction hash. Note that, the keccak-256 hash of the Ethereum event signature for DepositFinalized is ""0x3303facd24627943a92e9dc87cfbb34b15c49b726eec3ad3487c16be9ab8efe8"".",,"---
title: ""Optimism standard bridge contract walkthrough""
description: How does the standard bridge for Optimism work? Why does it work this way?
author: Ori Pomerantz
tags: [""solidity"", ""bridge"", ""layer 2""]
skill: intermediate
published: 2022-03-30
lang: en
---

[Optimism](https://www.optimism.io/) is an [Optimistic Rollup](/developers/docs/scaling/optimistic-rollups/).
Optimistic rollups can process transactions for a much lower price than Ethereum Mainnet (also known as layer 1 or L1) because transactions are only processed by a few nodes, instead of every node on the network.
At the same time, the data is all written to L1 so everything can be proved and reconstructed with all the integrity and availability guarantees of Mainnet.

To use L1 assets on Optimism (or any other L2), the assets need to be [bridged](/bridges/#prerequisites).
One way to achieve this is for users to lock assets (ETH and [ERC-20 tokens](/developers/docs/standards/tokens/erc-20/) are the most common ones) on L1, and receive equivalent assets to use on L2.
Eventually, whoever ends up with them might want to bridge them back to L1.
When doing this, the assets are burned on L2 and then released back to the user on L1.

This is the way the [Optimism standard bridge](https://community.optimism.io/docs/developers/bridge/standard-bridge) works.
In this article we go over the source code for that bridge to see how it works and study it as an example of well written Solidity code.

## Control flows {#control-flows}

The bridge has two main flows:

- Deposit (from L1 to L2)
- Withdrawal (from L2 to L1)

### Deposit flow {#deposit-flow}

#### Layer 1 {#deposit-flow-layer-1}

1. If depositing an ERC-20, the depositor gives the bridge an allowance to spend the amount being deposited
2. The depositor calls the L1 bridge (`depositERC20`, `depositERC20To`, `depositETH`, or `depositETHTo`)
3. The L1 bridge takes possession of the bridged asset
   - ETH: The asset is transferred by the depositor as part of the call
   - ERC-20: The asset is transferred by the bridge to itself using the allowance provided by the depositor
4. The L1 bridge uses the cross-domain message mechanism to call `finalizeDeposit` on the L2 bridge

#### Layer 2 {#deposit-flow-layer-2}

5. The L2 bridge verifies the call to `finalizeDeposit` is legitimate:
   - Came from the cross domain message contract
   - Was originally from the bridge on L1
6. The L2 bridge checks if the ERC-20 token contract on L2 is the correct one:
   - The L2 contract reports that its L1 counterpart is the same as the one the tokens came from on L1
   - The L2 contract reports that it supports the correct interface ([using ERC-165](https://eips.ethereum.org/EIPS/eip-165)).
7. If the L2 contract is the correct one, call it to mint the appropriate number of tokens to the appropriate address. If not, start a withdrawal process to allow the user to claim the tokens on L1.

### Withdrawal flow {#withdrawal-flow}

#### Layer 2 {#withdrawal-flow-layer-2}

1. The withdrawer calls the L2 bridge (`withdraw` or `withdrawTo`)
2. The L2 bridge burns the appropriate number of tokens belonging to `msg.sender`
3. The L2 bridge uses the cross-domain message mechanism to call `finalizeETHWithdrawal` or `finalizeERC20Withdrawal` on the L1 bridge

#### Layer 1 {#withdrawal-flow-layer-1}

4. The L1 bridge verifies the call to `finalizeETHWithdrawal` or `finalizeERC20Withdrawal` is legitimate:
   - Came from the cross domain message mechanism
   - Was originally from the bridge on L2
5. The L1 bridge transfers the appropriate asset (ETH or ERC-20) to the appropriate address

## Layer 1 code {#layer-1-code}

This is the code that runs on L1, the Ethereum Mainnet.

### IL1ERC20Bridge {#IL1ERC20Bridge}

[This interface is defined here](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L1/messaging/IL1ERC20Bridge.sol).
It includes functions and definitions required for bridging ERC-20 tokens.

```solidity
// SPDX-License-Identifier: MIT
```

[Most of Optimism's code is released under the MIT license](https://help.optimism.io/hc/en-us/articles/4411908707995-What-software-license-does-Optimism-use-).

```solidity
pragma solidity >0.5.0 <0.9.0;
```

At writing the latest version of Solidity is 0.8.12.
Until version 0.9.0 is released, we don't know if this code is compatible with it or not.

```solidity
/**
 * @title IL1ERC20Bridge
 */
interface IL1ERC20Bridge {
    /**********
     * Events *
     **********/

    event ERC20DepositInitiated(
```

In Optimism bridge terminology _deposit_ means transfer from L1 to L2, and _withdrawal_ means a transfer from L2 to L1.

```solidity
        address indexed _l1Token,
        address indexed _l2Token,
```

In most cases the address of an ERC-20 on L1 is not the same the address of the equivalent ERC-20 on L2.
[You can see the list of token addresses here](https://static.optimism.io/optimism.tokenlist.json).
The address with `chainId` 1 is on L1 (Mainnet) and the address with `chainId` 10 is on L2 (Optimism).
The other two `chainId` values are for the Kovan test network (42) and the Optimistic Kovan test network (69).

```solidity
        address indexed _from,
        address _to,
        uint256 _amount,
        bytes _data
    );
```

It is possible to add notes to transfers, in which case they are added to the events that report them.

```solidity
    event ERC20WithdrawalFinalized(
        address indexed _l1Token,
        address indexed _l2Token,
        address indexed _from,
        address _to,
        uint256 _amount,
        bytes _data
    );
```

The same bridge contract handles transfers in both directions.
In the case of the L1 bridge, this means initialization of deposits and finalization of withdrawals.

```solidity

    /********************
     * Public Functions *
     ********************/

    /**
     * @dev get the address of the corresponding L2 bridge contract.
     * @return Address of the corresponding L2 bridge contract.
     */
    function l2TokenBridge() external returns (address);
```

This function is not really needed, because on L2 it is a predeployed contract, so it is always at address `0x4200000000000000000000000000000000000010`.
It is here for symmetry with the L2 bridge, because the address of the L1 bridge is _not_ trivial to know.

```solidity
    /**
     * @dev deposit an amount of the ERC20 to the caller's balance on L2.
     * @param _l1Token Address of the L1 ERC20 we are depositing
     * @param _l2Token Address of the L1 respective L2 ERC20
     * @param _amount Amount of the ERC20 to deposit
     * @param _l2Gas Gas limit required to complete the deposit on L2.
     * @param _data Optional data to forward to L2. This data is provided
     *        solely as a convenience for external contracts. Aside from enforcing a maximum
     *        length, these contracts provide no guarantees about its content.
     */
    function depositERC20(
        address _l1Token,
        address _l2Token,
        uint256 _amount,
        uint32 _l2Gas,
        bytes calldata _data
    ) external;
```

The `_l2Gas` parameter is the amount of L2 gas the transaction is allowed to spend.
[Up to a certain (high) limit, this is free](https://community.optimism.io/docs/developers/bridge/messaging/#for-l1-%E2%87%92-l2-transactions-2), so unless the ERC-20 contract does something really strange when minting, it should not be an issue.
This function takes care of the common scenario, where a user bridges assets to the same address on a different blockchain.

```solidity
    /**
     * @dev deposit an amount of ERC20 to a recipient's balance on L2.
     * @param _l1Token Address of the L1 ERC20 we are depositing
     * @param _l2Token Address of the L1 respective L2 ERC20
     * @param _to L2 address to credit the withdrawal to.
     * @param _amount Amount of the ERC20 to deposit.
     * @param _l2Gas Gas limit required to complete the deposit on L2.
     * @param _data Optional data to forward to L2. This data is provided
     *        solely as a convenience for external contracts. Aside from enforcing a maximum
     *        length, these contracts provide no guarantees about its content.
     */
    function depositERC20To(
        address _l1Token,
        address _l2Token,
        address _to,
        uint256 _amount,
        uint32 _l2Gas,
        bytes calldata _data
    ) external;
```

This function is almost identical to `depositERC20`, but it lets you send the ERC-20 to a different address.

```solidity
    /*************************
     * Cross-chain Functions *
     *************************/

    /**
     * @dev Complete a withdrawal from L2 to L1, and credit funds to the recipient's balance of the
     * L1 ERC20 token.
     * This call will fail if the initialized withdrawal from L2 has not been finalized.
     *
     * @param _l1Token Address of L1 token to finalizeWithdrawal for.
     * @param _l2Token Address of L2 token where withdrawal was initiated.
     * @param _from L2 address initiating the transfer.
     * @param _to L1 address to credit the withdrawal to.
     * @param _amount Amount of the ERC20 to deposit.
     * @param _data Data provided by the sender on L2. This data is provided
     *   solely as a convenience for external contracts. Aside from enforcing a maximum
     *   length, these contracts provide no guarantees about its content.
     */
    function finalizeERC20Withdrawal(
        address _l1Token,
        address _l2Token,
        address _from,
        address _to,
        uint256 _amount,
        bytes calldata _data
    ) external;
}
```

Withdrawals (and other messages from L2 to L1) in Optimism are a two step process:

1. An initiating transaction on L2.
2. A finalizing or claiming transaction on L1.
   This transaction needs to happen after the [fault challenge period](https://community.optimism.io/docs/how-optimism-works/#fault-proofs) for the L2 transaction ends.

### IL1StandardBridge {#il1standardbridge}

[This interface is defined here](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L1/messaging/IL1StandardBridge.sol).
This file contains event and function definitions for ETH.
These definitions are very similar to those defined in `IL1ERC20Bridge` above for ERC-20.

The bridge interface is divided between two files because some ERC-20 tokens require custom processing and cannot be handled by the standard bridge.
This way the custom bridge that handles such a token can implement `IL1ERC20Bridge` and not have to also bridge ETH.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity >0.5.0 <0.9.0;

import ""./IL1ERC20Bridge.sol"";

/**
 * @title IL1StandardBridge
 */
interface IL1StandardBridge is IL1ERC20Bridge {
    /**********
     * Events *
     **********/
    event ETHDepositInitiated(
        address indexed _from,
        address indexed _to,
        uint256 _amount,
        bytes _data
    );
```

This event is nearly identical to the ERC-20 version (`ERC20DepositInitiated`), except without the L1 and L2 token addresses.
The same is true for the other events and the functions.

```solidity
    event ETHWithdrawalFinalized(
        .
        .
        .
    );

    /********************
     * Public Functions *
     ********************/

    /**
     * @dev Deposit an amount of the ETH to the caller's balance on L2.
            .
            .
            .
     */
    function depositETH(uint32 _l2Gas, bytes calldata _data) external payable;

    /**
     * @dev Deposit an amount of ETH to a recipient's balance on L2.
            .
            .
            .
     */
    function depositETHTo(
        address _to,
        uint32 _l2Gas,
        bytes calldata _data
    ) external payable;

    /*************************
     * Cross-chain Functions *
     *************************/

    /**
     * @dev Complete a withdrawal from L2 to L1, and credit funds to the recipient's balance of the
     * L1 ETH token. Since only the xDomainMessenger can call this function, it will never be called
     * before the withdrawal is finalized.
                .
                .
                .
     */
    function finalizeETHWithdrawal(
        address _from,
        address _to,
        uint256 _amount,
        bytes calldata _data
    ) external;
}
```

### CrossDomainEnabled {#crossdomainenabled}

[This contract](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/libraries/bridge/CrossDomainEnabled.sol) is inherited by both bridges ([L1](#the-l1-bridge-contract) and [L2](#the-l2-bridge-contract)) to send messages to the other layer.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity >0.5.0 <0.9.0;

/* Interface Imports */
import { ICrossDomainMessenger } from ""./ICrossDomainMessenger.sol"";
```

[This interface](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/libraries/bridge/ICrossDomainMessenger.sol) tells the contract how to send messages to the other layer, using the cross domain messenger.
This cross domain messenger is a whole other system, and deserves its own article, which I hope to write in the future.

```solidity
/**
 * @title CrossDomainEnabled
 * @dev Helper contract for contracts performing cross-domain communications
 *
 * Compiler used: defined by inheriting contract
 */
contract CrossDomainEnabled {
    /*************
     * Variables *
     *************/

    // Messenger contract used to send and receive messages from the other domain.
    address public messenger;

    /***************
     * Constructor *
     ***************/

    /**
     * @param _messenger Address of the CrossDomainMessenger on the current layer.
     */
    constructor(address _messenger) {
        messenger = _messenger;
    }
```

The one parameter that the contract needs to know, the address of the cross domain messenger on this layer.
This parameter is set once, in the constructor, and never changes.

```solidity

    /**********************
     * Function Modifiers *
     **********************/

    /**
     * Enforces that the modified function is only callable by a specific cross-domain account.
     * @param _sourceDomainAccount The only account on the originating domain which is
     *  authenticated to call this function.
     */
    modifier onlyFromCrossDomainAccount(address _sourceDomainAccount) {
```

The cross domain messaging is accessible by any contract on the blockchain where it is running (either Ethereum mainnet or Optimism).
But we need the bridge on each side to _only_ trust certain messages if they come from the bridge on the other side.

```solidity
        require(
            msg.sender == address(getCrossDomainMessenger()),
            ""OVM_XCHAIN: messenger contract unauthenticated""
        );
```

Only messages from the appropriate cross domain messenger (`messenger`, as you see below) can be trusted.

```solidity

        require(
            getCrossDomainMessenger().xDomainMessageSender() == _sourceDomainAccount,
            ""OVM_XCHAIN: wrong sender of cross-domain message""
        );
```

The way the cross domain messenger provides the address that sent a message with the other layer is [the `.xDomainMessageSender()` function](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L1/messaging/L1CrossDomainMessenger.sol#L122-L128).
As long as it is called in the transaction that was initiated by the message it can provide this information.

We need to make sure that the message we received came from the other bridge.

```solidity

        _;
    }

    /**********************
     * Internal Functions *
     **********************/

    /**
     * Gets the messenger, usually from storage. This function is exposed in case a child contract
     * needs to override.
     * @return The address of the cross-domain messenger contract which should be used.
     */
    function getCrossDomainMessenger() internal virtual returns (ICrossDomainMessenger) {
        return ICrossDomainMessenger(messenger);
    }
```

This function returns the cross domain messenger.
We use a function rather than the variable `messenger` to allow contracts that inherit from this one to use an algorithm to specify which cross domain messenger to use.

```solidity

    /**
     * Sends a message to an account on another domain
     * @param _crossDomainTarget The intended recipient on the destination domain
     * @param _message The data to send to the target (usually calldata to a function with
     *  `onlyFromCrossDomainAccount()`)
     * @param _gasLimit The gasLimit for the receipt of the message on the target domain.
     */
    function sendCrossDomainMessage(
        address _crossDomainTarget,
        uint32 _gasLimit,
        bytes memory _message
```

Finally, the function that sends a message to the other layer.

```solidity
    ) internal {
        // slither-disable-next-line reentrancy-events, reentrancy-benign
```

[Slither](https://github.com/crytic/slither) is a static analyzer Optimism runs on every contract to look for vulnerabilities and other potential problems.
In this case, the following line triggers two vulnerabilities:

1. [Reentrancy events](https://github.com/crytic/slither/wiki/Detector-Documentation#reentrancy-vulnerabilities-3)
2. [Benign reentrancy](https://github.com/crytic/slither/wiki/Detector-Documentation#reentrancy-vulnerabilities-2)

```solidity
        getCrossDomainMessenger().sendMessage(_crossDomainTarget, _message, _gasLimit);
    }
}
```

In this case we are not worried about reentrancy we know `getCrossDomainMessenger()` returns a trustworthy address, even if Slither has no way to know that.

### The L1 bridge contract {#the-l1-bridge-contract}

[The source code for this contract is here](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L1/messaging/L1StandardBridge.sol).

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.9;
```

The interfaces can be part of other contracts, so they have to support a wide range of Solidity versions.
But the bridge itself is our contract, and we can be strict about what Solidity version it uses.

```solidity
/* Interface Imports */
import { IL1StandardBridge } from ""./IL1StandardBridge.sol"";
import { IL1ERC20Bridge } from ""./IL1ERC20Bridge.sol"";
```

[IL1ERC20Bridge](#IL1ERC20Bridge) and [IL1StandardBridge](#IL1StandardBridge) are explained above.

```solidity
import { IL2ERC20Bridge } from ""../../L2/messaging/IL2ERC20Bridge.sol"";
```

[This interface](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L2/messaging/IL2ERC20Bridge.sol) lets us create messages to control the standard bridge on L2.

```solidity
import { IERC20 } from ""@openzeppelin/contracts/token/ERC20/IERC20.sol"";
```

[This interface](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/IERC20.sol) lets us control ERC-20 contracts.
[You can read more about it here](/developers/tutorials/erc20-annotated-code/#the-interface).

```solidity
/* Library Imports */
import { CrossDomainEnabled } from ""../../libraries/bridge/CrossDomainEnabled.sol"";
```

[As explained above](#crossdomainenabled), this contract is used for interlayer messaging.

```solidity
import { Lib_PredeployAddresses } from ""../../libraries/constants/Lib_PredeployAddresses.sol"";
```

[`Lib_PredeployAddresses`](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/libraries/constants/Lib_PredeployAddresses.sol) has the addresses for the L2 contracts that always have the same address. This includes the standard bridge on L2.

```solidity
import { Address } from ""@openzeppelin/contracts/utils/Address.sol"";
```

[OpenZeppelin's Address utilities](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/Address.sol). It is used to distinguish between contract addresses and those belonging to externally owned accounts (EOA).

Note that this isn't a perfect solution, because there is no way to distinguish between direct calls and calls made from a contract's constructor, but at least this lets us identify and prevent some common user errors.

```solidity
import { SafeERC20 } from ""@openzeppelin/contracts/token/ERC20/utils/SafeERC20.sol"";
```

[The ERC-20 standard](https://eips.ethereum.org/EIPS/eip-20) supports two ways for a contract to report failure:

1. Revert
2. Return `false`

Handling both cases would make our code more complicated, so instead we use [OpenZeppelin's `SafeERC20`](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol), which makes sure [all failures result in a revert](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/utils/SafeERC20.sol#L96).

```solidity
/**
 * @title L1StandardBridge
 * @dev The L1 ETH and ERC20 Bridge is a contract which stores deposited L1 funds and standard
 * tokens that are in use on L2. It synchronizes a corresponding L2 Bridge, informing it of deposits
 * and listening to it for newly finalized withdrawals.
 *
 */
contract L1StandardBridge is IL1StandardBridge, CrossDomainEnabled {
    using SafeERC20 for IERC20;
```

This line is how we specify to use the `SafeERC20` wrapper every time we use the `IERC20` interface.

```solidity

    /********************************
     * External Contract References *
     ********************************/

    address public l2TokenBridge;
```

The address of [L2StandardBridge](#the-l2-bridge-contract).

```solidity

    // Maps L1 token to L2 token to balance of the L1 token deposited
    mapping(address => mapping(address => uint256)) public deposits;
```

A double [mapping](https://www.tutorialspoint.com/solidity/solidity_mappings.htm) like this is the way you define a [two-dimensional sparse array](https://en.wikipedia.org/wiki/Sparse_matrix).
Values in this data structure are identified as `deposit[L1 token addr][L2 token addr]`.
The default value is zero.
Only cells that are set to a different value are written to storage.

```solidity

    /***************
     * Constructor *
     ***************/

    // This contract lives behind a proxy, so the constructor parameters will go unused.
    constructor() CrossDomainEnabled(address(0)) {}
```

To want to be able to upgrade this contract without having to copy all the variables in the storage.
To do that we use a [`Proxy`](https://docs.openzeppelin.com/contracts/3.x/api/proxy), a contract that uses [`delegatecall`](https://solidity-by-example.org/delegatecall/) to transfer calls to a separate contact whose address is stored by the proxy contract (when you upgrade you tell the proxy to change that address).
When you use `delegatecall` the storage remains the storage of the _calling_ contract, so the values of all the contract state variables are unaffected.

One effect of this pattern is that the storage of the contract that is the _called_ of `delegatecall` is not used and therefore the constructor values passed to it do not matter.
This is the reason we can provide a nonsensical value to the `CrossDomainEnabled` constructor.
It is also the reason the initialization below is separate from the constructor.

```solidity
    /******************
     * Initialization *
     ******************/

    /**
     * @param _l1messenger L1 Messenger address being used for cross-chain communications.
     * @param _l2TokenBridge L2 standard bridge address.
     */
    // slither-disable-next-line external-function
```

This [Slither test](https://github.com/crytic/slither/wiki/Detector-Documentation#public-function-that-could-be-declared-external) identifies functions that are not called from the contract code and could therefore be declared `external` instead of `public`.
The gas cost of `external` functions can be lower, because they can be provided with parameters in the calldata.
Functions declared `public` have to be accessible from within the contract.
Contracts cannot modify their own calldata, so the parameters have to be in memory.
When such a function is called externally, it is necessary to copy the calldata to memory, which costs gas.
In this case the function is only called once, so the inefficiency does not matter to us.

```solidity
    function initialize(address _l1messenger, address _l2TokenBridge) public {
        require(messenger == address(0), ""Contract has already been initialized."");
```

The `initialize` function should only be called once.
If the address of either the L1 cross domain messenger or the L2 token bridge changes, we create a new proxy and a new bridge that calls it.
This is unlikely to happen except when the entire system is upgraded, a very rare occurrence.

Note that this function does not have any mechanism that restricts _who_ can call it.
This means that in theory an attacker could wait until we deploy the proxy and the first version of the bridge and then [front-run](https://solidity-by-example.org/hacks/front-running/) to get to the `initialize` function before the legitimate user does. But there are two methods to prevent this:

1. If the contracts are deployed not directly by an EOA but [in a transaction that has another contract create them](https://medium.com/upstate-interactive/creating-a-contract-with-a-smart-contract-bdb67c5c8595) the entire process can be atomic, and finish before any other transaction is executed.
2. If the legitimate call to `initialize` fails it is always possible to ignore the newly created proxy and bridge and create new ones.

```solidity
        messenger = _l1messenger;
        l2TokenBridge = _l2TokenBridge;
    }
```

These are the two parameters that the bridge needs to know.

```solidity

    /**************
     * Depositing *
     **************/

    /** @dev Modifier requiring sender to be EOA.  This check could be bypassed by a malicious
     *  contract via initcode, but it takes care of the user error we want to avoid.
     */
    modifier onlyEOA() {
        // Used to stop deposits from contracts (avoid accidentally lost tokens)
        require(!Address.isContract(msg.sender), ""Account not EOA"");
        _;
    }
```

This is the reason we needed OpenZeppelin's `Address` utilities.

```solidity
    /**
     * @dev This function can be called with no data
     * to deposit an amount of ETH to the caller's balance on L2.
     * Since the receive function doesn't take data, a conservative
     * default amount is forwarded to L2.
     */
    receive() external payable onlyEOA {
        _initiateETHDeposit(msg.sender, msg.sender, 200_000, bytes(""""));
    }
```

This function exists for testing purposes.
Notice that it doesn't appear in the interface definitions - it isn't for normal use.

```solidity
    /**
     * @inheritdoc IL1StandardBridge
     */
    function depositETH(uint32 _l2Gas, bytes calldata _data) external payable onlyEOA {
        _initiateETHDeposit(msg.sender, msg.sender, _l2Gas, _data);
    }

    /**
     * @inheritdoc IL1StandardBridge
     */
    function depositETHTo(
        address _to,
        uint32 _l2Gas,
        bytes calldata _data
    ) external payable {
        _initiateETHDeposit(msg.sender, _to, _l2Gas, _data);
    }
```

These two functions are wrappers around `_initiateETHDeposit`, the function that handles the actual ETH deposit.

```solidity
    /**
     * @dev Performs the logic for deposits by storing the ETH and informing the L2 ETH Gateway of
     * the deposit.
     * @param _from Account to pull the deposit from on L1.
     * @param _to Account to give the deposit to on L2.
     * @param _l2Gas Gas limit required to complete the deposit on L2.
     * @param _data Optional data to forward to L2. This data is provided
     *        solely as a convenience for external contracts. Aside from enforcing a maximum
     *        length, these contracts provide no guarantees about its content.
     */
    function _initiateETHDeposit(
        address _from,
        address _to,
        uint32 _l2Gas,
        bytes memory _data
    ) internal {
        // Construct calldata for finalizeDeposit call
        bytes memory message = abi.encodeWithSelector(
```

The way that cross domain messages work is that the destination contract is called with the message as its calldata.
Solidity contracts always interpret their calldata in accordance with
[the ABI specifications](https://docs.soliditylang.org/en/v0.8.12/abi-spec.html).
The Solidity function [`abi.encodeWithSelector`](https://docs.soliditylang.org/en/v0.8.12/units-and-global-variables.html#abi-encoding-and-decoding-functions) creates that calldata.

```solidity
            IL2ERC20Bridge.finalizeDeposit.selector,
            address(0),
            Lib_PredeployAddresses.OVM_ETH,
            _from,
            _to,
            msg.value,
            _data
        );
```

The message here is to call [the `finalizeDeposit` function](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L2/messaging/L2StandardBridge.sol#L141-L148) with these parameters:

| Parameter | Value                          | Meaning                                                                                                                                      |
| --------- | ------------------------------ | -------------------------------------------------------------------------------------------------------------------------------------------- |
| \_l1Token | address(0)                     | Special value to stand for ETH (which isn't an ERC-20 token) on L1                                                                           |
| \_l2Token | Lib_PredeployAddresses.OVM_ETH | The L2 contract that manages ETH on Optimism, `0xDeadDeAddeAddEAddeadDEaDDEAdDeaDDeAD0000` (this contract is for internal Optimism use only) |
| \_from    | \_from                         | The address on L1 that sends the ETH                                                                                                         |
| \_to      | \_to                           | The address on L2 that receives the ETH                                                                                                      |
| amount    | msg.value                      | Amount of wei sent (which has already been sent to the bridge)                                                                               |
| \_data    | \_data                         | Additional data to attach to the deposit                                                                                                     |

```solidity
        // Send calldata into L2
        // slither-disable-next-line reentrancy-events
        sendCrossDomainMessage(l2TokenBridge, _l2Gas, message);
```

Send the message through the cross domain messenger.

```solidity
        // slither-disable-next-line reentrancy-events
        emit ETHDepositInitiated(_from, _to, msg.value, _data);
    }
```

Emit an event to inform any decentralized application that listens of this transfer.

```solidity
    /**
     * @inheritdoc IL1ERC20Bridge
     */
    function depositERC20(
		.
		.
		.
    ) external virtual onlyEOA {
        _initiateERC20Deposit(_l1Token, _l2Token, msg.sender, msg.sender, _amount, _l2Gas, _data);
    }

    /**
     * @inheritdoc IL1ERC20Bridge
     */
    function depositERC20To(
		.
		.
		.
    ) external virtual {
        _initiateERC20Deposit(_l1Token, _l2Token, msg.sender, _to, _amount, _l2Gas, _data);
    }
```

These two functions are wrappers around `_initiateERC20Deposit`, the function that handles the actual ERC-20 deposit.

```solidity
    /**
     * @dev Performs the logic for deposits by informing the L2 Deposited Token
     * contract of the deposit and calling a handler to lock the L1 funds. (e.g. transferFrom)
     *
     * @param _l1Token Address of the L1 ERC20 we are depositing
     * @param _l2Token Address of the L1 respective L2 ERC20
     * @param _from Account to pull the deposit from on L1
     * @param _to Account to give the deposit to on L2
     * @param _amount Amount of the ERC20 to deposit.
     * @param _l2Gas Gas limit required to complete the deposit on L2.
     * @param _data Optional data to forward to L2. This data is provided
     *        solely as a convenience for external contracts. Aside from enforcing a maximum
     *        length, these contracts provide no guarantees about its content.
     */
    function _initiateERC20Deposit(
        address _l1Token,
        address _l2Token,
        address _from,
        address _to,
        uint256 _amount,
        uint32 _l2Gas,
        bytes calldata _data
    ) internal {
```

This function is similar to `_initiateETHDeposit` above, with a few important differences.
The first difference is that this function receives the token addresses and the amount to transfer as parameters.
In the case of ETH the call to the bridge already includes the transfer of asset to the bridge account (`msg.value`).

```solidity
        // When a deposit is initiated on L1, the L1 Bridge transfers the funds to itself for future
        // withdrawals. safeTransferFrom also checks if the contract has code, so this will fail if
        // _from is an EOA or address(0).
        // slither-disable-next-line reentrancy-events, reentrancy-benign
        IERC20(_l1Token).safeTransferFrom(_from, address(this), _amount);
```

ERC-20 token transfers follow a different process from ETH:

1. The user (`_from`) gives an allowance to the bridge to transfer the appropriate tokens.
2. The user calls the bridge with the address of the token contract, the amount, etc.
3. The bridge transfers the tokens (to itself) as part of the deposit process.

The first step may happen in a separate transaction from the last two.
However, front-running is not a problem because the two functions that call `_initiateERC20Deposit` (`depositERC20` and `depositERC20To`) only call this function with `msg.sender` as the `_from` parameter.

```solidity
        // Construct calldata for _l2Token.finalizeDeposit(_to, _amount)
        bytes memory message = abi.encodeWithSelector(
            IL2ERC20Bridge.finalizeDeposit.selector,
            _l1Token,
            _l2Token,
            _from,
            _to,
            _amount,
            _data
        );

        // Send calldata into L2
        // slither-disable-next-line reentrancy-events, reentrancy-benign
        sendCrossDomainMessage(l2TokenBridge, _l2Gas, message);

        // slither-disable-next-line reentrancy-benign
        deposits[_l1Token][_l2Token] = deposits[_l1Token][_l2Token] + _amount;
```

Add the deposited amount of tokens to the `deposits` data structure.
There could be multiple addresses on L2 that correspond to the same L1 ERC-20 token, so it is not sufficient to use the bridge's balance of the L1 ERC-20 token to keep track of deposits.

```solidity

        // slither-disable-next-line reentrancy-events
        emit ERC20DepositInitiated(_l1Token, _l2Token, _from, _to, _amount, _data);
    }

    /*************************
     * Cross-chain Functions *
     *************************/

    /**
     * @inheritdoc IL1StandardBridge
     */
    function finalizeETHWithdrawal(
        address _from,
        address _to,
        uint256 _amount,
        bytes calldata _data
```

The L2 bridge sends a message to the L2 cross domain messenger which causes the L1 cross domain messenger to call this function (once the [transaction that finalizes the message](https://community.optimism.io/docs/developers/bridge/messaging/#fees-for-l2-%E2%87%92-l1-transactions) is submitted on L1, of course).

```solidity
    ) external onlyFromCrossDomainAccount(l2TokenBridge) {
```

Make sure that this is a _legitimate_ message, coming from the cross domain messenger and originating with the L2 token bridge.
This function is used to withdraw ETH from the bridge, so we have to make sure it is only called by the authorized caller.

```solidity
        // slither-disable-next-line reentrancy-events
        (bool success, ) = _to.call{ value: _amount }(new bytes(0));
```

The way to transfer ETH is to call the recipient with the amount of wei in the `msg.value`.

```solidity
        require(success, ""TransferHelper::safeTransferETH: ETH transfer failed"");

        // slither-disable-next-line reentrancy-events
        emit ETHWithdrawalFinalized(_from, _to, _amount, _data);
```

Emit an event about the withdrawal.

```solidity
    }

    /**
     * @inheritdoc IL1ERC20Bridge
     */
    function finalizeERC20Withdrawal(
        address _l1Token,
        address _l2Token,
        address _from,
        address _to,
        uint256 _amount,
        bytes calldata _data
    ) external onlyFromCrossDomainAccount(l2TokenBridge) {
```

This function is similar to `finalizeETHWithdrawal` above, with the necessary changes for ERC-20 tokens.

```solidity
        deposits[_l1Token][_l2Token] = deposits[_l1Token][_l2Token] - _amount;
```

Update the `deposits` data structure.

```solidity

        // When a withdrawal is finalized on L1, the L1 Bridge transfers the funds to the withdrawer
        // slither-disable-next-line reentrancy-events
        IERC20(_l1Token).safeTransfer(_to, _amount);

        // slither-disable-next-line reentrancy-events
        emit ERC20WithdrawalFinalized(_l1Token, _l2Token, _from, _to, _amount, _data);
    }


    /*****************************
     * Temporary - Migrating ETH *
     *****************************/

    /**
     * @dev Adds ETH balance to the account. This is meant to allow for ETH
     * to be migrated from an old gateway to a new gateway.
     * NOTE: This is left for one upgrade only so we are able to receive the migrated ETH from the
     * old contract
     */
    function donateETH() external payable {}
}
```

There was an earlier implementation of the bridge.
When we moved from the implementation to this one, we had to move all the assets.
ERC-20 tokens can just be moved.
However, to transfer ETH to a contract you need that contract's approval, which is what `donateETH` provides us.

## ERC-20 Tokens on L2 {#erc-20-tokens-on-l2}

For an ERC-20 token to fit into the standard bridge, it needs to allow the standard bridge, and _only_ the standard bridge, to mint token.
This is necessary because the bridges need to ensure that the number of tokens circulating on Optimism is equal to the number of tokens locked inside the L1 bridge contract.
If there are too many tokens on L2 some users would be unable to bridge their assets back to L1.
Instead of a trusted bridge, we would essentially recreate [fractional reserve banking](https://www.investopedia.com/terms/f/fractionalreservebanking.asp).
If there are too many tokens on L1, some of those tokens would stay locked inside the bridge contract forever because there is no way to release them without burning L2 tokens.

### IL2StandardERC20 {#il2standarderc20}

Every ERC-20 token on L2 that uses the standard bridge needs to provide [this interface](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/standards/IL2StandardERC20.sol), which has the functions and events that the standard bridge needs.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.9;

import { IERC20 } from ""@openzeppelin/contracts/token/ERC20/IERC20.sol"";
```

[The standard ERC-20 interface](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/IERC20.sol) does not include the `mint` and `burn` functions.
Those methods are not required by [the ERC-20 standard](https://eips.ethereum.org/EIPS/eip-20), which leaves unspecified the mechanisms to create and destroy tokens.

```solidity
import { IERC165 } from ""@openzeppelin/contracts/utils/introspection/IERC165.sol"";
```

[The ERC-165 interface](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/utils/introspection/IERC165.sol) is used to specify what functions a contract provides.
[You can read the standard here](https://eips.ethereum.org/EIPS/eip-165).

```solidity
interface IL2StandardERC20 is IERC20, IERC165 {
    function l1Token() external returns (address);
```

This function provides the address of the L1 token which is bridged to this contract.
Note that we do not have a similar function in the opposite direction.
We need to be able to bridge any L1 token, regardless of whether L2 support was planned when it was implemented or not.

```solidity

    function mint(address _to, uint256 _amount) external;

    function burn(address _from, uint256 _amount) external;

    event Mint(address indexed _account, uint256 _amount);
    event Burn(address indexed _account, uint256 _amount);
}
```

Functions and events to mint (create) and burn (destroy) tokens.
The bridge should be the only entity that can run these functions to ensure the number of tokens is correct (equal to the number of tokens locked on L1).

### L2StandardERC20 {#L2StandardERC20}

[This is our implementation of the `IL2StandardERC20` interface](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/standards/L2StandardERC20.sol).
Unless you need some kind of custom logic, you should use this one.

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.9;

import { ERC20 } from ""@openzeppelin/contracts/token/ERC20/ERC20.sol"";
```

[The OpenZeppelin ERC-20 contract](https://github.com/OpenZeppelin/openzeppelin-contracts/blob/master/contracts/token/ERC20/ERC20.sol).
Optimism does not believe in reinventing the wheel, especially when the wheel is well audited and needs to be trustworthy enough to hold assets.

```solidity
import ""./IL2StandardERC20.sol"";

contract L2StandardERC20 is IL2StandardERC20, ERC20 {
    address public l1Token;
    address public l2Bridge;
```

These are the two additional configuration parameters that we require and ERC-20 normally does not.

```solidity

    /**
     * @param _l2Bridge Address of the L2 standard bridge.
     * @param _l1Token Address of the corresponding L1 token.
     * @param _name ERC20 name.
     * @param _symbol ERC20 symbol.
     */
    constructor(
        address _l2Bridge,
        address _l1Token,
        string memory _name,
        string memory _symbol
    ) ERC20(_name, _symbol) {
        l1Token = _l1Token;
        l2Bridge = _l2Bridge;
    }
```

First call the constructor for the contract we inherit from (`ERC20(_name, _symbol)`) and then set our own variables.

```solidity

    modifier onlyL2Bridge() {
        require(msg.sender == l2Bridge, ""Only L2 Bridge can mint and burn"");
        _;
    }


    // slither-disable-next-line external-function
    function supportsInterface(bytes4 _interfaceId) public pure returns (bool) {
        bytes4 firstSupportedInterface = bytes4(keccak256(""supportsInterface(bytes4)"")); // ERC165
        bytes4 secondSupportedInterface = IL2StandardERC20.l1Token.selector ^
            IL2StandardERC20.mint.selector ^
            IL2StandardERC20.burn.selector;
        return _interfaceId == firstSupportedInterface || _interfaceId == secondSupportedInterface;
    }
```

This is the way [ERC-165](https://eips.ethereum.org/EIPS/eip-165) works.
Every interface is a number of supported functions, and is identified as the [exclusive or](https://en.wikipedia.org/wiki/Exclusive_or) of the [ABI function selectors](https://docs.soliditylang.org/en/v0.8.12/abi-spec.html#function-selector) of those functions.

The L2 bridge uses ERC-165 as a sanity check to make sure that the ERC-20 contract to which it sends assets is an `IL2StandardERC20`.

**Note:** There is nothing to prevent rogue contract from providing false answers to `supportsInterface`, so this is a sanity check mechanism, _not_ a security mechanism.

```solidity
    // slither-disable-next-line external-function
    function mint(address _to, uint256 _amount) public virtual onlyL2Bridge {
        _mint(_to, _amount);

        emit Mint(_to, _amount);
    }

    // slither-disable-next-line external-function
    function burn(address _from, uint256 _amount) public virtual onlyL2Bridge {
        _burn(_from, _amount);

        emit Burn(_from, _amount);
    }
}
```

Only the L2 bridge is allowed to mint and burn assets.

`_mint` and `_burn` are actually defined in the [OpenZeppelin ERC-20 contract](/developers/tutorials/erc20-annotated-code/#the-_mint-and-_burn-functions-_mint-and-_burn).
That contract just doesn't expose them externally, because the conditions to mint and burn tokens are as varied as the number of ways to use ERC-20.

## L2 Bridge Code {#l2-bridge-code}

This is code that runs the bridge on Optimism.
[The source for this contract is here](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L2/messaging/L2StandardBridge.sol).

```solidity
// SPDX-License-Identifier: MIT
pragma solidity ^0.8.9;

/* Interface Imports */
import { IL1StandardBridge } from ""../../L1/messaging/IL1StandardBridge.sol"";
import { IL1ERC20Bridge } from ""../../L1/messaging/IL1ERC20Bridge.sol"";
import { IL2ERC20Bridge } from ""./IL2ERC20Bridge.sol"";
```

The [IL2ERC20Bridge](https://github.com/ethereum-optimism/optimism/blob/develop/packages/contracts/contracts/L2/messaging/IL2ERC20Bridge.sol) interface is very similar to the [L1 equivalent](#IL1ERC20Bridge) we saw above.
There are two significant differences:

1. On L1 you initiate deposits and finalize withdrawals.
   Here you initiate withdrawals and finalize deposits.
2. On L1 it is necessary to distinguish between ETH and ERC-20 tokens.
   On L2 we can use the same functions for both because internally ETH balances on Optimism are handled as an ERC-20 token with the address [0xDeadDeAddeAddEAddeadDEaDDEAdDeaDDeAD0000](https://optimistic.etherscan.io/address/0xDeadDeAddeAddEAddeadDEaDDEAdDeaDDeAD0000).

```solidity
/* Library Imports */
import { ERC165Checker } from ""@openzeppelin/contracts/utils/introspection/ERC165Checker.sol"";
import { CrossDomainEnabled } from ""../../libraries/bridge/CrossDomainEnabled.sol"";
import { Lib_PredeployAddresses } from ""../../libraries/constants/Lib_PredeployAddresses.sol"";

/* Contract Imports */
import { IL2StandardERC20 } from ""../../standards/IL2StandardERC20.sol"";

/**
 * @title L2StandardBridge
 * @dev The L2 Standard bridge is a contract which works together with the L1 Standard bridge to
 * enable ETH and ERC20 transitions between L1 and L2.
 * This contract acts as a minter for new tokens when it hears about deposits into the L1 Standard
 * bridge.
 * This contract also acts as a burner of the tokens intended for withdrawal, informing the L1
 * bridge to release L1 funds.
 */
contract L2StandardBridge is IL2ERC20Bridge, CrossDomainEnabled {
    /********************************
     * External Contract References *
     ********************************/

    address public l1TokenBridge;
```

Keep track of the address of the L1 bridge.
Note that in contrast to the L1 equivalent, here we _need_ this variable.
The address of the L1 bridge is not known in advance.

```solidity

    /***************
     * Constructor *
     ***************/

    /**
     * @param _l2CrossDomainMessenger Cross-domain messenger used by this contract.
     * @param _l1TokenBridge Address of the L1 bridge deployed to the main chain.
     */
    constructor(address _l2CrossDomainMessenger, address _l1TokenBridge)
        CrossDomainEnabled(_l2CrossDomainMessenger)
    {
        l1TokenBridge = _l1TokenBridge;
    }

    /***************
     * Withdrawing *
     ***************/

    /**
     * @inheritdoc IL2ERC20Bridge
     */
    function withdraw(
        address _l2Token,
        uint256 _amount,
        uint32 _l1Gas,
        bytes calldata _data
    ) external virtual {
        _initiateWithdrawal(_l2Token, msg.sender, msg.sender, _amount, _l1Gas, _data);
    }

    /**
     * @inheritdoc IL2ERC20Bridge
     */
    function withdrawTo(
        address _l2Token,
        address _to,
        uint256 _amount,
        uint32 _l1Gas,
        bytes calldata _data
    ) external virtual {
        _initiateWithdrawal(_l2Token, msg.sender, _to, _amount, _l1Gas, _data);
    }
```

These two functions initiate withdrawals.
Note that there is no needs to specify the L1 token address.
L2 tokens are expected to tell us the L1 equivalent's address.

```solidity

    /**
     * @dev Performs the logic for withdrawals by burning the token and informing
     *      the L1 token Gateway of the withdrawal.
     * @param _l2Token Address of L2 token where withdrawal is initiated.
     * @param _from Account to pull the withdrawal from on L2.
     * @param _to Account to give the withdrawal to on L1.
     * @param _amount Amount of the token to withdraw.
     * @param _l1Gas Unused, but included for potential forward compatibility considerations.
     * @param _data Optional data to forward to L1. This data is provided
     *        solely as a convenience for external contracts. Aside from enforcing a maximum
     *        length, these contracts provide no guarantees about its content.
     */
    function _initiateWithdrawal(
        address _l2Token,
        address _from,
        address _to,
        uint256 _amount,
        uint32 _l1Gas,
        bytes calldata _data
    ) internal {
        // When a withdrawal is initiated, we burn the withdrawer's funds to prevent subsequent L2
        // usage
        // slither-disable-next-line reentrancy-events
        IL2StandardERC20(_l2Token).burn(msg.sender, _amount);
```

Notice that we are _not_ relying on the `_from` parameter but on `msg.sender` which is a lot harder to fake (impossible, as far as I know).

```solidity

        // Construct calldata for l1TokenBridge.finalizeERC20Withdrawal(_to, _amount)
        // slither-disable-next-line reentrancy-events
        address l1Token = IL2StandardERC20(_l2Token).l1Token();
        bytes memory message;

        if (_l2Token == Lib_PredeployAddresses.OVM_ETH) {
```

On L1 it is necessary to distinguish between ETH and ERC-20.

```solidity
            message = abi.encodeWithSelector(
                IL1StandardBridge.finalizeETHWithdrawal.selector,
                _from,
                _to,
                _amount,
                _data
            );
        } else {
            message = abi.encodeWithSelector(
                IL1ERC20Bridge.finalizeERC20Withdrawal.selector,
                l1Token,
                _l2Token,
                _from,
                _to,
                _amount,
                _data
            );
        }

        // Send message up to L1 bridge
        // slither-disable-next-line reentrancy-events
        sendCrossDomainMessage(l1TokenBridge, _l1Gas, message);

        // slither-disable-next-line reentrancy-events
        emit WithdrawalInitiated(l1Token, _l2Token, msg.sender, _to, _amount, _data);
    }

    /************************************
     * Cross-chain Function: Depositing *
     ************************************/

    /**
     * @inheritdoc IL2ERC20Bridge
     */
    function finalizeDeposit(
        address _l1Token,
        address _l2Token,
        address _from,
        address _to,
        uint256 _amount,
        bytes calldata _data
```

This function is called by `L1StandardBridge`.

```solidity
    ) external virtual onlyFromCrossDomainAccount(l1TokenBridge) {
```

Make sure the source of the message is legitimate.
This is important because this function calls `_mint` and could be used to give tokens that are not covered by tokens the bridge owns on L1.

```solidity
        // Check the target token is compliant and
        // verify the deposited token on L1 matches the L2 deposited token representation here
        if (
            // slither-disable-next-line reentrancy-events
            ERC165Checker.supportsInterface(_l2Token, 0x1d1d8b63) &&
            _l1Token == IL2StandardERC20(_l2Token).l1Token()
```

Sanity checks:

1. The correct interface is supported
2. The L2 ERC-20 contract's L1 address matches the L1 source of the tokens

```solidity
        ) {
            // When a deposit is finalized, we credit the account on L2 with the same amount of
            // tokens.
            // slither-disable-next-line reentrancy-events
            IL2StandardERC20(_l2Token).mint(_to, _amount);
            // slither-disable-next-line reentrancy-events
            emit DepositFinalized(_l1Token, _l2Token, _from, _to, _amount, _data);
```

If the sanity checks pass, finalize the deposit:

1. Mint the tokens
2. Emit the appropriate event

```solidity
        } else {
            // Either the L2 token which is being deposited-into disagrees about the correct address
            // of its L1 token, or does not support the correct interface.
            // This should only happen if there is a  malicious L2 token, or if a user somehow
            // specified the wrong L2 token address to deposit into.
            // In either case, we stop the process here and construct a withdrawal
            // message so that users can get their funds out in some cases.
            // There is no way to prevent malicious token contracts altogether, but this does limit
            // user error and mitigate some forms of malicious contract behavior.
```

If a user made a detectable error by using the wrong L2 token address, we want to cancel the deposit and return the tokens on L1.
The only way we can do this from L2 is to send a message that will have to wait the fault challenge period, but that is much better for the user than losing the tokens permanently.

```solidity
            bytes memory message = abi.encodeWithSelector(
                IL1ERC20Bridge.finalizeERC20Withdrawal.selector,
                _l1Token,
                _l2Token,
                _to, // switched the _to and _from here to bounce back the deposit to the sender
                _from,
                _amount,
                _data
            );

            // Send message up to L1 bridge
            // slither-disable-next-line reentrancy-events
            sendCrossDomainMessage(l1TokenBridge, 0, message);
            // slither-disable-next-line reentrancy-events
            emit DepositFailed(_l1Token, _l2Token, _from, _to, _amount, _data);
        }
    }
}
```

## Conclusion {#conclusion}

The standard bridge is the most flexible mechanism for asset transfers.
However, because it is so generic it is not always the easiest mechanism to use.
Especially for withdrawals, most users prefer to use [third party bridges](https://www.optimism.io/apps/bridges) that do not wait the challenge period and do not require a Merkle proof to finalize the withdrawal.

These bridges typically work by having assets on L1, which they provide immediately for a small fee (often less than the cost of gas for a standard bridge withdrawal).
When the bridge (or the people running it) anticipates being short on L1 assets it transfers sufficient assets from L2. As these are very big withdrawals, the withdrawal cost is amortized over a large amount and is a much smaller percentage.

Hopefully this article helped you understand more about how layer 2 works, and how to write Solidity code that is clear and secure.
",snowflake
197,sf_bq416,GOOG_BLOCKCHAIN,"Could you retrieve the top three largest USDT transfers on the TRON blockchain by listing the block numbers, source addresses, destination addresses (in TronLink format), and transfer amounts, using the USDT contract address '0xa614f803b6fd780986a42c78ec9c7f77e6ded13c' and the transfer event signature '0xddf252ad1be2c89b69c2b068fc378daa952ba7f163c4a11628f55a4df523b3ef', dividing the raw transfer value by 1,000,000 to convert it into the final USDT amount, and then ordering the results by the largest transferred amounts first?",,"In this guide, you will learn how to work with blockchain data, specifically focusing on address transformations and how to represent TRON blockchain addresses. We will also introduce some key concepts related to JavaScript functions that you may need to apply when working with such data in your queries.

TRON Addresses and Base58 Encoding
TRON addresses use a specialized format that differs from Ethereum addresses, though they are closely related. The conversion of an Ethereum-like address (hexadecimal) to a TRON address (Base58 format) involves several key steps:

Hexadecimal to Byte Array Conversion:
Blockchain addresses are commonly represented in hexadecimal form (prefixed with 0x). These hex strings must be converted into byte arrays for further processing. In JavaScript, this can be done using libraries such as ethers.js.

Checksum Calculation Using SHA-256:
A checksum ensures the integrity of the address. For TRON, the address bytes undergo two rounds of SHA-256 hashing. Only the first 4 bytes of the second hash are retained as the checksum.

Base58 Encoding:
Once the checksum is generated, the byte array (composed of the address bytes followed by the checksum) is encoded using Base58. This encoding method is commonly used in blockchain systems to make the addresses shorter and more readable.

Final Address Format:
After the conversion process, you will get the final TRON address, which can be used in TRON blockchain queries.

JavaScript Functions for Address Conversion
When working with blockchain data in SQL, it's important to understand how JavaScript functions can be used within SQL queries to handle custom transformations like address conversions. In this case, the function hexToTron can be used to convert a standard Ethereum-like hex address into a TRON-compatible format.

Here’s an outline of how this function works:

encode58(buffer): This function converts a byte array into a Base58 string. It uses an alphabet of characters commonly used in TRON and Bitcoin addresses.
sha256(msgBytes): This function computes the SHA-256 hash of the input byte array. It uses the ethers.js library to perform the hashing operation and returns the resulting byte array.
Checksum and Final Address: The function then appends the checksum to the address bytes, and finally encodes everything using Base58 to produce the TRON address.
These steps are essential when querying TRON blockchain data, as addresses must be transformed into the correct format for the blockchain to recognize them.






",snowflake
198,sf_bq226,GOOG_BLOCKCHAIN,"Which sender address, represented as a complete URL on https://cronoscan.com, has been used most frequently on the Cronos blockchain in transactions to non-null 'to_address' fields, within blocks larger than 4096 bytes, since January 1, 2023?",,,snowflake
199,sf_bq016,DEPS_DEV_V1,"Considering only the highest release versions of NPM packages, which dependency (package and its version) appears most frequently among the dependencies of these packages?",,,snowflake
200,sf_bq062,DEPS_DEV_V1,What is the most frequently used license by packages in each system?,,,snowflake
201,sf_bq063,DEPS_DEV_V1,"Find the GitHub URL (with link label 'SOURCE_REPO') of the latest released version of the NPM package that has the highest number of dependencies in its latest released version, excluding packages whose names contain the character '@' and only considering URLs where the link label is 'SOURCE_REPO' and the URL contains 'github.com'.",,,snowflake
202,sf_bq028,DEPS_DEV_V1,"Considering only the latest release versions of NPM package, which packages are the top 8 most popular based on the Github star number, as well as their versions?","WITH HighestReleases AS (
    SELECT
        HR.""Name"",
        HR.""Version""
    FROM (
        SELECT
            ""Name"",
            ""Version"",
            ROW_NUMBER() OVER (
                PARTITION BY ""Name""
                ORDER BY 
                    TO_NUMBER(PARSE_JSON(""VersionInfo""):""Ordinal"") DESC
            ) AS RowNumber
        FROM
            DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONS
        WHERE
            ""System"" = 'NPM'
            AND TO_BOOLEAN(PARSE_JSON(""VersionInfo""):""IsRelease"") = TRUE
    ) AS HR
    WHERE HR.RowNumber = 1
),
PVP AS (
    SELECT
        PVP.""Name"", 
        PVP.""Version"", 
        PVP.""ProjectType"", 
        PVP.""ProjectName""
    FROM
        DEPS_DEV_V1.DEPS_DEV_V1.PACKAGEVERSIONTOPROJECT AS PVP
    JOIN
        HighestReleases AS HR
    ON
        PVP.""Name"" = HR.""Name""
        AND PVP.""Version"" = HR.""Version""
    WHERE
        PVP.""System"" = 'NPM'
        AND PVP.""ProjectType"" = 'GITHUB'
)
SELECT
    PVP.""Name"", 
    PVP.""Version""
FROM
    PVP
JOIN
    DEPS_DEV_V1.DEPS_DEV_V1.PROJECTS AS P
ON
    PVP.""ProjectType"" = P.""Type"" 
    AND PVP.""ProjectName"" = P.""Name""
ORDER BY 
    P.""StarsCount"" DESC
LIMIT 8;
",,snowflake
203,bq022,chicago,"Calculate the minimum and maximum trip duration in minutes (rounded to the nearest whole number), total number of trips, and average fare for each of six equal quantile groups based on trip duration, considering only trips between 0 and 60 minutes.","SELECT
  ROUND(MIN(trip_seconds) / 60, 0) AS min_minutes,
  ROUND(MAX(trip_seconds) / 60, 0) AS max_minutes,
  COUNT(*) AS total_trips,
  AVG(fare) AS average_fare
FROM (
  SELECT
    trip_seconds,
    NTILE(6) OVER (ORDER BY trip_seconds) AS quantile,
    fare
  FROM
    `bigquery-public-data.chicago_taxi_trips.taxi_trips`
  WHERE
    trip_seconds BETWEEN 0 AND 3600
)
GROUP BY
  quantile
ORDER BY
  min_minutes, max_minutes;",,snowflake
204,bq362,chicago,Which three companies had the largest increase in trip numbers between two consecutive months in 2018?,"select company from
            (select *,
            row_number() over(partition by company order by month_o_month_calc desc) as rownum
            from
            (select *,
            num_trips - lag(num_trips) over(partition by company order by month) as month_o_month_calc
                from
                (SELECT 
                company,
                format_date(""%Y-%m"", date_sub((cast(trip_start_timestamp as date)), interval 1 month)) as prev_month,
                format_date(""%Y-%m"", cast(trip_start_timestamp as date)) AS month,
                count(1) AS num_trips
                from `bigquery-public-data.chicago_taxi_trips.taxi_trips`
                where extract(YEAR from trip_start_timestamp) = 2018
                group by company, month, prev_month
                order by company,month)
            order by company, month_o_month_calc desc)
            ) 
        where rownum = 1
        order by month_o_month_calc desc, company 
        limit 3",,snowflake
205,bq363,chicago,"Calculate the total number of trips and average fare (formatted to two decimal places) for ten equal-sized quantile groups. Each quantile group should contain approximately the same number of trips based on their rounded trip duration in minutes (between 1-50 minutes). Display each group's time range formatted as ""XXm to XXm"" (where the numbers are zero-padded to two digits), the total trips count, and the average fare. The time ranges should represent the minimum and maximum duration values within each quantile. Sort the results chronologically by time range. Use NTILE(10) to create the quantiles from the ordered trip durations.","SELECT
  FORMAT('%02.0fm to %02.0fm', min_minutes, max_minutes) AS minutes_range,
  SUM(trips) AS total_trips,
  FORMAT('%3.2f', SUM(total_fare) / SUM(trips)) AS average_fare
FROM (
  SELECT
    MIN(duration_in_minutes) OVER (quantiles) AS min_minutes,
    MAX(duration_in_minutes) OVER (quantiles) AS max_minutes,
    SUM(trips) AS trips,
    SUM(total_fare) AS total_fare
  FROM (
    SELECT
      ROUND(trip_seconds / 60) AS duration_in_minutes,
      NTILE(10) OVER (ORDER BY trip_seconds / 60) AS quantile,
      COUNT(1) AS trips,
      SUM(fare) AS total_fare
    FROM
      `bigquery-public-data.chicago_taxi_trips.taxi_trips`
    WHERE
      ROUND(trip_seconds / 60) BETWEEN 1 AND 50
    GROUP BY
      trip_seconds,
      duration_in_minutes )
  GROUP BY
    duration_in_minutes,
    quantile
  WINDOW quantiles AS (PARTITION BY quantile)
  )
GROUP BY
  minutes_range
ORDER BY
  Minutes_range",,snowflake
206,bq076,chicago,What is the highest number of motor vehicle theft incidents that occurred in any single month during 2016?,"
SELECT
  incidents AS highest_monthly_thefts
FROM (
  SELECT
    year,
    EXTRACT(MONTH FROM date) AS month,
    COUNT(1) AS incidents,
    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking
  FROM
    `bigquery-public-data.chicago_crime.crime`
  WHERE
    primary_type = 'MOTOR VEHICLE THEFT'
    AND year = 2016
  GROUP BY
    year,
    month
)
WHERE
  ranking = 1
ORDER BY
  year DESC
LIMIT 1;",,snowflake
207,bq077,chicago,"For each year from 2010 to 2016, what is the highest number of motor thefts in one month?","SELECT
  year,
  incidents
FROM (
  SELECT
    year,
    EXTRACT(MONTH
    FROM
      date) AS month,
    COUNT(1) AS incidents,
    RANK() OVER (PARTITION BY year ORDER BY COUNT(1) DESC) AS ranking
  FROM
    `bigquery-public-data.chicago_crime.crime`
  WHERE
    primary_type = 'MOTOR VEHICLE THEFT'
    AND year BETWEEN 2010 AND 2016
  GROUP BY
    year,
    month )
WHERE
  ranking = 1
ORDER BY
  year ASC",,snowflake
208,sf_bq350,OPEN_TARGETS_PLATFORM_1,"For the detailed molecule data, Please display the drug id, drug type and withdrawal status for approved drugs with a black box warning and known drug type among 'Keytruda', 'Vioxx', 'Premarin', and 'Humira'",,,snowflake
209,sf_bq379,OPEN_TARGETS_PLATFORM_1,Which target approved symbol has the overall association score closest to the mean score for psoriasis?,,,snowflake
210,sf_bq078,OPEN_TARGETS_PLATFORM_2,Retrieve the approved symbol of target genes with the highest overall score that are associated with the disease 'EFO_0000676' from the data source 'IMPC'.,,,snowflake
211,sf_bq095,OPEN_TARGETS_PLATFORM_1,"Generate a list of drugs from the table containing molecular details that have completed clinical trials for pancreatic endocrine carcinoma, disease ID EFO_0007416. Please include each drug's name, the target approved symbol, and links to the relevant clinical trials.",,,snowflake
212,bq109,open_targets_genetics_1,"Find the average, variance, max-min difference, and the QTL source(right study) of the maximum log2(h4/h3) for data where right gene id is ""ENSG00000169174"", h4 > 0.8, h3 < 0.02, reported trait includes ""lesterol levels"", right biological feature is ""IPSC"", and the variant is '1_55029009_C_T'.","WITH coloc_stats AS (
  SELECT
    coloc.coloc_log2_h4_h3,
    coloc.right_study AS qtl_source
  FROM
    `open-targets-genetics.genetics.variant_disease_coloc` AS coloc
  JOIN
    `open-targets-genetics.genetics.studies` AS studies
  ON
    coloc.left_study = studies.study_id
  WHERE
    coloc.right_gene_id = ""ENSG00000169174""
    AND coloc.coloc_h4 > 0.8
    AND coloc.coloc_h3 < 0.02
    AND studies.trait_reported LIKE ""%lesterol levels%""
    AND coloc.right_bio_feature = 'IPSC'
    AND CONCAT(coloc.left_chrom, '_', coloc.left_pos, '_', coloc.left_ref, '_', coloc.left_alt) = '1_55029009_C_T'
),
max_value AS (
  SELECT
    MAX(coloc_log2_h4_h3) AS max_log2_h4_h3
  FROM
    coloc_stats
)

SELECT
  AVG(coloc_log2_h4_h3) AS average,
  VAR_SAMP(coloc_log2_h4_h3) AS variance,
  MAX(coloc_log2_h4_h3) - MIN(coloc_log2_h4_h3) AS max_min_difference,
  (SELECT qtl_source FROM coloc_stats WHERE coloc_log2_h4_h3 = (SELECT max_log2_h4_h3 FROM max_value)) AS qtl_source_of_max
FROM
  coloc_stats;",,snowflake
213,sf_bq325,OPEN_TARGETS_GENETICS_2,"Please identify the top 10 genes with the strongest associations across all studies by first selecting, for each gene within each study, the variant with the lowest p-value, and then ranking all such gene–variant pairs to return the 10 genes with the smallest p-values overall.",,,snowflake
214,bq090,CYMBAL_INVESTMENTS,How much higher the average intrinsic value is for trades using the feeling-lucky strategy compared to those using the momentum strategy under long-side trades?,"WITH MomentumTrades AS (
  SELECT
    StrikePrice - LastPx AS priceDifference
  FROM
    `bigquery-public-data.cymbal_investments.trade_capture_report`
  WHERE
    SUBSTR(TargetCompID, 0, 4) = 'MOMO'
    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'
),

FeelingLuckyTrades AS (
  SELECT
    StrikePrice - LastPx AS priceDifference
  FROM
    `bigquery-public-data.cymbal_investments.trade_capture_report`
  WHERE
    SUBSTR(TargetCompID, 0, 4) = 'LUCK'
    AND (SELECT Side FROM UNNEST(Sides)) = 'LONG'
)

SELECT
  AVG(FeelingLuckyTrades.priceDifference) - AVG(MomentumTrades.priceDifference) AS averageDifference 
FROM
  MomentumTrades,
  FeelingLuckyTrades",,snowflake
215,bq442,CYMBAL_INVESTMENTS,Please collect the information of the top 6 trade report with the highest closing prices. Refer to the document for all the information I want.,"SELECT
  OrderID AS tradeID,
  MaturityDate AS tradeTimestamp,
  (
    CASE SUBSTR(TargetCompID, 0, 4)
      WHEN 'MOMO' THEN 'Momentum'
      WHEN 'LUCK' THEN 'Feeling Lucky'
      WHEN 'PRED' THEN 'Prediction'
  END
    ) AS algorithm,
  Symbol AS symbol,
  LastPx AS openPrice,
  StrikePrice AS closePrice,
  (
  SELECT
    Side
  FROM
    UNNEST(Sides)
  ) AS tradeDirection,
  (CASE (
    SELECT
      Side
    FROM
      UNNEST(Sides))
      WHEN 'SHORT' THEN -1
      WHEN 'LONG' THEN 1
  END
    ) AS tradeMultiplier
FROM
  `bigquery-public-data.cymbal_investments.trade_capture_report`cv
ORDER BY closePrice DESC
LIMIT 6","## Trade Capture Report Data List

Below is a detailed description of each extracted field:

### Extracted Data Fields



1. **`tradeID`**:

   \- Represents the unique identifier for each order in the dataset.

   \- **Type**: STRING



2. **`tradeTimestamp` **:

   \- Indicates the maturity date when the trade is due.

   \- **Type**: TIMESTAMP



3. **`algorithm`**:

   \- Deduces the algorithm used for the trade based on the first four characters of the `TargetCompID`:

​     \- 'MOMO' mapped to 'Momentum'

​     \- 'LUCK' mapped to 'Feeling Lucky'

​     \- 'PRED' mapped to 'Prediction'

   \- **Type**: STRING



4. **`symbol`**:

   \- The trading symbol of the financial instrument involved in the trade.

   \- **Type**: STRING



5. **`openPrice` **:

   \- The last price at which the trade was executed, considered as the opening price for analysis purposes.

   \- **Type**: FLOAT



6. **`closePrice` **:

   \- Represents the strike price of the option for the trade, considered here as the closing price for analysis.

   \- **Type**: FLOAT



7. **`tradeDirection`**:

   \- Extracted from a nested array column `Sides` using `UNNEST`. It signifies the direction of the trade:

​     \- Possible values include 'SHORT' or 'LONG'.

   \- **Type**: STRING



8. **`tradeMultiplier`**:

   \- Derived from the trade direction:

​     \- 'SHORT' results in a value of `-1`

​     \- 'LONG' results in a value of `1`

   \- **Type**: INTEGER",snowflake
216,bq079,usfs_fia,"Considering only the latest evaluation group per state for the 'EXPCURR' evaluation type, determine which state has the highest total acreage of timberland and which has the highest total acreage of forestland. For timberland, include plots where the condition status code is 1, the reserved status code is 0, and the site productivity class code is between 1 and 6. For forestland, include plots where the condition status code is 1. Calculate the total acres by summing the adjusted expansion factors for macroplots and subplots, using their respective proportion bases ('MACR' for macroplots and 'SUBP' for subplots) and adjustment factors when greater than zero. For each category (timberland and forestland), provide the state code, evaluation group, state name, and the total acres for the state with the highest total acreage, considering only the latest evaluation group per state.",,,snowflake
217,bq024,usfs_fia,"For the year 2012, which top 10 evaluation groups have the largest subplot acres when considering only the condition with the largest subplot acres within each group? Please include the evaluation group, evaluation type, condition status code, evaluation description, state code, macroplot acres, and subplot acres.",,,snowflake
218,bq220,usfs_fia,"Based on the condition, plot_tree, and population tables in bigquery-public-data.usfs_fia, for the evaluation_type set to 'EXPCURR' and condition_status_code equal to 1, which states had the largest average subplot size and the largest average macroplot size, respectively, for each of the years 2015, 2016, and 2017? Please include the type of plot (subplot or macroplot), the specific year, the state, and the corresponding average size in your results.",,"# Subplot Size and Macroplot Size Calculation Methods

## Overview

In forest inventory analysis, different types of plots are used to estimate forest characteristics. Two commonly used plot sizes are **subplot** and **macroplot**, each serving a distinct purpose for data collection at different scales.

This document provides a detailed explanation of the **subplot size** and **macroplot size** calculation methods using mathematical formulas.

## Subplot Size Calculation

### Definition
A **subplot** is a smaller area within a larger plot, often used to collect more detailed forest data. Subplots are utilized when fine-grained measurements are needed for specific forest conditions.

### Calculation Formula
The size of a subplot is calculated as follows:

\[
\text{Subplot Area} = 
\begin{cases} 
E \times P \times A_s, & \text{if proportion basis} = \text{'SUBP'} \text{ and } A_s > 0 \\
0, & \text{otherwise}
\end{cases}
\]

### Explanation of Terms:
- **E** (Expansion Factor): A multiplier used to adjust the size of the subplot to account for its proportion within a larger area.
- **P** (Condition Proportion Unadjusted): Represents the unadjusted proportion of the forest condition within the subplot.
- **A_s** (Adjustment Factor for Subplot): A factor applied to adjust the subplot size based on specific conditions or measurements.

### Interpretation:
If the **proportion basis** is `'SUBP'` and the **adjustment factor for the subplot** \( A_s \) is greater than 0, the subplot size is calculated by multiplying the **expansion factor** \( E \), **condition proportion** \( P \), and the **adjustment factor** \( A_s \):

\[
\text{Subplot Area} = E \times P \times A_s
\]

If these conditions are not met, the subplot size is set to 0.

## Macroplot Size Calculation

### Definition
A **macroplot** is a larger area within a forest inventory used for broader data collection. Macroplots cover more extensive areas compared to subplots and are used to estimate forest characteristics applicable to large sections of the forest.

### Calculation Formula
The size of a macroplot is calculated as follows:

\[
\text{Macroplot Area} = 
\begin{cases} 
E \times P \times A_m, & \text{if proportion basis} = \text{'MACR'} \text{ and } A_m > 0 \\
0, & \text{otherwise}
\end{cases}
\]

### Explanation of Terms:
- **E** (Expansion Factor): A factor that adjusts the macroplot size to reflect its relative size within the larger forest area.
- **P** (Condition Proportion Unadjusted): The unadjusted proportion of the forest condition within the macroplot.
- **A_m** (Adjustment Factor for Macroplot): A factor used to adjust the macroplot size based on specific forest conditions or measurements.

### Interpretation:
If the **proportion basis** is `'MACR'` and the **adjustment factor for the macroplot** \( A_m \) is greater than 0, the macroplot size is calculated using the following formula:

\[
\text{Macroplot Area} = E \times P \times A_m
\]

If these conditions are not met, the macroplot size is set to 0.

## Summary of Key Differences

| Parameter                     | Subplot Size Calculation                   | Macroplot Size Calculation                  |
|--------------------------------|--------------------------------------------|---------------------------------------------|
| **Proportion Basis**           | `'SUBP'`                                   | `'MACR'`                                    |
| **Adjustment Factor Applied**  | Adjustment Factor for Subplot \( A_s \)     | Adjustment Factor for Macroplot \( A_m \)   |
| **Applicability**              | Used for fine-grained data collection       | Used for broader, large-scale data collection |

### Common Calculation Components:
Both **subplot** and **macroplot** calculations use:
- **E** (Expansion Factor): Reflects how much area the plot represents in the larger forest.
- **P** (Condition Proportion Unadjusted): The proportion of the forest condition applicable to the plot.

These components are scaled based on whether the condition applies to a subplot or macroplot.
",snowflake
219,bq096,gbif,"Determine which year had the earliest date after January on which more than 10 sightings of Sterna paradisaea were recorded north of 40 degrees latitude. For each year, find the first day after January with over 10 sightings of this species in that region, and identify the year whose earliest such date is the earliest among all years.","WITH tenplus AS (
  SELECT 
    year, 
    EXTRACT(DAYOFYEAR FROM DATE(eventdate)) AS dayofyear, 
    COUNT(*) AS count
  FROM 
    bigquery-public-data.gbif.occurrences
  WHERE 
    eventdate IS NOT NULL 
    AND species = 'Sterna paradisaea' 
    AND decimallatitude > 40.0 
    AND month > 1
  GROUP BY 
    year, 
    eventdate
  HAVING 
    COUNT(*) > 10
)

SELECT 
  year AS year
FROM 
  tenplus
GROUP BY 
  year
ORDER BY 
  MIN(dayofyear)
LIMIT 1;",,snowflake
220,sf_bq276,NOAA_PORTS,"Can you provide a comprehensive list of all ports in region number 6585 that lie within U.S. state boundaries and have been affected by named storms in the North Atlantic basin with wind speeds of at least 35 knots and a Saffir-Simpson classification of at least minimal tropical storm strength, including for each port its name, the state name, the distinct years in which storms occurred, the total count of distinct storms, the distinct storm names, the average storm category, the average wind speed, and the respective geometries for both the port and the tropical storm areas?",,"

# Requirements

The following shows the SQL operations you must perform. Please implement the corresponding functionality in your SQL code according to the description.

## `nautical_miles_conversion`

### Description
Converts nautical miles to statute miles by multiplying the input nautical miles by a fixed conversion factor. This operation is commonly used in geographic analysis to convert marine-based distances into land-based measurements, which are more widely used in logistics and mapping.

### SQL Definition
```sql
nautical_miles_conversion(input_nautical_miles FLOAT64)
AS (
  input_nautical_miles * 1.15078
);
```

## `azimuth_to_geog_point`

### Description
Calculates a geographic point based on input latitude and longitude, an azimuth, and a distance. This operation is particularly useful for spatial analyses that require generating new locations based on directional and distance parameters from a given point.

### Mathematical Operation
Employs trigonometric calculations to determine new geographic coordinates, accounting for Earth's curvature. The operation adjusts direction (azimuth) and distance from degrees and nautical miles to radians and kilometers respectively.

### SQL Definition
```sql
SELECT
  ST_MAKEPOINT(
    57.2958 * (
      input_lon * (3.14159 / 180) + ATAN2(
        SIN(azimuth * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(input_lat * (3.14159 / 180)),
        COS(distance * 1.61 / 6378.1) - SIN(input_lat * (3.14159 / 180)) * SIN(
          57.2958 * ASIN(
            SIN(input_lat * (3.14159 / 180)) * COS(distance * 1.61 / 6378.1) +
            COS(input_lat * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(azimuth * (3.14159 / 180))
          )
        )
      )
    ),
    57.2958 * ASIN(
      SIN(input_lat * (3.14159 / 180)) * COS(distance * 1.61 / 6378.1) +
      COS(input_lat * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(azimuth * (3.14159 / 180))
    )
  ) AS destination_point
FROM
  your_table;
```




# Functions

You should use the following built-in Snowflake functions.


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+




## ST_MAKELINE

Constructs a GEOGRAPHY or GEOMETRY object that represents a line connecting the points in the input objects.

See also:TO_GEOGRAPHY , TO_GEOMETRY


## Syntax

ST_MAKELINE( <geography_expression_1> , <geography_expression_2> )

ST_MAKELINE( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geography_expression_2A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_1A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_2A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY. The value is a LineString that connects all of the points specified by the input GEOGRAPHY or GEOMETRY objects.

## Usage notes


If an input GEOGRAPHY object contains multiple points, ST_MAKELINE connects all of the points specified in the object.
ST_MAKELINE connects the points in the order in which they are specified in the input.

For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

The examples in this section display output in WKT format:

alter session set GEOGRAPHY_OUTPUT_FORMAT='WKT';


The following example uses ST_MAKELINE to construct a LineString that connects two Points:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(37.0 45.0)'),
                   TO_GEOGRAPHY('POINT(38.5 46.5)')
                  ) AS line_between_two_points;
+-----------------------------+
| LINE_BETWEEN_TWO_POINTS     |
|-----------------------------|
| LINESTRING(37 45,38.5 46.5) |
+-----------------------------+


The following example constructs a LineString that connects a Point with the points in a MultiPoint:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(-122.306067 37.55412)'),
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))')
                  ) AS line_between_point_and_multipoint;
+-----------------------------------------------------------------------------+
| LINE_BETWEEN_POINT_AND_MULTIPOINT                                           |
|-----------------------------------------------------------------------------|
| LINESTRING(-122.306067 37.55412,-122.32328 37.561801,-122.325879 37.586852) |
+-----------------------------------------------------------------------------+


As demonstrated by the output of the example, ST_MAKELINE connects the points in the order in which they are specified in the input.
The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))'),
                   TO_GEOGRAPHY('LINESTRING(-122.306067 37.55412, -122.496691 37.495627)')
                  ) AS line_between_multipoint_and_linestring;
+---------------------------------------------------------------------------------------------------+
| LINE_BETWEEN_MULTIPOINT_AND_LINESTRING                                                            |
|---------------------------------------------------------------------------------------------------|
| LINESTRING(-122.32328 37.561801,-122.325879 37.586852,-122.306067 37.55412,-122.496691 37.495627) |
+---------------------------------------------------------------------------------------------------+



## GEOMETRY examples

The examples in this section display output in WKT format:

ALTER SESSION SET GEOMETRY_OUTPUT_FORMAT='WKT';


The first example constructs a line between two Points:

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('POINT(3.5 4.5)')) AS line_between_two_points;

+-------------------------+
| LINE_BETWEEN_TWO_POINTS |
|-------------------------|
| LINESTRING(1 2,3.5 4.5) |
+-------------------------+


The next example demonstrates creating a LineString that connects points in a MultiPoint with a Point

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_point_and_multipoint;

+---------------------------------+
| LINE_FROM_POINT_AND_MULTIPOINT  |
|---------------------------------|
| LINESTRING(1 2,3.5 4.5,6.1 7.9) |
+---------------------------------+


The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
  TO_GEOMETRY('LINESTRING(1.0 2.0, 10.1 5.5)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_linestring_and_multipoint;

+------------------------------------------+
| LINE_FROM_LINESTRING_AND_MULTIPOINT      |
|------------------------------------------|
| LINESTRING(1 2,10.1 5.5,3.5 4.5,6.1 7.9) |
+------------------------------------------+







## ST_MAKEPOLYGON , ST_POLYGON

Constructs a GEOGRAPHY or GEOMETRY object that represents a Polygon without holes. The function uses the specified LineString as the outer loop.
This function corrects the orientation of the loop to prevent the creation of Polygons that span more than half of the globe. In contrast, ST_MAKEPOLYGONORIENTED does not attempt to correct the orientation of the loop.

See also:TO_GEOGRAPHY , TO_GEOMETRY , ST_MAKEPOLYGONORIENTED


## Syntax

ST_MAKEPOLYGON( <geography_or_geometry_expression> )


## Arguments


geography_or_geometry_expressionA GEOGRAPHY or GEOMETRY object that represents a LineString in which the last point is the same as the first (i.e. a loop).


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY.

## Usage notes


The lines of the Polygon must form a loop. In other words, the last Point in the sequence of Points defining the LineString must be the same Point as the first Point in the sequence.
ST_POLYGON is an alias for ST_MAKEPOLYGON.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_MAKEPOLYGON function. The sequence of points below defines a geodesic rectangular area 1 degree wide and 2 degrees high, with the lower left corner of the polygon starting at the equator (latitude) and Greenwich (longitude). The last point in the sequence is the same as the first point,
which completes the loop.

SELECT ST_MAKEPOLYGON(
   TO_GEOGRAPHY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
   ) AS polygon1;
+--------------------------------+
| POLYGON1                       |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_MAKEPOLYGON function.

SELECT ST_MAKEPOLYGON(
  TO_GEOMETRY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
  ) AS polygon;

+--------------------------------+
| POLYGON                        |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+",snowflake
221,bq277,noaa_ports,"Which single port, listed under region number '6585', is located within a U.S. state boundary and appears most frequently inside the geographic areas of named tropical storms with wind speeds of at least 35 knots in the North Atlantic basin, excluding those labeled 'NOT_NAMED'?",,"

# Requirements

The following shows the SQL operations you must perform. Please implement the corresponding functionality in your SQL code according to the description.

## `nautical_miles_conversion`

### Description
Converts nautical miles to statute miles by multiplying the input nautical miles by a fixed conversion factor. This operation is commonly used in geographic analysis to convert marine-based distances into land-based measurements, which are more widely used in logistics and mapping.

### SQL Definition
```sql
nautical_miles_conversion(input_nautical_miles FLOAT64)
AS (
  input_nautical_miles * 1.15078
);
```

## `azimuth_to_geog_point`

### Description
Calculates a geographic point based on input latitude and longitude, an azimuth, and a distance. This operation is particularly useful for spatial analyses that require generating new locations based on directional and distance parameters from a given point.

### Mathematical Operation
Employs trigonometric calculations to determine new geographic coordinates, accounting for Earth's curvature. The operation adjusts direction (azimuth) and distance from degrees and nautical miles to radians and kilometers respectively.

### SQL Definition
```sql
SELECT
  ST_MAKEPOINT(
    57.2958 * (
      input_lon * (3.14159 / 180) + ATAN2(
        SIN(azimuth * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(input_lat * (3.14159 / 180)),
        COS(distance * 1.61 / 6378.1) - SIN(input_lat * (3.14159 / 180)) * SIN(
          57.2958 * ASIN(
            SIN(input_lat * (3.14159 / 180)) * COS(distance * 1.61 / 6378.1) +
            COS(input_lat * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(azimuth * (3.14159 / 180))
          )
        )
      )
    ),
    57.2958 * ASIN(
      SIN(input_lat * (3.14159 / 180)) * COS(distance * 1.61 / 6378.1) +
      COS(input_lat * (3.14159 / 180)) * SIN(distance * 1.61 / 6378.1) * COS(azimuth * (3.14159 / 180))
    )
  ) AS destination_point
FROM
  your_table;
```




# Functions

You should use the following built-in Snowflake functions.


## ST_WITHIN

Returns true if the first geospatial object is fully contained by the second geospatial object. In other words:

The first GEOGRAPHY object g1 is fully contained by the second GEOGRAPHY object g2.
The first GEOMETRY object g1 is fully contained by the second GEOMETRY object g2.

Calling ST_WITHIN(g1, g2) is equivalent to calling ST_CONTAINS(g2, g1).
Although ST_COVEREDBY and ST_WITHIN might seem similar, the two functions have subtle differences. For details on the differences between “covered by” and “within”, see the Dimensionally Extended 9-Intersection Model (DE-9IM).

Note This function does not support using a GeometryCollection or FeatureCollection as input values.

Tip You can use the search optimization service to improve the performance of queries that call this function.
For details, see Search Optimization Service.

See also:ST_CONTAINS , ST_COVEREDBY


## Syntax

ST_WITHIN( <geography_expression_1> , <geography_expression_2> )

ST_WITHIN( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geography_expression_2A GEOGRAPHY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_1A GEOMETRY object that is not a GeometryCollection or FeatureCollection.

geometry_expression_2A GEOMETRY object that is not a GeometryCollection or FeatureCollection.


## Returns

BOOLEAN.

## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_WITHIN function:

create table geospatial_table_01 (g1 GEOGRAPHY, g2 GEOGRAPHY);
insert into geospatial_table_01 (g1, g2) values 
    ('POLYGON((0 0, 3 0, 3 3, 0 3, 0 0))', 'POLYGON((1 1, 2 1, 2 2, 1 2, 1 1))');

Copy SELECT ST_WITHIN(g1, g2) 
    FROM geospatial_table_01;
+-------------------+
| ST_WITHIN(G1, G2) |
|-------------------|
| False             |
+-------------------+




## ST_MAKELINE

Constructs a GEOGRAPHY or GEOMETRY object that represents a line connecting the points in the input objects.

See also:TO_GEOGRAPHY , TO_GEOMETRY


## Syntax

ST_MAKELINE( <geography_expression_1> , <geography_expression_2> )

ST_MAKELINE( <geometry_expression_1> , <geometry_expression_2> )


## Arguments


geography_expression_1A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geography_expression_2A GEOGRAPHY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_1A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.

geometry_expression_2A GEOMETRY object containing the points to connect. This object must be a Point, MultiPoint, or LineString.


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY. The value is a LineString that connects all of the points specified by the input GEOGRAPHY or GEOMETRY objects.

## Usage notes


If an input GEOGRAPHY object contains multiple points, ST_MAKELINE connects all of the points specified in the object.
ST_MAKELINE connects the points in the order in which they are specified in the input.

For GEOMETRY objects, the function reports an error if the two input GEOMETRY objects have different SRIDs.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

The examples in this section display output in WKT format:

alter session set GEOGRAPHY_OUTPUT_FORMAT='WKT';


The following example uses ST_MAKELINE to construct a LineString that connects two Points:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(37.0 45.0)'),
                   TO_GEOGRAPHY('POINT(38.5 46.5)')
                  ) AS line_between_two_points;
+-----------------------------+
| LINE_BETWEEN_TWO_POINTS     |
|-----------------------------|
| LINESTRING(37 45,38.5 46.5) |
+-----------------------------+


The following example constructs a LineString that connects a Point with the points in a MultiPoint:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('POINT(-122.306067 37.55412)'),
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))')
                  ) AS line_between_point_and_multipoint;
+-----------------------------------------------------------------------------+
| LINE_BETWEEN_POINT_AND_MULTIPOINT                                           |
|-----------------------------------------------------------------------------|
| LINESTRING(-122.306067 37.55412,-122.32328 37.561801,-122.325879 37.586852) |
+-----------------------------------------------------------------------------+


As demonstrated by the output of the example, ST_MAKELINE connects the points in the order in which they are specified in the input.
The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
                   TO_GEOGRAPHY('MULTIPOINT((-122.32328 37.561801), (-122.325879 37.586852))'),
                   TO_GEOGRAPHY('LINESTRING(-122.306067 37.55412, -122.496691 37.495627)')
                  ) AS line_between_multipoint_and_linestring;
+---------------------------------------------------------------------------------------------------+
| LINE_BETWEEN_MULTIPOINT_AND_LINESTRING                                                            |
|---------------------------------------------------------------------------------------------------|
| LINESTRING(-122.32328 37.561801,-122.325879 37.586852,-122.306067 37.55412,-122.496691 37.495627) |
+---------------------------------------------------------------------------------------------------+



## GEOMETRY examples

The examples in this section display output in WKT format:

ALTER SESSION SET GEOMETRY_OUTPUT_FORMAT='WKT';


The first example constructs a line between two Points:

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('POINT(3.5 4.5)')) AS line_between_two_points;

+-------------------------+
| LINE_BETWEEN_TWO_POINTS |
|-------------------------|
| LINESTRING(1 2,3.5 4.5) |
+-------------------------+


The next example demonstrates creating a LineString that connects points in a MultiPoint with a Point

SELECT ST_MAKELINE(
  TO_GEOMETRY('POINT(1.0 2.0)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_point_and_multipoint;

+---------------------------------+
| LINE_FROM_POINT_AND_MULTIPOINT  |
|---------------------------------|
| LINESTRING(1 2,3.5 4.5,6.1 7.9) |
+---------------------------------+


The following example constructs a LineString that connects the points in a MultiPoint with another LineString:

SELECT ST_MAKELINE(
  TO_GEOMETRY('LINESTRING(1.0 2.0, 10.1 5.5)'),
  TO_GEOMETRY('MULTIPOINT(3.5 4.5, 6.1 7.9)')) AS line_from_linestring_and_multipoint;

+------------------------------------------+
| LINE_FROM_LINESTRING_AND_MULTIPOINT      |
|------------------------------------------|
| LINESTRING(1 2,10.1 5.5,3.5 4.5,6.1 7.9) |
+------------------------------------------+







## ST_MAKEPOLYGON , ST_POLYGON

Constructs a GEOGRAPHY or GEOMETRY object that represents a Polygon without holes. The function uses the specified LineString as the outer loop.
This function corrects the orientation of the loop to prevent the creation of Polygons that span more than half of the globe. In contrast, ST_MAKEPOLYGONORIENTED does not attempt to correct the orientation of the loop.

See also:TO_GEOGRAPHY , TO_GEOMETRY , ST_MAKEPOLYGONORIENTED


## Syntax

ST_MAKEPOLYGON( <geography_or_geometry_expression> )


## Arguments


geography_or_geometry_expressionA GEOGRAPHY or GEOMETRY object that represents a LineString in which the last point is the same as the first (i.e. a loop).


## Returns

The function returns a value of type GEOGRAPHY or GEOMETRY.

## Usage notes


The lines of the Polygon must form a loop. In other words, the last Point in the sequence of Points defining the LineString must be the same Point as the first Point in the sequence.
ST_POLYGON is an alias for ST_MAKEPOLYGON.

For GEOMETRY objects, the returned GEOMETRY object has the same SRID as the input.


## Examples


## GEOGRAPHY examples

This shows a simple use of the ST_MAKEPOLYGON function. The sequence of points below defines a geodesic rectangular area 1 degree wide and 2 degrees high, with the lower left corner of the polygon starting at the equator (latitude) and Greenwich (longitude). The last point in the sequence is the same as the first point,
which completes the loop.

SELECT ST_MAKEPOLYGON(
   TO_GEOGRAPHY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
   ) AS polygon1;
+--------------------------------+
| POLYGON1                       |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+



## GEOMETRY examples

This shows a simple use of the ST_MAKEPOLYGON function.

SELECT ST_MAKEPOLYGON(
  TO_GEOMETRY('LINESTRING(0.0 0.0, 1.0 0.0, 1.0 2.0, 0.0 2.0, 0.0 0.0)')
  ) AS polygon;

+--------------------------------+
| POLYGON                        |
|--------------------------------|
| POLYGON((0 0,1 0,1 2,0 2,0 0)) |
+--------------------------------+",snowflake
222,bq278,sunroof_solar,"Please provide a detailed comparison of the solar potential for each state, distinguishing between postal code and census tract levels. For each state, include the total number of buildings available for solar installations, the average percentage of Google Maps area covered by Project Sunroof, the average percentage of that coverage which is suitable for solar, the total potential panel count, the total kilowatt capacity, the energy generation potential, the carbon dioxide offset, the current number of buildings with solar panels, and the gap in potential installations calculated by adjusting the total qualified buildings with the coverage and suitability percentages and subtracting the current installations.",,,snowflake
223,bq102,gnomAD,"Identify which start positions are associated with missense variants in the BRCA1 gene on chromosome 17, where the reference base is 'C' and the alternate base is 'T'. Using data from the gnomAD v2.1.1 version.","WITH gene_region AS (
  SELECT 
    MIN(start_position) AS start_pos, 
    MAX(end_position) AS end_pos
  FROM `bigquery-public-data.gnomAD.v2_1_1_genomes__chr17` AS main_table
  WHERE EXISTS (
    SELECT 1 
    FROM UNNEST(main_table.alternate_bases) AS alternate_bases
    WHERE EXISTS (
      SELECT 1 
      FROM UNNEST(alternate_bases.vep) AS vep
      WHERE vep.SYMBOL = 'BRCA1'
    )
  )
)


SELECT 
  DISTINCT start_position
FROM `bigquery-public-data.gnomAD.v2_1_1_genomes__chr17` AS main_table,
     UNNEST(main_table.alternate_bases) AS alternate_bases,
     UNNEST(alternate_bases.vep) AS vep,
     gene_region
WHERE main_table.start_position >= gene_region.start_pos
  AND main_table.start_position <= gene_region.end_pos
  AND REGEXP_CONTAINS(vep.Consequence, r""missense_variant"")
  AND reference_bases = ""C""
  AND alternate_bases.alt = ""T""
",,snowflake
224,bq445,gnomAD,"Using the gnomAD v2.1.1 genomes data for chromosome 17, determine the smallest start position and largest end position of any variant whose nested VEP annotations contain the symbol 'BRCA1'. Then, for all variants whose positions fall within that gene region, retrieve the 'Protein_position' values only if the 'Consequence' includes 'missense_variant', sort them in ascending order by 'Protein_position', and finally output the first such result.",,,snowflake
225,bq103,gnomAD,"Generate summary statistics on genetic variants in the region between positions 55039447 and 55064852 on chromosome 1. This includes the number of variants, the total allele count, the total number of alleles, and distinct gene symbols (using Variant Effect Predictor, VEP, for gene annotation). Additionally, compute the density of mutations by dividing the length of the region by the number of variants.  Using data from the gnomAD v3 version.","WITH summary_stats AS (
  SELECT
    COUNT(1) AS num_variants,
    SUM((SELECT alt.AC FROM UNNEST(alternate_bases) AS alt)) AS sum_AC,
    SUM(AN) AS sum_AN,
    -- Also include some information from Variant Effect Predictor (VEP).
    STRING_AGG(DISTINCT (SELECT annot.symbol FROM UNNEST(alternate_bases) AS alt,
                                               UNNEST(vep) AS annot LIMIT 1), ', ') AS genes
  FROM bigquery-public-data.gnomAD.v3_genomes__chr1 AS main_table
  WHERE start_position >= 55039447 AND start_position <= 55064852
)
SELECT
  ROUND((55064852 - 55039447) / num_variants, 3) AS burden_of_mutation,
  *
FROM summary_stats;",,snowflake
226,sf_bq104,GOOGLE_TRENDS,"Based on the most recent refresh date, identify the top-ranked rising search term for the week that is exactly one year prior to the latest available week in the dataset.","WITH LatestWeek AS (
    SELECT
        DATEADD(WEEK, -52, MAX(""week"")) AS ""last_year_week""
    FROM
        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS
),
LatestRefreshDate AS (
    SELECT
        MAX(""refresh_date"") AS ""latest_refresh_date""
    FROM
        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS
),
RankedTerms AS (
    SELECT
        ""term"",
        ""week"",
        CASE WHEN ""score"" IS NULL THEN NULL ELSE ""dma_name"" END AS ""dma_name"",
        ""rank"",
        ""score"",
        ROW_NUMBER() OVER (
            PARTITION BY ""term"", ""week""
            ORDER BY ""score"" DESC
        ) AS rn
    FROM
        GOOGLE_TRENDS.GOOGLE_TRENDS.TOP_RISING_TERMS
    WHERE
        ""week"" = (SELECT ""last_year_week"" FROM LatestWeek)
        AND ""refresh_date"" = (SELECT ""latest_refresh_date"" FROM LatestRefreshDate)
)

SELECT
    ""term""
FROM
    RankedTerms
WHERE
    rn = 1
ORDER BY
    ""rank""
LIMIT 1;
",,snowflake
227,sf_bq411,GOOGLE_TRENDS,"Please retrieve the top three Google Trends search terms (ranks 1, 2, and 3) from top_terms for each weekday (Monday through Friday) between September 1, 2024, and September 14, 2024, grouped by the refresh_date column and ordered in descending order of refresh_date.",,,snowflake
228,bq105,nhtsa_traffic_fatalities_plus,"According to the 2015 and 2016 accident and driver distraction, and excluding cases where the driver’s distraction status is recorded as 'Not Distracted,' 'Unknown if Distracted,' or 'Not Reported,' how many traffic accidents per 100,000 people were caused by driver distraction in each U.S. state for those two years, based on 2010 census population data, and which five states each year had the highest rates?","SELECT * FROM
(
SELECT
  '2015' AS year,
  COUNT(a.consecutive_number) AS total,
  a.state_name AS state,
  c.state_pop AS population,
  (COUNT(a.consecutive_number) / c.state_pop * 100000) AS rate_per_100000
FROM
  `bigquery-public-data.nhtsa_traffic_fatalities.accident_2015` a
JOIN
  `bigquery-public-data.nhtsa_traffic_fatalities.distract_2015` b
ON
  a.consecutive_number = b.consecutive_number
JOIN (
  SELECT
    SUM(d.population) AS state_pop,
    e.state_name AS state
  FROM
    `bigquery-public-data.census_bureau_usa.population_by_zip_2010` d
  JOIN
    `bigquery-public-data.utility_us.zipcode_area` e
  ON
    d.zipcode = e.zipcode
  GROUP BY
    state ) c
ON
  c.state = a.state_name
WHERE
  b.driver_distracted_by_name NOT IN ('Not Distracted', 'Unknown if Distracted', 'Not Reported')
GROUP BY
  state,
  population,
  c.state_pop
ORDER BY
  rate_per_100000 DESC
LIMIT 5
)
UNION ALL
(
SELECT
  '2016' AS year,
  COUNT(a.consecutive_number) AS total,
  a.state_name AS state,
  c.state_pop AS population,
  (COUNT(a.consecutive_number) / c.state_pop * 100000) AS rate_per_100000
FROM
  `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016` a
JOIN
  `bigquery-public-data.nhtsa_traffic_fatalities.distract_2016` b
ON
  a.consecutive_number = b.consecutive_number
JOIN (
  SELECT
    SUM(d.population) AS state_pop,
    e.state_name AS state
  FROM
    `bigquery-public-data.census_bureau_usa.population_by_zip_2010` d
  JOIN
    `bigquery-public-data.utility_us.zipcode_area` e
  ON
    d.zipcode = e.zipcode
  GROUP BY
    state ) c
ON
  c.state = a.state_name
WHERE
  b.driver_distracted_by_name NOT IN ('Not Distracted', 'Unknown if Distracted', 'Not Reported')
GROUP BY
  state,
  population,
  c.state_pop
ORDER BY
  rate_per_100000 DESC
LIMIT 5
)",,snowflake
229,bq108,nhtsa_traffic_fatalities,"Within the 2015 dataset for accidents that occurred from January through August and involved more than one distinct person, what percentage of these accidents had more than one individual with a severe injury (injury severity = 4)",,,snowflake
230,bq067,nhtsa_traffic_fatalities,"I want to create a labeled dataset from the National Highway Traffic Safety Administration traffic fatality data that predicts whether a traffic accident involving more than one distinct person results in more than one fatality, where the label is 1 if an accident has more than one person with an injury severity code of 4 (fatal injury) and 0 otherwise. For each accident, include the numeric predictors: state_number, the vehicle body_type, the number_of_drunk_drivers, the day_of_week, the hour_of_crash, and a binary indicator for whether the accident occurred in a work zone (1 if it is not “None,” otherwise 0). Also, engineer a feature for the average absolute difference between travel_speed and speed_limit per accident, only considering travel speeds up to 151 mph (excluding codes 997, 998, 999) and speed limits up to 80 mph (excluding codes 98, 99), and categorize this average speed difference into levels from 0 to 4 in 20 mph increments with lower bounds inclusive and upper bounds exclusive. Finally, only include accidents that involve more than one distinct person.",,"### Overview 

Within the nhtsa_traffic_fatalities dataset there are 40 different tables. Interestingly, each table has a suffix of either _2015 or _2016 denoting the year the events in the table occured in. Essentially, there are then only 20 tables. We decided for this project to use only the tables with the _2016 suffix in order to take into account the most recent data. 

Within all tables there are columns for state_number and consecutive_number. State_number identifies the state in which the crash occured. Consecutive_number is a unique case number that is assigned to each crash.

In general here are each table and the information that the table contains.

* accident_2016 - one record per a crash, contains details on crash characteristics and environmental conditions
* cevent_2016 - one record per event, contains a description of the event or object contacted, vehicles involved, and where the vehicle was impacted
* damage_2016 - one record per damaged area, contains details on the areas on a vehicle that were damaged in the crash
* distract_2016 - at least one record per in-transport motor vehicle, each distraction is a seperate record, contains details on driver distractions
* drimpair_2016 - at least one record for each driver of an in-transport motor vehicle, one record per impairment, contains details on physical impairments of drivers of motor vehicles
* factor_2016 - at least one record per in-transport motor vehicle, each factor is a seperate record, contains details about vehicle circumstances which may have contributed to the crash
* maneuver_2016 - at least one record per in-transport motor vehicle, each maneuver is a seperate record, contains details regarding actions taken by the driver to avoid something/someone in the road
* nmcrash_2016 - at least one record for each person who is not an occupant of a motor vehicle, one record per action, contains details about any contributing circumstances or improper actions of people who are not occupants of motor vehicles
* nmimpair_2016 - at least one record for each person who is not an occupant of a motor vehicle, one record per impairment, contains details about physical impairements of people who are not occupants of motor vehicles
* nmprior_2016 - at least one record for each person who is not an occupant of a motor vehicle, one record per action, contains details about the actions of people who are not occupants of motor vehicles at the time of their involvment in the crash
* parkwork_2016 - one record per parked/working vehicle, contains details about parked or working vehicles involved in FARS (Fatality Analysis Reporting System) crashes
* pbtype_2016 - one record for each pedestrian, bicyclist or person on a personal conveyance, contains details about crashes between motor vehicles and pedestrians, people on personal conveyances and bicyclists.
* person_2016 - one record per person, contains details describing all persons involved in the crash like age, sex, vehicle occupant restraint use, and injury severity
* safetyeq_2016 - at least one record for each person who is not an occupant of a motor vehicle, one record per equipment item, contains details about safety equipment used by people who are not occupants of motor vehicles.
* vehicle_2016 - one record per in-transport motor vehicle, contains details describing the in-transport motor vehicles and the drivers of in-transport motor vehicles who are involved in the crash.
* vevent_2016 - one record for each event for each in-transport motor vehicle, contains details on the sequence of events for each intransport motor vehicle involved in the crash. (Same data elements as Cevent data but also records the sequential event number for each vehicle)
* vindecode_2016 - one record per vehicle, contains details describing a vehicle based on the vehicle's VIN.
* violatn_2016 - at least one record per in-transport motor vehicle, each violation is a seperate record, contains details about violations which were charged to drivers.
* vision_2016 - at least one record per in-transport motor vehicle, each obstruction is a seperate record, contains details about circumstances which may have obscured the driver's vision.
* vsoe_2016 - one record for each event for each in-transport motor vehicle, contains the sequence of events for each intransport motor vehicle involved in the crash. (Simplified Vevent)

### Redundant Data/Tables

There is redundant data. For example, vsoe_2016 is an abridged version of vevent_2016. We can see that both tables have one record for each event for each in-transport motor vehicle and contains details on the sequence of events for each intransport motor vehicle. Furthermore, we can go another layer and see that vevent_2016 has redundant data with cevent_2016 as vevent_2016 has the same data elements but also record the VIN as an identifier for data. 

If we compare vehicle_2016 and accident_2016 we can see many redundant columns such as: timestamp_of_crash, first_harmful_event, first_harmful_event_name, and (day/month/hour/minute)_of_crash, etc. This is interesting as vehicle_2016 contains one record per in-transport motor vehicle and accident_2016 contains one record per crash which may contain multiple in-transport motor vehicles. Therefore, this data could be redundant multiple times. 

Moreover, many tables include the raw timestamp as well as year, month, day, hour, minute columns which is redundant as timestamp tells all this information.

In general we see some redundant data, and we believe the purpose is to make querying easier for the user. So for example, if they want to determine the hour of events they do not need to manually calculate the hour from the timestamp, or if they want information about the first_harmful_event in a specific vehicle now they don't need to join accident_2016 to get the data. 

### Edge Cases

- The dataset we choose only contains accidents in which there is at least 1 fatality;
- When you use `travel_speed`, its value should be constrained because certain codes represent speeds beyond a threshold or unknown values, e.g.,
    - 997 indicates speeds greater than 96
    - 998 indicates speeds greater than 151
    - 999 represents unknown speeds
- When you use `speed_limit`, it should be similarly constrained because certain codes indicate that:
    - the speed limit was not reported (98), or
    - the value is unknown (99)",snowflake
231,bq396,nhtsa_traffic_fatalities,Which top 3 states had the largest differences in the number of traffic accidents between rainy and clear weather during weekends in 2016? Please also provide the respective differences for each state.,"WITH weekend_accidents AS (
    SELECT
        state_name,
        CASE
            WHEN atmospheric_conditions_1_name = 'Rain' THEN 'Rain'
            WHEN atmospheric_conditions_1_name = 'Clear' THEN 'Clear'
            ELSE 'Other'
        END AS Weather_Condition,
        COUNT(DISTINCT consecutive_number) AS num_accidents
    FROM
        `bigquery-public-data.nhtsa_traffic_fatalities.accident_2016`
    WHERE
        EXTRACT(DAYOFWEEK FROM timestamp_of_crash) IN (1, 7)  -- 1 = Sunday, 7 = Saturday
        AND atmospheric_conditions_1_name IN ('Rain', 'Clear')
    GROUP BY
        state_name, Weather_Condition
),

weather_difference AS (
    SELECT
        state_name,
        MAX(CASE WHEN Weather_Condition = 'Rain' THEN num_accidents ELSE 0 END) AS Rain_Accidents,
        MAX(CASE WHEN Weather_Condition = 'Clear' THEN num_accidents ELSE 0 END) AS Clear_Accidents,
        ABS(MAX(CASE WHEN Weather_Condition = 'Rain' THEN num_accidents ELSE 0 END) -
            MAX(CASE WHEN Weather_Condition = 'Clear' THEN num_accidents ELSE 0 END)) AS Difference
    FROM
        weekend_accidents
    GROUP BY
        state_name
)

SELECT
    state_name,
    Difference
FROM
    weather_difference
ORDER BY
    Difference DESC
LIMIT 3;",,snowflake
232,bq441,nhtsa_traffic_fatalities,"Please help me compile the critical details on traffic accidents in 2015, as listed in the info document.",,"## Traffic Fatalities Info List 2015

### 1. `consecutive_number`

- Unique identifier for each accident.

### 2. `county`

- The county where the accident occurred.

### 3. `type_of_intersection`

- Describes the type of intersection where the accident took place.

### 4. `light_condition`

- The lighting condition during the accident, for example, daylight, dawn, dusk, etc.

### 5. `atmospheric_conditions_1`

- Describes the weather conditions at the time of the accident.

### 6. `hour_of_crash`

- The hour during which the accident occurred.

### 7. `functional_system`

- A classification of the roadway based on function, such as arterial, collector, etc.

### 8. `related_factors` (alias for `related_factors_crash_level_1`)

- Factors related to the crash at a macro level.

### 9. `delay_to_hospital`

- Calculated as the time delay between the accident and EMS arrival at the hospital.

  - Formula: `hour_of_ems_arrival_at_hospital - hour_of_crash`

  - Conditions:

​    - Valid when `hour_of_ems_arrival_at_hospital` is between 0 and 23.

​    - Otherwise `NULL`.

### 10. `delay_to_scene`

- Calculated as the time delay between the accident and arrival at the scene.

  - Formula: `hour_of_arrival_at_scene - hour_of_crash`

  - Conditions:

​    - Valid when `hour_of_arrival_at_scene` is between 0 and 23.

​    - Otherwise `NULL`.

### 11. `age`

- Age of the individual involved in the accident.

### 12. `person_type`

- The type of person involved, e.g., driver, pedestrian, etc.

### 13. `seating_position`

- Where the person was seated in the vehicle.

### 14. `restraint`

- A calculated value indicating the level of restraint system helmet use:

  - `0` if 0.

  - `0.33` if 1.

  - `0.67` if 2.

  - `1.0` if 3.

  - Default to `0.5` for unspecified cases.

### 15. `survived`

- A binary value indicating if the person survived:

  - `1` if injury severity is 4 (survived),

  - `0` otherwise.

### 16. `rollover`

- A binary value indicating if a rollover occurred:

  - `0` for no rollover.

  - `1` for a rollover event.



### 17. `airbag`

- A binary indicator for airbag deployment:

  - `1` if deployed (values between 1 and 9).

  - `0` if not deployed.



### 18. `alcohol`

- A binary indicator for alcohol involvement:

  - `1` if alcohol involvement contains ""Yes"".

  - `0` otherwise.



### 19. `drugs`

- A binary indicator for drug involvement:

  - `1` if drug involvement contains ""Yes"".

  - `0` otherwise.



### 20. `related_factors_person_level1`

- Factors related to the individual involved in the accident.



### 21. `travel_speed`

- The speed at which the vehicle was traveling.



### 22. `speeding_related`

- A binary value indicating if the accident was speeding-related:

  - `1` if contains ""Yes"".

  - `0` otherwise.

### 23. `extent_of_damage`

- Describes how extensive the vehicle damage was.

### 24. `body_type`

- Type of vehicle body.

### 25. `vehicle_removal`

- Indicates whether the vehicle was removed from the scene.

### 26. `manner_of_collision`

- A capped value of the manner of collision:

  - Capped at `11` for values over 11.

### 27. `roadway_surface_condition`

- A capped value for the condition of the roadway surface:

  - Capped at `8` for values over 11.

### 28. `first_harmful_event`

- Describes the first harmful event in the crash:

  - Original value if less than 90, else 0.

### 29. `most_harmful_event`

- Describes the most harmful event in the crash:

  - Original value if less than 90, else 0.",snowflake
233,bq097,sdoh,"What is the increasing amount of the average earnings per job between the years 2012 and 2017 for each geographic region in Massachusetts (indicated by ""MA"" at the end of GeoName)?","WITH bea_2012 AS (
  SELECT GeoFIPS, GeoName, Earnings_per_job_avg AS earnings_2012
  FROM `bigquery-public-data.sdoh_bea_cainc30.fips`
  WHERE Year='2012-01-01' AND ENDS_WITH(GeoName, ""MA"") IS TRUE
),

bea_2017 AS (
  SELECT GeoFIPS, GeoName, Earnings_per_job_avg AS earnings_2017
  FROM `bigquery-public-data.sdoh_bea_cainc30.fips`
  WHERE Year='2017-01-01' AND ENDS_WITH(GeoName, ""MA"") IS TRUE
),

earnings_diff AS (
  SELECT
    bea_2017.GeoFIPS, bea_2017.GeoName, bea_2017.earnings_2017, bea_2012.earnings_2012, 
    (bea_2017.earnings_2017 - bea_2012.earnings_2012) AS earnings_change
   FROM bea_2017 
   JOIN bea_2012
   ON bea_2017.GeoFIPS = bea_2012.GeoFIPS
)
 
SELECT * FROM earnings_diff WHERE earnings_change IS NOT NULL ORDER BY earnings_change DESC",,snowflake
234,bq120,sdoh,"Identify the top 10 regions (counties) with the highest total number of SNAP-participating households, using the 2017 5-year ACS county-level data and SNAP enrollment data from January 1, 2017, excluding regions where the total SNAP participation is zero. For each of these regions, calculate the ratio of households earning under $20,000 to the total number of SNAP-participating households.","WITH acs_2017 AS (
  SELECT geo_id, income_less_10000 AS i10, income_10000_14999 AS i15, income_15000_19999 AS i20
  FROM `bigquery-public-data.census_bureau_acs.county_2017_5yr`
 ),

snap_2017_Jan AS (
  SELECT FIPS, SNAP_All_Participation_Households AS snap_total
  FROM `bigquery-public-data.sdoh_snap_enrollment.snap_enrollment`
  WHERE Date = '2017-01-01'
)

SELECT acs_2017.geo_id, snap_2017_Jan.snap_total,
(acs_2017.i10 + acs_2017.i15 + acs_2017.i20) As households_under_20,
(acs_2017.i10 + acs_2017.i15 + acs_2017.i20)/snap_2017_Jan.snap_total As under_20_snap_ratio 
FROM acs_2017
JOIN snap_2017_Jan
ON  acs_2017.geo_id = snap_2017_Jan.FIPS
WHERE snap_2017_Jan.snap_total > 0
ORDER BY snap_2017_Jan.snap_total DESC
LIMIT 10
",,snowflake
235,bq110,sdoh,What is the change in the number of homeless veterans between 2012 and 2018 for each CoC region in New York that has data available in both years?,"WITH homeless_2012 AS (
  SELECT Homeless_Veterans AS Vet12, CoC_Name  
  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc` 
  WHERE SUBSTR(CoC_Number,0,2) = ""NY"" AND Count_Year = 2012
),
 
homeless_2018 AS (
  SELECT Homeless_Veterans AS Vet18, CoC_Name  
  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc` 
  WHERE SUBSTR(CoC_Number,0,2) = ""NY"" AND Count_Year = 2018
),
 
veterans_change AS (
  SELECT homeless_2012.COC_Name, Vet12, Vet18, Vet18 - Vet12 AS VetChange
  FROM homeless_2018
  JOIN homeless_2012
  ON homeless_2018.CoC_Name = homeless_2012.CoC_Name
)

SELECT COC_Name, VetChange FROM veterans_change
ORDER BY CoC_Name;",,snowflake
236,bq395,sdoh,"Calculate the percentage change in the total number of unsheltered homeless people from 2015 to 2018 for each state by summing the counts over all Continuums of Care (CoCs) within each state. Then, determine the national average of these state percentage changes. Identify the five states whose percentage change is closest to this national average percentage change. Please provide the state abbreviations.","WITH homeless_2015 AS (
  SELECT Unsheltered_Homeless AS U15, SUBSTR(CoC_Number, 0, 2) as State_Abbr
  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc`
  WHERE Count_Year = 2015
),
 
homeless_2018 AS (
  SELECT Unsheltered_Homeless AS U18, SUBSTR(CoC_Number, 0, 2) as State_Abbr
  FROM `bigquery-public-data.sdoh_hud_pit_homelessness.hud_pit_by_coc`
  WHERE Count_Year = 2018
),

unsheltered_change AS (
  SELECT homeless_2018.State_Abbr, 
         SUM(U15) AS Unsheltered_2015, 
         SUM(U18) AS Unsheltered_2018, 
         (SUM(U18) - SUM(U15)) / SUM(U15) * 100 AS Percent_Change
  FROM homeless_2018
  JOIN homeless_2015
  ON homeless_2018.State_Abbr = homeless_2015.State_Abbr
  GROUP BY State_Abbr
),

average_change AS (
  SELECT AVG(Percent_Change) AS Avg_Change
  FROM unsheltered_change
),

closest_to_avg AS (
  SELECT State_Abbr
  FROM unsheltered_change, average_change
  ORDER BY ABS(Percent_Change - Avg_Change)
  LIMIT 5
)

SELECT State_Abbr FROM closest_to_avg;",,snowflake
237,bq352,sdoh,Please list the average number of prenatal weeks in 2018 for counties in Wisconsin where more than 5% of the employed population had commutes of 45-59 minutes in 2017.,"WITH natality_2018 AS (
  SELECT County_of_Residence_FIPS AS FIPS, Ave_Number_of_Prenatal_Wks AS Vist_Ave, County_of_Residence
  FROM `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality` 
  WHERE SUBSTR(County_of_Residence_FIPS, 0, 2) = ""55"" AND Year = '2018-01-01'
),

acs_2017 AS (
  SELECT geo_id, commute_45_59_mins, employed_pop
  FROM `bigquery-public-data.census_bureau_acs.county_2017_5yr`
),

corr_tbl AS (
  SELECT
    n.County_of_Residence,
    ROUND((a.commute_45_59_mins / a.employed_pop) * 100, 2) AS percent_high_travel,
    n.Vist_Ave
  FROM acs_2017 a
  JOIN natality_2018 n
  ON a.geo_id = n.FIPS
)

SELECT County_of_Residence, Vist_Ave
FROM corr_tbl
WHERE percent_high_travel > 5
",,snowflake
238,bq074,sdoh,"Count the number of counties that experienced an increase in unemployment from 2015 to 2018, using 5-year ACS data, and a decrease in dual-eligible enrollee counts between December 1, 2015, and December 1, 2018.","WITH acs_2018 AS (
  SELECT geo_id, unemployed_pop AS unemployed_2018  
  FROM `bigquery-public-data.census_bureau_acs.county_2018_5yr` 
),
 
acs_2015 AS (
  SELECT geo_id, unemployed_pop AS unemployed_2015  
  FROM `bigquery-public-data.census_bureau_acs.county_2015_5yr` 
),
 
unemployed_change AS (
  SELECT
    u18.unemployed_2018, u18.geo_id, u15.unemployed_2015,
    (u18.unemployed_2018 - u15.unemployed_2015) AS u_change
  FROM acs_2018 u18
  JOIN acs_2015 u15
  ON u18.geo_id = u15.geo_id
),
 
duals_Jan_2018 AS (
  SELECT Public_Total AS duals_2018, County_Name, FIPS 
  FROM `bigquery-public-data.sdoh_cms_dual_eligible_enrollment.dual_eligible_enrollment_by_county_and_program` 
  WHERE Date = '2018-12-01'
),

duals_Jan_2015 AS (
  SELECT Public_Total AS duals_2015, County_Name, FIPS
  FROM `bigquery-public-data.sdoh_cms_dual_eligible_enrollment.dual_eligible_enrollment_by_county_and_program` 
  WHERE Date = '2015-12-01'
),

duals_change AS (
  SELECT
    d18.FIPS, d18.County_Name, d18.duals_2018, d15.duals_2015,
    (d18.duals_2018 - d15.duals_2015) AS total_duals_diff
  FROM duals_Jan_2018 d18
  JOIN duals_Jan_2015 d15
  ON d18.FIPS = d15.FIPS
),
 
corr_tbl AS (
  SELECT unemployed_change.geo_id, duals_change.County_Name, unemployed_change.u_change, duals_change.total_duals_diff
  FROM unemployed_change
  JOIN duals_change
  ON unemployed_change.geo_id = duals_change.FIPS
)


SELECT COUNT(*)
FROM corr_tbl
WHERE
u_change >0
AND
corr_tbl.total_duals_diff < 0",,snowflake
239,bq066,sdoh,"Could you assess the relationship between the poverty rates from the previous year's census data and the percentage of births without maternal morbidity for the years 2016 to 2018? Use only data for births where no maternal morbidity was reported and for each year, use the 5-year census data from the year before to compute the Pearson correlation coefficient","WITH poverty_and_natality AS (
  SELECT
    EXTRACT(YEAR FROM n.Year) AS data_year,
    p.geo_id AS county_fips,
    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,
    SUM(n.Births) AS total_births,
    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity
  FROM
    `bigquery-public-data.census_bureau_acs.county_2015_5yr` p
  JOIN
    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n
  ON p.geo_id = n.County_of_Residence_FIPS
  WHERE
    p.pop_determined_poverty_status > 0 AND
    EXTRACT(YEAR FROM n.Year) = 2016
  GROUP BY
    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)
  UNION ALL
  SELECT
    EXTRACT(YEAR FROM n.Year) AS data_year,
    p.geo_id AS county_fips,
    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,
    SUM(n.Births) AS total_births,
    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity
  FROM
    `bigquery-public-data.census_bureau_acs.county_2016_5yr` p
  JOIN
    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n
  ON p.geo_id = n.County_of_Residence_FIPS
  WHERE
    p.pop_determined_poverty_status > 0 AND
    EXTRACT(YEAR FROM n.Year) = 2017
  GROUP BY
    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)
  UNION ALL
  SELECT
    EXTRACT(YEAR FROM n.Year) AS data_year,
    p.geo_id AS county_fips,
    (p.poverty / p.pop_determined_poverty_status) * 100 AS poverty_rate,
    SUM(n.Births) AS total_births,
    SUM(CASE WHEN n.Maternal_Morbidity_YN = 0 THEN n.Births ELSE 0 END) AS births_without_morbidity
  FROM
    `bigquery-public-data.census_bureau_acs.county_2017_5yr` p
  JOIN
    `bigquery-public-data.sdoh_cdc_wonder_natality.county_natality_by_maternal_morbidity` n
  ON p.geo_id = n.County_of_Residence_FIPS
  WHERE
    p.pop_determined_poverty_status > 0 AND
    EXTRACT(YEAR FROM n.Year) = 2018
  GROUP BY
    p.geo_id, p.poverty, p.pop_determined_poverty_status, EXTRACT(YEAR FROM n.Year)
)

SELECT
  data_year,
  CORR(poverty_rate, (births_without_morbidity / total_births) * 100) AS correlation_coefficient
FROM
  poverty_and_natality
GROUP BY
  data_year
",,snowflake
240,bq114,openaq,"Which three cities have the largest difference between their 1990 EPA PM2.5 measurements (using units_of_measure = 'Micrograms/cubic meter (LC)' and parameter_name = 'Acceptable PM2.5 AQI & Speciation Mass') and their 2020 OpenAQ PM2.5 measurements (where pollutant = 'pm25' based on the year extracted from the timestamp), with both datasets matched by latitude and longitude rounded to two decimals, and the difference ordered from greatest to least?","SELECT
  aq.city,
  epa.arithmetic_mean,
  aq.value,
  aq.timestamp,
  (epa.arithmetic_mean - aq.value)
FROM
  `bigquery-public-data.openaq.global_air_quality` AS aq
JOIN
  `bigquery-public-data.epa_historical_air_quality.air_quality_annual_summary` AS epa
ON
  ROUND(aq.latitude, 2) = ROUND(epa.latitude, 2)
  AND ROUND(aq.longitude, 2) = ROUND(epa.longitude, 2)
WHERE
  epa.units_of_measure = ""Micrograms/cubic meter (LC)""
  AND epa.parameter_name = ""Acceptable PM2.5 AQI & Speciation Mass""
  AND epa.year = 1990
  AND aq.pollutant = ""pm25""
  AND EXTRACT(YEAR FROM aq.timestamp) = 2020
ORDER BY
  (epa.arithmetic_mean - aq.value) DESC
LIMIT 3



",,snowflake
241,bq116,sec_quarterly_financials,"Which U.S. state reported the highest total annual revenue in billions of dollars during fiscal year 2016, considering companies that provided four quarters of data and reported measure tags in ('Revenues','SalesRevenueNet','SalesRevenueGoodsNet'), excluding any entries where the state field (stprba) is null or empty?",,,snowflake
242,sf_bq015,STACKOVERFLOW_PLUS,"Identify and rank the top 10 tags from Stack Overflow questions that were referenced in Hacker News comments on or after 2014 by counting how many times each question was mentioned, then splitting the questions’ tag strings by the '|' delimiter, grouping by tag",,,snowflake
243,bq041,stackoverflow,"Compute the monthly statistics for new StackOverflow users created in 2021. For each month, report the total number of new users, the percentage of these new users who asked at least one question within 30 days of signing up, and among those who asked a question within 30 days, the percentage who then answered at least one question after their first question and within 30 days following their first question.",,,snowflake
244,sf_bq121,STACKOVERFLOW,"How do the average reputation and number of badges vary among Stack Overflow users based on the number of complete years they have been members, considering only those who joined on or before October 1, 2021?","WITH sub AS (
  SELECT 
    ""users"".""id"",
    CAST(TO_TIMESTAMP(MAX(""users"".""creation_date"") / 1000000.0) AS DATE) AS ""user_creation_date"",  -- 使用 MAX 聚合 creation_date 并转换为 DATE
    MAX(""users"".""reputation"") AS ""reputation"",  
    SUM(CASE WHEN badges.""user_id"" IS NULL THEN 0 ELSE 1 END) AS ""num_badges""
  FROM ""STACKOVERFLOW"".""STACKOVERFLOW"".""USERS"" ""users""
  LEFT JOIN ""STACKOVERFLOW"".""STACKOVERFLOW"".""BADGES"" badges
    ON ""users"".""id"" = badges.""user_id""
  WHERE CAST(TO_TIMESTAMP(""users"".""creation_date"" / 1000000.0) AS DATE) <= DATE '2021-10-01'
  GROUP BY ""users"".""id""
)

SELECT 
  DATEDIFF(YEAR, ""user_creation_date"", DATE '2021-10-01') AS ""user_tenure"",
  COUNT(1) AS ""Num_Users"",
  AVG(""reputation"") AS ""Avg_Reputation"",
  AVG(""num_badges"") AS ""Avg_Num_Badges""
FROM sub
GROUP BY ""user_tenure""
ORDER BY ""user_tenure"";
",,snowflake
245,bq123,stackoverflow,"You need to determine which day of the week has the third highest percentage of questions on Stack Overflow that receive an answer within an hour. To do this, use the question creation date from the posts_questions table and the earliest answer creation date from the posts_answers table. Once you’ve calculated the percentage of questions that get answered within an hour for each day, identify the day with the third highest percentage and report that percentage.","WITH first_answers AS (
  SELECT
    parent_id AS question_id,
    MIN(creation_date) AS first_answer_date
  FROM
    `bigquery-public-data.stackoverflow.posts_answers`
  GROUP BY
    parent_id
)

SELECT
  FORMAT_DATE('%A', DATE(q.creation_date)) AS question_day,
  SUM(CASE WHEN f.first_answer_date IS NOT NULL AND TIMESTAMP_DIFF(f.first_answer_date, q.creation_date, MINUTE) <= 60 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_questions
FROM
  `bigquery-public-data.stackoverflow.posts_questions` q
LEFT JOIN
  first_answers f
ON
  q.id = f.question_id
GROUP BY
  question_day
ORDER BY
  percent_questions DESC
LIMIT 1 OFFSET 2",,snowflake
246,bq280,stackoverflow,"Please provide the display name of the user who has answered the most questions on Stack Overflow, considering only users with a reputation greater than 10.","WITH UserAnswers AS (
  SELECT
    owner_user_id AS answer_owner_id,
    COUNT(id) AS answer_count
  FROM bigquery-public-data.stackoverflow.posts_answers
  WHERE owner_user_id IS NOT NULL
  GROUP BY owner_user_id
),
DetailedUsers AS (
  SELECT
    id AS user_id,
    display_name AS user_display_name,
    reputation
  FROM bigquery-public-data.stackoverflow.users
  WHERE display_name IS NOT NULL AND reputation > 10
),
RankedUsers AS (
  SELECT
    u.user_display_name,
    u.reputation,
    a.answer_count,
    ROW_NUMBER() OVER (ORDER BY a.answer_count DESC) AS rank
  FROM DetailedUsers u
  JOIN UserAnswers a ON u.user_id = a.answer_owner_id
)
SELECT
  user_display_name,
FROM RankedUsers
WHERE rank = 1;
",,snowflake
247,bq300,stackoverflow,"What is the highest number of answers received for a single Python 2 specific question on Stack Overflow, excluding any discussions that involve Python 3?","WITH
  python2_questions AS (
    SELECT
      q.id AS question_id,
      q.title,
      q.body AS question_body,
      q.tags
    FROM
      `bigquery-public-data.stackoverflow.posts_questions` q
    WHERE
      (LOWER(q.tags) LIKE '%python-2%'
      OR LOWER(q.tags) LIKE '%python-2.x%'
      OR (
        LOWER(q.title) LIKE '%python 2%'
        OR LOWER(q.body) LIKE '%python 2%'
        OR LOWER(q.title) LIKE '%python2%'
        OR LOWER(q.body) LIKE '%python2%'
      ))
      AND (
        LOWER(q.title) NOT LIKE '%python 3%'
        AND LOWER(q.body) NOT LIKE '%python 3%'
        AND LOWER(q.title) NOT LIKE '%python3%'
        AND LOWER(q.body) NOT LIKE '%python3%'
      )
  )

SELECT
  COUNT(*) AS count_number
FROM
  python2_questions q
LEFT JOIN
  `bigquery-public-data.stackoverflow.posts_answers` a
ON
  q.question_id = a.parent_id
GROUP BY q.question_id
ORDER BY count_number DESC
LIMIT 1

",,snowflake
248,bq301,stackoverflow,"Retrieve details of accepted answers to Stack Overflow questions posted in January 2016 that have tags including ""javascript"" and at least one of ""xss"", ""cross-site"", ""exploit"", or ""cybersecurity""; the answers themselves must also have been posted in January 2016. For each accepted answer, include the answer's ID, the answerer's reputation, score, and comment count, along with the associated question's tags, score, answer count, the asker's reputation, view count, and comment count.","SELECT
    answer.id AS a_id,
    (SELECT users.reputation FROM `bigquery-public-data.stackoverflow.users` users
        WHERE users.id = answer.owner_user_id) AS a_user_reputation,
    answer.score AS a_score,
    answer.comment_count AS answer_comment_count,
    questions.tags as q_tags,
    questions.score AS q_score,  
    questions.answer_count AS answer_count, 
    (SELECT users.reputation FROM `bigquery-public-data.stackoverflow.users` users
        WHERE users.id = questions.owner_user_id) AS q_user_reputation,
    questions.view_count AS q_view_count,
    questions.comment_count AS q_comment_count
FROM
   `bigquery-public-data.stackoverflow.posts_answers` AS answer 
LEFT JOIN
   `bigquery-public-data.stackoverflow.posts_questions` AS questions
      ON answer.parent_id = questions.id
WHERE
    answer.id = questions.accepted_answer_id
    AND 
    (
        questions.tags LIKE '%javascript%' AND
        (questions.tags LIKE '%xss%' OR
        questions.tags LIKE '%cross-site%' OR
        questions.tags LIKE '%exploit%' OR
        questions.tags LIKE '%cybersecurity%')
    )
    AND DATE(questions.creation_date) BETWEEN '2016-01-01' AND '2016-01-31'
    AND DATE(answer.creation_date) BETWEEN '2016-01-01' AND '2016-01-31'
",,snowflake
249,bq302,stackoverflow,What is the monthly proportion of Stack Overflow questions tagged with 'python' in the year 2022?,"WITH
-- Get recent data
RecentData AS (
    SELECT
        FORMAT_TIMESTAMP('%Y%m', creation_date) AS month_index,
        tags
    FROM
        `bigquery-public-data.stackoverflow.posts_questions`
    WHERE
        EXTRACT(YEAR FROM DATE(creation_date)) = 2022
),

-- Monthly number of questions posted
MonthlyQuestions AS (
    SELECT
        month_index,
        COUNT(*) AS num_questions
    FROM
        RecentData
    GROUP BY
        month_index
),

-- Monthly number of questions posted with specific tags
TaggedQuestions AS (
    SELECT
        month_index,
        tag,
        COUNT(*) AS num_tags
    FROM
        RecentData,
        UNNEST(SPLIT(tags, '|')) AS tag
    WHERE
        tag IN ('python')
    GROUP BY
        month_index, tag
)

SELECT
    a.month_index,
    a.num_tags / b.num_questions AS proportion
FROM
    TaggedQuestions a
LEFT JOIN
    MonthlyQuestions b ON a.month_index = b.month_index
ORDER BY
    a.month_index, proportion DESC;",,snowflake
250,bq303,stackoverflow,"From July 1, 2019 through December 31, 2019, for all users with IDs between 16712208 and 18712208 on Stack Overflow, retrieve the user ID and the tags of the relevant question for each of their contributions, including comments on both questions and answers, any answers they posted, and any questions they authored, making sure to correctly associate the comment or answer with its parent question’s tags.","SELECT u_id, tags
FROM (
    -- select comments with tags from the post
    SELECT cm.u_id, cm.creation_date, cm.text, pq.tags, ""comment"" as type
    FROM (
            SELECT a.parent_id as q_id, c.user_id as u_id, c.creation_date as creation_date, c.text as text
            FROM `bigquery-public-data.stackoverflow.comments` as c
            INNER JOIN `bigquery-public-data.stackoverflow.posts_answers` as a ON (a.id = c.post_id)
            WHERE c.user_id BETWEEN 16712208 AND 18712208
              AND DATE(c.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'
            
            UNION ALL 
            
            SELECT q.id as q_id, c.user_id as u_id, c.creation_date as creation_date, c.text as text
            FROM `bigquery-public-data.stackoverflow.comments` as c
            INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` as q ON (q.id = c.post_id)
            WHERE c.user_id BETWEEN 16712208 AND 18712208
              AND DATE(c.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'
        ) as cm
    INNER JOIN `bigquery-public-data.stackoverflow.posts_questions` as pq ON (pq.id = cm.q_id)
        
    UNION ALL
    -- select answers with tags related to the post
    SELECT pa.owner_user_id as u_id, pa.creation_date as creation_date, pa.body as text, pq.tags as tags, ""answer"" as type
    FROM `bigquery-public-data.stackoverflow.posts_answers` as pa
    LEFT OUTER JOIN `bigquery-public-data.stackoverflow.posts_questions` as pq ON pq.id = pa.parent_id
    WHERE pa.owner_user_id BETWEEN 16712208 AND 18712208
      AND DATE(pa.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'
    
    UNION ALL
    -- select posts
    SELECT pq.owner_user_id as u_id, pq.creation_date as creation_date, pq.body as text, pq.tags as tags, ""question"" as type
    FROM `bigquery-public-data.stackoverflow.posts_questions` as pq
    WHERE pq.owner_user_id BETWEEN 16712208 AND 18712208
      AND DATE(pq.creation_date) BETWEEN '2019-07-01' AND '2019-12-31'
)
ORDER BY u_id, creation_date;

",,snowflake
251,bq304,stackoverflow,"Retrieve the top 50 most viewed questions for each of the following Android-related tags on StackOverflow: 'android-layout', 'android-activity', 'android-intent', 'android-edittext', 'android-fragments', 'android-recyclerview', 'listview', 'android-actionbar', 'google-maps', and 'android-asynctask'. Each question must contain the word 'how' in either its title or body and must not contain any of the following troubleshooting terms in either its title or body: 'fail', 'problem', 'error', 'wrong', 'fix', 'bug', 'issue', 'solve', or 'trouble'. Only include tags that have at least 50 questions meeting these criteria, and for each such tag, select the top 50 questions ranked by view count.","WITH
tags_to_use AS (
    SELECT tag, idx
    FROM UNNEST([
        'android-layout', 
        'android-activity', 
        'android-intent', 
        'android-edittext', 
        'android-fragments', 
        'android-recyclerview', 
        'listview', 
        'android-actionbar', 
        'google-maps', 
        'android-asynctask'
    ]) AS tag WITH OFFSET idx
),
android_how_to_questions AS (
    SELECT
        PQ.*
    FROM
        bigquery-public-data.stackoverflow.posts_questions PQ
    WHERE
        EXISTS (
            SELECT 1
            FROM UNNEST(SPLIT(PQ.tags, '|')) tag
            WHERE tag IN (SELECT tag FROM tags_to_use)
        )
        AND (LOWER(PQ.title) LIKE '%how%' OR LOWER(PQ.body) LIKE '%how%')
        AND NOT (LOWER(PQ.title) LIKE '%fail%' OR LOWER(PQ.title) LIKE '%problem%' OR LOWER(PQ.title) LIKE '%error%'
                 OR LOWER(PQ.title) LIKE '%wrong%' OR LOWER(PQ.title) LIKE '%fix%' OR LOWER(PQ.title) LIKE '%bug%'
                 OR LOWER(PQ.title) LIKE '%issue%' OR LOWER(PQ.title) LIKE '%solve%' OR LOWER(PQ.title) LIKE '%trouble%')
        AND NOT (LOWER(PQ.body) LIKE '%fail%' OR LOWER(PQ.body) LIKE '%problem%' OR LOWER(PQ.body) LIKE '%error%'
                 OR LOWER(PQ.body) LIKE '%wrong%' OR LOWER(PQ.body) LIKE '%fix%' OR LOWER(PQ.body) LIKE '%bug%'
                 OR LOWER(PQ.body) LIKE '%issue%' OR LOWER(PQ.body) LIKE '%solve%' OR LOWER(PQ.body) LIKE '%trouble%')
),
questions_with_tag_rankings AS (
    SELECT
        T.id AS tag_id,
        TTU.idx AS tag_offset,
        T.tag_name,
        T.wiki_post_id AS tag_wiki_post_id,
        Q.id AS question_id,
        Q.title,
        Q.tags,
        Q.view_count,
        RANK() OVER (PARTITION BY T.id ORDER BY Q.view_count DESC) AS question_view_count_rank,
        COUNT(*) OVER (PARTITION BY T.id) AS total_valid_questions
    FROM
        bigquery-public-data.stackoverflow.tags T
    INNER JOIN
        tags_to_use TTU ON T.tag_name = TTU.tag
    INNER JOIN
        android_how_to_questions Q ON T.tag_name IN UNNEST(SPLIT(Q.tags, '|'))
)
SELECT
    question_id
FROM
    questions_with_tag_rankings
WHERE
    question_view_count_rank <= 50 AND total_valid_questions >= 50
ORDER BY
    tag_offset ASC, question_view_count_rank ASC;
",,snowflake
252,bq310,stackoverflow,"What is the title of the most viewed ""how"" question related to Android development on StackOverflow, across specified tags such as 'android-layout', 'android-activity', 'android-intent', and others","WITH
tags_to_use AS (
    SELECT tag, idx
    FROM UNNEST([
        'android-layout', 
        'android-activity', 
        'android-intent', 
        'android-edittext', 
        'android-fragments', 
        'android-recyclerview', 
        'listview', 
        'android-actionbar', 
        'google-maps', 
        'android-asynctask'
    ]) AS tag WITH OFFSET idx
),
android_how_to_questions AS (
    SELECT
        PQ.*
    FROM
        `bigquery-public-data.stackoverflow.posts_questions` PQ
    WHERE
        EXISTS (
            SELECT 1
            FROM UNNEST(SPLIT(PQ.tags, '|')) tag
            WHERE tag IN (SELECT tag FROM tags_to_use)
        )
        AND (LOWER(PQ.title) LIKE '%how%' OR LOWER(PQ.body) LIKE '%how%')
),
most_viewed_question AS (
    SELECT
        T.id AS tag_id,
        T.tag_name,
        Q.id AS question_id,
        Q.title,
        Q.tags,
        Q.view_count
    FROM
        `bigquery-public-data.stackoverflow.tags` T
    INNER JOIN
        tags_to_use TTU ON T.tag_name = TTU.tag
    INNER JOIN
        android_how_to_questions Q ON T.tag_name IN UNNEST(SPLIT(Q.tags, '|'))
    ORDER BY Q.view_count DESC
    LIMIT 1
)
SELECT
    title
FROM
    most_viewed_question;",,snowflake
253,bq305,stackoverflow,"Which 10 users have the highest combined view counts for questions they are associated with, where a user is considered associated if they own the question, or their answer is the accepted answer, or their answer's score is greater than 5, or their answer's score exceeds 20% of the total answer scores for that question (and is above 0), or their answer is among the top three highest-scoring answers for that question?",,,snowflake
254,bq306,stackoverflow,"Identify the top 10 tags for user 1908967, based only on answers posted before June 7, 2018, where each tag’s score is 10 times the number of upvotes (vote_type_id=2) and 15 times the number of accepted answers (vote_type_id=1). Derive tags from the questions associated with those answers, and consider only the upvotes and accepted answers for those answers. Return the tags with the highest total scores in descending order, limited to 10 tags.",,,snowflake
255,sf_bq307,STACKOVERFLOW,"Find the top 10 gold badges that users most commonly earn as their first gold badge on Stack Overflow. For each of these badges, display the badge name, the number of users who earned it as their first gold badge, and the average number of days from the user's account creation date to the date they earned the badge, calculated in days without any adjustments for date formats.",,,snowflake
256,bq308,stackoverflow,"Show the number of Stack Overflow questions asked each day of the week in 2021, and find out how many and what percentage of those were answered within one hour.","SELECT
  Day_of_Week,
  COUNT(1) AS Num_Questions,
  SUM(answered_in_1h) AS Num_Answered_in_1H,
  ROUND(100 * SUM(answered_in_1h) / COUNT(1),1) AS Percent_Answered_in_1H
FROM
(
  SELECT
    q.id AS question_id,
    EXTRACT(DAYOFWEEK FROM q.creation_date) AS day_of_week,
    MAX(IF(a.parent_id IS NOT NULL AND
           (UNIX_SECONDS(a.creation_date)-UNIX_SECONDS(q.creation_date))/(60*60) <= 1, 1, 0)) AS answered_in_1h
  FROM
    `bigquery-public-data.stackoverflow.posts_questions` q
  LEFT JOIN
    `bigquery-public-data.stackoverflow.posts_answers` a
  ON q.id = a.parent_id
  WHERE EXTRACT(YEAR FROM a.creation_date) = 2020
    AND EXTRACT(YEAR FROM q.creation_date) = 2020
  GROUP BY question_id, day_of_week
)
GROUP BY
  Day_of_Week
ORDER BY
  Day_of_Week;",,snowflake
257,bq309,stackoverflow,"Retrieve the top 10 longest questions on Stack Overflow, measured by the length of their body text, where each question either has an accepted answer or has no accepted answer but has at least one answer with a score-to-view ratio exceeding 0.01. For each of these questions, include the reputation of the user who asked the question, the user's net votes (calculated as their total up_votes minus down_votes), and the total number of badges the user has earned.","WITH badge_counts AS (
  SELECT
    c.id,
    COUNT(DISTINCT d.id) AS badge_number
  FROM
    `bigquery-public-data.stackoverflow.users` AS c
  JOIN
    `bigquery-public-data.stackoverflow.badges` AS d
  ON
    c.id = d.user_id
  GROUP BY
    c.id
),
labeled_questions AS (
  SELECT
    a.id,
    IF(
      a.id IN (
        SELECT DISTINCT b.id
        FROM
          `bigquery-public-data.stackoverflow.posts_answers` AS a
        JOIN
          `bigquery-public-data.stackoverflow.posts_questions` AS b
        ON
          a.parent_id = b.id
        WHERE
          b.accepted_answer_id IS NULL
          AND a.score / b.view_count > 0.01
      ) OR accepted_answer_id IS NOT NULL,
      1,
      0
    ) AS label,
    a.owner_user_id,
    LENGTH(a.body) AS body_length
  FROM
    `bigquery-public-data.stackoverflow.posts_questions` AS a
)
SELECT
  lq.id,
  b.reputation,
  b.up_votes - b.down_votes AS net_votes,
  e.badge_number
FROM
  labeled_questions AS lq
JOIN
  `bigquery-public-data.stackoverflow.users` AS b
ON
  lq.owner_user_id = b.id
JOIN
  badge_counts AS e
ON
  b.id = e.id
WHERE
  lq.label = 1
ORDER BY
  lq.body_length DESC
LIMIT
  10;


",,snowflake
258,bq124,fhir_synthea,"Among all patients, how many individuals remain alive (i.e., with no recorded deceased.dateTime), have a diagnosis of either Diabetes or Hypertension, and are prescribed at least seven distinct active medications?","With INFO AS (
SELECT 
  MR.patientId, 
  P.last_name,
  ARRAY_TO_STRING(P.first_name, "" "") AS First_name,
  Condition.Codes, 
  Condition.Conditions,
  MR.med_count AS COUNT_NUMBER
FROM
  (SELECT 
    id, 
    name[safe_offset(0)].family as last_name, 
    name[safe_offset(0)].given as first_name, 
    TIMESTAMP(deceased.dateTime) AS deceased_datetime 
  FROM `bigquery-public-data.fhir_synthea.patient`) AS P
JOIN
  (SELECT  subject.patientId as patientId, 
           COUNT(DISTINCT medication.codeableConcept.coding[safe_offset(0)].code) AS med_count
   FROM    `bigquery-public-data.fhir_synthea.medication_request`
   WHERE   status = 'active'
   GROUP BY 1
   ) AS MR
ON MR.patientId = P.id 
JOIN
  (SELECT 
  PatientId, 
  STRING_AGG(DISTINCT condition_desc, "", "") AS Conditions, 
  STRING_AGG(DISTINCT condition_code, "", "") AS Codes
  FROM(
    SELECT 
      subject.patientId as PatientId, 
              code.coding[safe_offset(0)].code condition_code,
              code.coding[safe_offset(0)].display condition_desc
       FROM `bigquery-public-data.fhir_synthea.condition`
       wHERE 
         code.coding[safe_offset(0)].display = 'Diabetes'
         OR 
         code.coding[safe_offset(0)].display = 'Hypertension' 
    )
  GROUP BY PatientId
  ) AS Condition
ON MR.patientId = Condition.PatientId
WHERE med_count >= 7 
AND P.deceased_datetime is NULL /*only alive patients*/
GROUP BY patientId, last_name, first_name, Condition.Codes, Condition.Conditions, MR.med_count
ORDER BY last_name
)

SELECT COUNT(*) FROM INFO",,snowflake
259,bq391,fhir_synthea,"Among living patients whose last names begin with ""A"" and who each have exactly one distinct condition, which eight conditions have the highest number of different active medications prescribed to any single patient, and what are their corresponding codes?",,,snowflake
260,bq126,the_met,"What are the titles, artist names, mediums, and original image URLs of objects with 'Photograph' in their names from the 'Photographs' department, created not by an unknown artist, with an object end date of 1839 or earlier?","SELECT
  o.artist_display_name,
  o.title,
  o.object_end_date,
  o.medium,
  i.original_image_url
FROM (
  SELECT
    object_id,
    title,
    artist_display_name,
    object_end_date,
    medium
  FROM
    `bigquery-public-data.the_met.objects`
  WHERE
    department = ""Photographs""
    AND object_name LIKE ""%Photograph%""
    AND artist_display_name != ""Unknown""
    AND object_end_date <= 1839
) o
INNER JOIN (
  SELECT
    original_image_url,
    object_id
  FROM
    `bigquery-public-data.the_met.images`
) i
ON
  o.object_id = i.object_id
ORDER BY
  o.object_end_date
;",,snowflake
261,bq366,the_met,"What are the top three most frequently associated labels with artworks from each historical period in The Met's collection, only considering labels linked to 500 or more artworks? Provide me with the period, label, and the associated count.","SELECT period, description, c FROM (
  SELECT 
a.period, 
b.description, 
count(*) c, 
row_number() over (partition by period order by count(*) desc) seqnum 
  FROM `bigquery-public-data.the_met.objects` a
  JOIN (
    SELECT 
        label.description as description, 
        object_id 
    FROM `bigquery-public-data.the_met.vision_api_data`, UNNEST(labelAnnotations) label
  ) b
  ON a.object_id = b.object_id
  WHERE a.period is not null
  group by 1,2
)
WHERE seqnum <= 3
AND c >= 500 # only include labels that have 50 or more pieces associated with it
ORDER BY period, c desc;",,snowflake
262,bq414,the_met,"Retrieve the object id, title, and the formatted metadata date (as a string in 'YYYY-MM-DD' format) for objects in the ""The Libraries"" department where the cropConfidence is greater than 0.5, the object's title contains the word ""book"".","SELECT 
  a.object_id,
  a.title,
  FORMAT_TIMESTAMP('%Y-%m-%d', a.metadata_date) AS formatted_metadata_date
FROM `bigquery-public-data.the_met.objects` a
JOIN (
  SELECT object_id,
         cropHints.confidence AS cropConfidence
  FROM `bigquery-public-data.the_met.vision_api_data`, 
       UNNEST(cropHintsAnnotation.cropHints) cropHints
) b
ON a.object_id = b.object_id
WHERE a.department = ""The Libraries""
AND b.cropConfidence > 0.5
AND a.title LIKE ""%book%""",,snowflake
263,bq200,mlb,"Using data from both the regular season and the post-season, identify the pitcher who achieved the highest non-zero pitch speed for each team by confirming whether the pitcher’s ID appears in the relevant home or away player lists for that game, then retrieve that pitcher’s full name along with the maximum valid pitch speed they achieved while playing for that specific team.",,,snowflake
264,sf_bq458,WORD_VECTORS_US,"Tokenize the body text of each article into words, excluding stop words, and obtain the corresponding word vectors for these words from the glove vector. For each word, weight its word vector by dividing each component by the 0.4th power of the word's frequency from the word frequencies. Then, for each article, aggregate these weighted word vectors by summing their components to form an article vector. Normalize each article vector to unit length by dividing by its magnitude. Finally, retrieve the ID, date, title, and the normalized article vector for each article.",,"# Requirements

The following shows the SQL operations you must perform. Please implement the corresponding functionality in your SQL code according to the description.

## `tokenise_no_stop`

### Description
Removes common stopwords from a text string and tokenizes the remaining content into an array of words. This function is essential for natural language processing tasks where common words (like ""and"", ""the"", etc.) that offer little value in understanding the text's meaning are filtered out to focus on more significant words.

### SQL Definition

Please use Snowflake SQL. The following shows an example of this operation in BigQuery SQL.

```sql
tokenise_no_stop(text STRING)
AS (
(
    SELECT ARRAY_AGG(word) FROM UNNEST(REGEXP_EXTRACT_ALL(
                                REGEXP_REPLACE(text, r'’|\'s(\W)', r'\1'),
                                r'((?:\d+(?:,\d+)*(?:\.\d+)?)+|(?:[\w])+)')) AS word
    WHERE LOWER(word) not in UNNEST(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'arent', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'does', 'doesn', 'doesnt', 'doing', 'don', 'dont', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'wasnt', 'we', 'were', 'weren', 'werent', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve'])
    )
);
```
",snowflake
265,sf_bq459,WORD_VECTORS_US,"Please find the top 10 most relevant articles by only processing each article’s 'body' field, where each body is tokenized with no stopwords, each remaining token is turned into a GloVe-based word vector and weighted by dividing each dimension by the 0.4th power of its word frequency, then these weighted vectors are summed and normalized to get a unit vector for each article. Perform the same weighting and normalization on the query phrase 'Epigenetics and cerebral organoids: promising directions in autism spectrum disorders' and compute the cosine similarity between the query vector and each article vector. Finally, return the id, date, title, and the cosine similarity score for the top 10 articles with the highest similarity.",,"# Requirements

The following shows the SQL operations you must perform. Please implement the corresponding functionality in your SQL code according to the description.

## `tokenise_no_stop`

### Description
Removes common stopwords from a text string and tokenizes the remaining content into an array of words. This function is essential for natural language processing tasks where common words (like ""and"", ""the"", etc.) that offer little value in understanding the text's meaning are filtered out to focus on more significant words.

### SQL Definition

Please use Snowflake SQL. The following shows an example of this operation in BigQuery SQL.

```sql
tokenise_no_stop(text STRING)
AS (
(
    SELECT ARRAY_AGG(word) FROM UNNEST(REGEXP_EXTRACT_ALL(
                                REGEXP_REPLACE(text, r'’|\'s(\W)', r'\1'),
                                r'((?:\d+(?:,\d+)*(?:\.\d+)?)+|(?:[\w])+)')) AS word
    WHERE LOWER(word) not in UNNEST(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'arent', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'does', 'doesn', 'doesnt', 'doing', 'don', 'dont', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'wasnt', 'we', 'were', 'weren', 'werent', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve'])
    )
);
```
",snowflake
266,sf_bq460,WORD_VECTORS_US,"Please process the articles from the 'nature' dataset by first tokenizing the body text into words and removing stopwords. For each remaining word, retrieve its word vector from the glove_vectors table and its frequency from the word_frequencies table, then divide each word vector by the 0.4th power of the word's frequency to weight it. Sum the weighted vectors to obtain an aggregate vector for each article, normalize this aggregate vector to unit length, and then compute the cosine similarity scores between these normalized vectors. Finally, return the IDs, dates, titles, and cosine similarity scores of the top 10 articles most similar to the article with the ID '8a78ef2d-d5f7-4d2d-9b47-5adb25cbd373'.",,"# Requirements

The following shows the SQL operations you must perform. Please implement the corresponding functionality in your SQL code according to the description.

## `tokenise_no_stop`

### Description
Removes common stopwords from a text string and tokenizes the remaining content into an array of words. This function is essential for natural language processing tasks where common words (like ""and"", ""the"", etc.) that offer little value in understanding the text's meaning are filtered out to focus on more significant words.

### SQL Definition

Please use Snowflake SQL. The following shows an example of this operation in BigQuery SQL.

```sql
tokenise_no_stop(text STRING)
AS (
(
    SELECT ARRAY_AGG(word) FROM UNNEST(REGEXP_EXTRACT_ALL(
                                REGEXP_REPLACE(text, r'’|\'s(\W)', r'\1'),
                                r'((?:\d+(?:,\d+)*(?:\.\d+)?)+|(?:[\w])+)')) AS word
    WHERE LOWER(word) not in UNNEST(['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', 'arent', 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', 'couldnt', 'd', 'did', 'didn', 'didnt', 'do', 'does', 'doesn', 'doesnt', 'doing', 'don', 'dont', 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', 'hadnt', 'has', 'hasn', 'hasnt', 'have', 'haven', 'havent', 'having', 'he', 'her', 'here', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'i', 'if', 'in', 'into', 'is', 'isn', 'isnt', 'it', 'its', 'itself', 'just', 'll', 'm', 'ma', 'me', 'mightn', 'mightnt', 'more', 'most', 'mustn', 'mustnt', 'my', 'myself', 'needn', 'neednt', 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', 'shant', 'she', 'shes', 'should', 'shouldn', 'shouldnt', 'shouldve', 'so', 'some', 'such', 't', 'than', 'that', 'thatll', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', 'wasnt', 'we', 'were', 'weren', 'werent', 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', 'wont', 'wouldn', 'wouldnt', 'y', 'you', 'youd', 'youll', 'your', 'youre', 'yours', 'yourself', 'yourselves', 'youve'])
    )
);
```
",snowflake
267,bq204,eclipse_megamovie,Find the user with the highest total clicks across all records from all available photo collections.,"SELECT user
FROM (
Select user
      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`
      UNION ALL
      Select user
      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`
      UNION ALL
      Select user
      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`
) 
GROUP BY user 
HAVING COUNT (user)=( 
SELECT MAX(mycount) 
FROM ( 
SELECT user, COUNT(user) mycount 
FROM (
Select user
      From `bigquery-public-data.eclipse_megamovie.photos_v_0_1`
      UNION ALL
      Select user
      From`bigquery-public-data.eclipse_megamovie.photos_v_0_2`
      UNION ALL
      Select user
      From`bigquery-public-data.eclipse_megamovie.photos_v_0_3`
)
GROUP BY user))
ORDER BY COUNT(user) 
LIMIT 1",,snowflake
268,bq389,epa_historical_air_quality,"Please calculate the monthly average levels of PM10, PM2.5 FRM, PM2.5 non-FRM, volatile organic emissions, SO2 (scaled by a factor of 10), and Lead (scaled by a factor of 100) air pollutants in California for the year 2020.","SELECT
  pm10.month AS month,
  pm10.avg AS pm10,
  pm25_frm.avg AS pm25_frm,
  pm25_nonfrm.avg AS pm25_nonfrm,
  co.avg AS co,
  so2.avg AS so2,
  lead.avg AS lead
FROM
  (SELECT AVG(arithmetic_mean) AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.pm10_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS pm10
JOIN
  (SELECT AVG(arithmetic_mean) AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.pm25_frm_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS pm25_frm
ON pm10.year = pm25_frm.year AND pm10.month = pm25_frm.month
JOIN
  (SELECT AVG(arithmetic_mean) AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.pm25_nonfrm_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS pm25_nonfrm
ON pm10.year = pm25_nonfrm.year AND pm10.month = pm25_nonfrm.month
JOIN
  (SELECT AVG(arithmetic_mean) * 100 AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.lead_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS lead
ON pm10.year = lead.year AND pm10.month = lead.month
JOIN
  (SELECT AVG(arithmetic_mean) AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.voc_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS co
ON pm10.year = co.year AND pm10.month = co.month
JOIN
  (SELECT AVG(arithmetic_mean) * 10 AS avg, 
          EXTRACT(YEAR FROM date_local) AS year, 
          EXTRACT(MONTH FROM date_local) AS month
   FROM `bigquery-public-data.epa_historical_air_quality.so2_daily_summary`
   WHERE state_name = 'California' AND EXTRACT(YEAR FROM date_local) = 2020
   GROUP BY year, month) AS so2
ON pm10.year = so2.year AND pm10.month = so2.month
ORDER BY
  month;",,snowflake
269,sf_bq345,IDC,"How large are the DICOM image files with SEG or RTSTRUCT modalities and the SOP Class UID ""1.2.840.10008.5.1.4.1.1.66.4"", when grouped by collection, study, and series IDs, if they have no references to other series, images, or sources? Can you also provide a viewer URL formatted as ""https://viewer.imaging.datacommons.cancer.gov/viewer/"" followed by the study ID, and list these sizes in kilobytes, sorted from largest to smallest?","WITH seg_rtstruct AS (
  SELECT
    ""collection_id"",
    ""StudyInstanceUID"",
    ""SeriesInstanceUID"",
    CONCAT('https://viewer.imaging.datacommons.cancer.gov/viewer/', ""StudyInstanceUID"") AS ""viewer_url"",
    ""instance_size""
  FROM
    ""IDC"".""IDC_V17"".""DICOM_ALL""
  WHERE
    ""Modality"" IN ('SEG', 'RTSTRUCT')
    AND ""SOPClassUID"" = '1.2.840.10008.5.1.4.1.1.66.4'
    AND ARRAY_SIZE(""ReferencedSeriesSequence"") = 0
    AND ARRAY_SIZE(""ReferencedImageSequence"") = 0
    AND ARRAY_SIZE(""SourceImageSequence"") = 0
)

SELECT
  seg_rtstruct.""collection_id"",
  seg_rtstruct.""SeriesInstanceUID"",
  seg_rtstruct.""StudyInstanceUID"",
  seg_rtstruct.""viewer_url"",
  SUM(seg_rtstruct.""instance_size"") / 1024 AS ""collection_size_KB""
FROM
  seg_rtstruct
GROUP BY
  seg_rtstruct.""collection_id"",
  seg_rtstruct.""SeriesInstanceUID"",
  seg_rtstruct.""StudyInstanceUID"",
  seg_rtstruct.""viewer_url""
ORDER BY
  ""collection_size_KB"" DESC;
",,snowflake
270,sf_bq346,IDC,"In publicly accessible DICOM data where the Modality is 'SEG' and the SOPClassUID is '1.2.840.10008.5.1.4.1.1.66.4', and each segmentation references its original SOPInstanceUID, which five segmentation categories (by 'SegmentedPropertyCategory.CodeMeaning') occur most frequently?","WITH
  sampled_sops AS (
    SELECT
      ""collection_id"",
      ""SeriesDescription"",
      ""SeriesInstanceUID"",
      ""SOPInstanceUID"" AS ""seg_SOPInstanceUID"",
      COALESCE(
        ""ReferencedSeriesSequence""[0].""ReferencedInstanceSequence""[0].""ReferencedSOPInstanceUID"",
        ""ReferencedImageSequence""[0].""ReferencedSOPInstanceUID"",
        ""SourceImageSequence""[0].""ReferencedSOPInstanceUID""
      ) AS ""referenced_sop""
    FROM
      ""IDC"".""IDC_V17"".""DICOM_ALL""
    WHERE
      ""Modality"" = 'SEG'
      AND ""SOPClassUID"" = '1.2.840.10008.5.1.4.1.1.66.4'
      AND ""access"" = 'Public'
  ),
  segmentations_data AS (
    SELECT
      dicom_all.""collection_id"",
      dicom_all.""PatientID"",
      dicom_all.""SOPInstanceUID"",
      REPLACE(segmentations.""SegmentedPropertyCategory"":CodeMeaning::STRING, '""', '') AS ""segmentation_category"",
      REPLACE(segmentations.""SegmentedPropertyType"":CodeMeaning::STRING, '""', '') AS ""segmentation_type""
    FROM
      sampled_sops
    JOIN
      ""IDC"".""IDC_V17"".""DICOM_ALL"" AS dicom_all
    ON
      sampled_sops.""referenced_sop"" = dicom_all.""SOPInstanceUID""
    JOIN
      ""IDC"".""IDC_V17"".""SEGMENTATIONS"" AS segmentations
    ON
      segmentations.""SOPInstanceUID"" = sampled_sops.""seg_SOPInstanceUID""
  )
SELECT
  ""segmentation_category"",
  COUNT(*) AS ""count_""
FROM
  segmentations_data
GROUP BY
  ""segmentation_category""
ORDER BY
  ""count_"" DESC
LIMIT 5;
",,snowflake
271,sf_bq347,IDC,"From the union of the specified MR series with SeriesInstanceUID 1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147 and all associated segmentation instances, which modality has the greatest number of SOP instances in total, and how many are there?","WITH union_mr_seg AS (
  SELECT
    ""dicom_all_mr"".""SOPInstanceUID"",
    '' AS ""segPropertyTypeCodeMeaning"", 
    '' AS ""segPropertyCategoryCodeMeaning""
  FROM
    ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""dicom_all_mr""
  WHERE
    ""dicom_all_mr"".""SeriesInstanceUID"" IN ('1.3.6.1.4.1.14519.5.2.1.3671.4754.105976129314091491952445656147')
    
  UNION ALL

  SELECT
    ""dicom_all_seg"".""SOPInstanceUID"",
    ""segmentations"".""SegmentedPropertyType"":""CodeMeaning"" AS ""segPropertyTypeCodeMeaning"",
    ""segmentations"".""SegmentedPropertyCategory"":""CodeMeaning"" AS ""segPropertyCategoryCodeMeaning""
  FROM
    ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""dicom_all_seg""
  JOIN
    ""IDC"".""IDC_V17"".""SEGMENTATIONS"" AS ""segmentations""
  ON
    ""dicom_all_seg"".""SOPInstanceUID"" = ""segmentations"".""SOPInstanceUID""
)

SELECT
  ""dc_all"".""Modality"",
  COUNT(*) AS ""count_""
FROM 
  ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""dc_all""
INNER JOIN
  union_mr_seg
ON 
  ""dc_all"".""SOPInstanceUID"" = union_mr_seg.""SOPInstanceUID""
GROUP BY
  ""dc_all"".""Modality""
ORDER BY
  ""count_"" DESC
LIMIT 1;
",,snowflake
272,sf_bq390,IDC,"In the ""qin_prostate_repeatability"" collection, please provide the distinct StudyInstanceUIDs for studies that include T2-weighted axial MR imaging and also contain anatomical structure segmentations labeled as ""Peripheral zone.""","WITH
-- Studies that have MR volumes
""mr_studies"" AS (
  SELECT
    ""dicom_all_mr"".""StudyInstanceUID""
  FROM
    ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""dicom_all_mr""
  WHERE
    ""Modality"" = 'MR'
    AND ""collection_id"" = 'qin_prostate_repeatability'
    AND CONTAINS(""SeriesDescription"", 'T2 Weighted Axial')
),

""seg_studies"" AS (
  SELECT
    ""dicom_all_seg"".""StudyInstanceUID""
  FROM
    ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""dicom_all_seg""
  JOIN
    ""IDC"".""IDC_V17"".""SEGMENTATIONS"" AS ""segmentations""
  ON
    ""dicom_all_seg"".""SOPInstanceUID"" = ""segmentations"".""SOPInstanceUID""
  WHERE
    ""collection_id"" = 'qin_prostate_repeatability'
    AND CONTAINS(""segmentations"".""SegmentedPropertyType"":""CodeMeaning"", 'Peripheral zone')
    AND ""segmentations"".""SegmentedPropertyCategory"":""CodeMeaning"" = 'Anatomical Structure'
)

SELECT DISTINCT
  ""mr_studies"".""StudyInstanceUID""
FROM
  ""mr_studies""
JOIN
  ""seg_studies""
ON
  ""mr_studies"".""StudyInstanceUID"" = ""seg_studies"".""StudyInstanceUID"";
",,snowflake
273,sf_bq421,IDC,"Can you list all unique pairs of embedding medium and staining substance code meanings, along with the number of occurrences for each pair, based on distinct embedding medium and staining substance codes from the 'SM' modality in the DICOM dataset's un-nested specimen preparation sequences, ensuring that the codes are from the SCT coding scheme?","WITH
  SpecimenPreparationSequence_unnested AS (
    SELECT
      d.""SOPInstanceUID"",
      concept_name_code_sequence.value:""CodeMeaning""::STRING AS ""cnc_cm"",
      concept_name_code_sequence.value:""CodingSchemeDesignator""::STRING AS ""cnc_csd"",
      concept_name_code_sequence.value:""CodeValue""::STRING AS ""cnc_val"",
      concept_code_sequence.value:""CodeMeaning""::STRING AS ""ccs_cm"",
      concept_code_sequence.value:""CodingSchemeDesignator""::STRING AS ""ccs_csd"",
      concept_code_sequence.value:""CodeValue""::STRING AS ""ccs_val""
    FROM
      ""IDC"".""IDC_V17"".""DICOM_ALL"" AS d,
      LATERAL FLATTEN(input => d.""SpecimenDescriptionSequence"") AS spec_desc,
      LATERAL FLATTEN(input => spec_desc.value:""SpecimenPreparationSequence"") AS prep_seq,
      LATERAL FLATTEN(input => prep_seq.value:""SpecimenPreparationStepContentItemSequence"") AS prep_step,
      LATERAL FLATTEN(input => prep_step.value:""ConceptNameCodeSequence"") AS concept_name_code_sequence,
      LATERAL FLATTEN(input => prep_step.value:""ConceptCodeSequence"") AS concept_code_sequence
  ),
  slide_embedding AS (
    SELECT
      ""SOPInstanceUID"",
      ARRAY_AGG(DISTINCT(CONCAT(""ccs_cm"", ':', ""ccs_csd"", ':', ""ccs_val""))) AS ""embeddingMedium_code_str""
    FROM
      SpecimenPreparationSequence_unnested
    WHERE
      ""cnc_csd"" = 'SCT' AND ""cnc_val"" = '430863003' -- CodeMeaning is 'Embedding medium'
    GROUP BY
      ""SOPInstanceUID""
  ),
  slide_staining AS (
    SELECT
      ""SOPInstanceUID"",
      ARRAY_AGG(DISTINCT(CONCAT(""ccs_cm"", ':', ""ccs_csd"", ':', ""ccs_val""))) AS ""staining_usingSubstance_code_str""
    FROM
      SpecimenPreparationSequence_unnested
    WHERE
      ""cnc_csd"" = 'SCT' AND ""cnc_val"" = '424361007' -- CodeMeaning is 'Using substance'
    GROUP BY
      ""SOPInstanceUID""
  ),
  embedding_data AS (
    SELECT
      d.""SOPInstanceUID"",
      d.""instance_size"",
      e.""embeddingMedium_code_str"",
      s.""staining_usingSubstance_code_str""
    FROM
      ""IDC"".""IDC_V17"".""DICOM_ALL"" AS d
    LEFT JOIN
      slide_embedding AS e ON d.""SOPInstanceUID"" = e.""SOPInstanceUID""
    LEFT JOIN
      slide_staining AS s ON d.""SOPInstanceUID"" = s.""SOPInstanceUID""
    WHERE
      d.""Modality"" = 'SM'
  )
SELECT
  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1) AS ""embeddingMedium_CodeMeaning"",
  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1) AS ""staining_usingSubstance_CodeMeaning"",
  COUNT(*) AS ""count_""
FROM
  embedding_data
  , LATERAL FLATTEN(input => embedding_data.""embeddingMedium_code_str"") AS embeddingMedium_CodeMeaning_flat
  , LATERAL FLATTEN(input => embedding_data.""staining_usingSubstance_code_str"") AS staining_usingSubstance_CodeMeaning_flat
GROUP BY
  SPLIT_PART(embeddingMedium_CodeMeaning_flat.VALUE::STRING, ':', 1),
  SPLIT_PART(staining_usingSubstance_CodeMeaning_flat.VALUE::STRING, ':', 1);
",,snowflake
274,sf_bq422,IDC,"Using the 'nlst' collection's CT images, calculate and compare two separate metrics: 1) The average series size in MiB for the top 3 patients with the highest slice interval difference tolerance (defined as the difference between the maximum and minimum unique slice intervals across all their series), and 2) The average series size in MiB for the top 3 patients with the highest exposure difference (defined as the difference between the maximum and minimum unique exposure values across all their series). For each patient, calculate the series size by summing the instance sizes of all images in that series and converting to MiB. Return the results as two separate groups labeled ""Top 3 by Slice Interval"" and ""Top 3 by Max Exposure"" with their respective average series sizes.","WITH
  nonLocalizerRawData AS (
    SELECT
      ""SeriesInstanceUID"",
      ""StudyInstanceUID"",
      ""PatientID"",
      TRY_CAST(""Exposure""::STRING AS FLOAT) AS ""Exposure"",  -- 直接从 bid 获取 Exposure
      TRY_CAST(axes.VALUE::STRING AS FLOAT) AS ""zImagePosition"",
      LEAD(TRY_CAST(axes.VALUE::STRING AS FLOAT)) OVER (
        PARTITION BY ""SeriesInstanceUID"" 
        ORDER BY TRY_CAST(axes.VALUE::STRING AS FLOAT)
      ) - TRY_CAST(axes.VALUE::STRING AS FLOAT) AS ""slice_interval"",
      ""instance_size"" AS ""instanceSize""
    FROM
      ""IDC"".""IDC_V17"".""DICOM_ALL"" AS ""bid"",
      LATERAL FLATTEN(input => ""bid"".""ImagePositionPatient"") AS axes  -- 使用 LATERAL FLATTEN 展开数组
    WHERE
      ""collection_id"" = 'nlst' 
      AND ""Modality"" = 'CT' 
  ),
  geometryChecks AS (
    SELECT
      ""SeriesInstanceUID"",
      ""StudyInstanceUID"",
      ""PatientID"",
      ARRAY_AGG(DISTINCT ""slice_interval"") AS ""sliceIntervalDifferences"",
      ARRAY_AGG(DISTINCT ""Exposure"") AS ""distinctExposures"",
      SUM(""instanceSize"") / 1024 / 1024 AS ""seriesSizeInMB""
    FROM
      nonLocalizerRawData
    GROUP BY
      ""SeriesInstanceUID"", 
      ""StudyInstanceUID"",
      ""PatientID""
  ),
  patientMetrics AS (
    SELECT
      ""PatientID"",
      MAX(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS ""maxSliceIntervalDifference"",
      MIN(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS ""minSliceIntervalDifference"",
      MAX(TRY_CAST(sid.VALUE::STRING AS FLOAT)) - MIN(TRY_CAST(sid.VALUE::STRING AS FLOAT)) AS ""sliceIntervalDifferenceTolerance"",
      MAX(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS ""maxExposure"",
      MIN(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS ""minExposure"",
      MAX(TRY_CAST(de.VALUE::STRING AS FLOAT)) - MIN(TRY_CAST(de.VALUE::STRING AS FLOAT)) AS ""maxExposureDifference"",
      ""seriesSizeInMB""
    FROM
      geometryChecks,
      LATERAL FLATTEN(input => ""sliceIntervalDifferences"") AS sid,  -- 展开 sliceIntervalDifferences
      LATERAL FLATTEN(input => ""distinctExposures"") AS de  -- 展开 distinctExposures
    WHERE
      sid.VALUE IS NOT NULL
      AND de.VALUE IS NOT NULL
    GROUP BY
      ""PatientID"",
      ""seriesSizeInMB""
  ),
  top3BySliceInterval AS (
    SELECT
      ""PatientID"",
      ""seriesSizeInMB""
    FROM
      patientMetrics
    ORDER BY
      ""sliceIntervalDifferenceTolerance"" DESC
    LIMIT 3
  ),
  top3ByMaxExposure AS (
    SELECT
      ""PatientID"",
      ""seriesSizeInMB""
    FROM
      patientMetrics
    ORDER BY
      ""maxExposureDifference"" DESC
    LIMIT 3
  )
SELECT
  'Top 3 by Slice Interval' AS ""MetricGroup"",
  AVG(""seriesSizeInMB"") AS ""AverageSeriesSizeInMB""
FROM
  top3BySliceInterval
UNION ALL
SELECT
  'Top 3 by Max Exposure' AS ""MetricGroup"",
  AVG(""seriesSizeInMB"") AS ""AverageSeriesSizeInMB""
FROM
  top3ByMaxExposure;
",,snowflake
275,sf_bq069,IDC,"Could you help me generate a report of CT image series from the dicom_all table such that all series from the NLST collection are excluded, any localizers or JPEG-compressed series (transfer syntaxes 1.2.840.10008.1.2.4.70 or 1.2.840.10008.1.2.4.51) are skipped, and only those passing certain geometry checks—namely a single orientation, identical pixel spacing, matching SOP instance and position counts, uniform pixel rows and columns, and a near-unity dot product of image orientation vectors—are included, while also computing slice interval differences, exposure differences, and approximate series size in MB for each qualified series?",,"The assumptions include:
- consider only those series that have CT modality and do not belong to the NLST collection
- filter out DICOM images that use the following two compression formats:
    - JPEG Lossless, Non-Hierarchical, First-Order Prediction, 1.2.840.10008.1.2.4.70;
    - JPEG Baseline (Process 1), 1.2.840.10008.1.2.4.51;
- do not contain localizer image type
- all instances in a series have identical values for image orientation (patient) and pixel spacing (converted to string before comparison)
- all instances in a series have 1 ± 0.01 as the dot product between the two vectors below:
    - cross product of first and second vectors;
    - vector [0, 0, 1];
- have number of instances in the series equal to the number of distinct values of Image Position (Patient) attribute (converted to string for comparison)
- all instances in a series have identical values for the first two components of Image Position (Patient)
- all instances in a series have identical pixel rows, and similarly identical pixel columns
- for the difference between the values of the 3rd component of Image Position (Patient), after sorting all instances by that third component


BTW, the desired output report should contain:
1. The unique identifier for each image series;
2. The number assigned to the series within a study;
3. The unique identifier for the study that the series belongs to;
4. The unique identifier for the patient;
5. The maximum dot product value between the cross product of image orientation vectors and a reference vector, indicating geometric alignment;
6. The total number of SOP instances (individual images) in each series;
7. The number of distinct slice thickness values within the series;
8. The maximum and minimum difference between consecutive slice intervals (the z-coordinate spacing);
9. The tolerance value for slice interval differences (the z-coordinate spacing);
10. The number of distinct exposure values;
11. The maximum and minimum exposure value recorded for the images;
12. The difference between the maximum and minimum exposure values within the series, indicating the range of exposure variation;
13. The total size of the image series in megabytes (MiB).
And please sort the final output by slice interval difference tolerance, maximum exposure difference and series unique ID (all in descending order).",snowflake
276,sf_bq219,IOWA_LIQUOR_SALES,"In the Iowa Liquor Sales dataset, starting from January 1, 2022 through the last fully completed month, which two liquor categories, each contributing an average of at least 1% to the monthly sales volume over at least 24 months of available data, have the lowest Pearson correlation coefficient when comparing their monthly percentages of total liquor sales across those months, and what are their names?","WITH
MonthlyTotals AS
(
  SELECT
    TO_CHAR(""date"", 'YYYY-MM') AS ""month"",
    SUM(""volume_sold_gallons"") AS ""total_monthly_volume""
  FROM
    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.""SALES""
  WHERE
    ""date"" >= '2022-01-01' 
    AND TO_CHAR(""date"", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')
  GROUP BY
    TO_CHAR(""date"", 'YYYY-MM')
),

MonthCategory AS
(
  SELECT
    TO_CHAR(""date"", 'YYYY-MM') AS ""month"",
    ""category"",
    ""category_name"",
    SUM(""volume_sold_gallons"") AS ""category_monthly_volume"",
    CASE 
      WHEN ""total_monthly_volume"" != 0 THEN (SUM(""volume_sold_gallons"") / ""total_monthly_volume"") * 100
      ELSE NULL
    END AS ""category_pct_of_month_volume""
  FROM
    IOWA_LIQUOR_SALES.IOWA_LIQUOR_SALES.""SALES"" AS Sales
  LEFT JOIN
    MonthlyTotals ON TO_CHAR(Sales.""date"", 'YYYY-MM') = MonthlyTotals.""month""
  WHERE
    Sales.""date"" >= '2022-01-01' 
    AND TO_CHAR(Sales.""date"", 'YYYY-MM') < TO_CHAR(CURRENT_DATE(), 'YYYY-MM')
  GROUP BY
    TO_CHAR(Sales.""date"", 'YYYY-MM'), ""category"", ""category_name"", ""total_monthly_volume""
),

middle_info AS 
(
  SELECT
    Category1.""category"" AS ""category1"",
    Category1.""category_name"" AS ""category_name1"",
    Category2.""category"" AS ""category2"",
    Category2.""category_name"" AS ""category_name2"",
    COUNT(DISTINCT Category1.""month"") AS ""num_months"",
    CORR(Category1.""category_pct_of_month_volume"", Category2.""category_pct_of_month_volume"") AS ""category_corr_across_months"",
    AVG(Category1.""category_pct_of_month_volume"") AS ""category1_avg_pct_of_month_volume"",
    AVG(Category2.""category_pct_of_month_volume"") AS ""category2_avg_pct_of_month_volume""
  FROM
    MonthCategory Category1
  INNER JOIN
    MonthCategory Category2 
    ON Category1.""month"" = Category2.""month""
  GROUP BY
    Category1.""category"", Category1.""category_name"", Category2.""category"", Category2.""category_name""
  HAVING
    ""num_months"" >= 24
    AND ""category1_avg_pct_of_month_volume"" >= 1
    AND ""category2_avg_pct_of_month_volume"" >= 1
)

SELECT 
  ""category_name1"", 
  ""category_name2""
FROM 
  middle_info
ORDER BY 
  ""category_corr_across_months""
LIMIT 1;
",,snowflake
277,bq199,iowa_liquor_sales,"Identify the top 10 liquor categories in Iowa in 2021 by calculating, for each category, the average of the per-liter retail prices across all sales transactions in that category during 2021. For these top categories, provide their average per-liter retail prices calculated in the same manner for the years 2019, 2020, and 2021.","WITH price_2020 AS (
  SELECT 
    category_name AS category, 
    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2020
  FROM 
    `bigquery-public-data.iowa_liquor_sales.sales`
  WHERE 
    bottle_volume_ml > 0 
    AND EXTRACT(YEAR FROM date) = 2020
  GROUP BY 
    category
),
price_2019 AS (
  SELECT 
    category_name AS category, 
    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2019
  FROM 
    `bigquery-public-data.iowa_liquor_sales.sales`
  WHERE 
    bottle_volume_ml > 0 
    AND EXTRACT(YEAR FROM date) = 2019
  GROUP BY 
    category
),
price_2021 AS (
  SELECT 
    category_name AS category, 
    AVG(state_bottle_retail / (bottle_volume_ml / 1000)) AS avg_price_liter_2021
  FROM 
    `bigquery-public-data.iowa_liquor_sales.sales`
  WHERE 
    bottle_volume_ml > 0 
    AND EXTRACT(YEAR FROM date) = 2021
  GROUP BY 
    category
)
SELECT 
  price_2021.category, 
  price_2019.avg_price_liter_2019, 
  price_2020.avg_price_liter_2020, 
  price_2021.avg_price_liter_2021
FROM 
  price_2021
LEFT JOIN 
  price_2019 ON price_2021.category = price_2019.category
LEFT JOIN 
  price_2020 ON price_2021.category = price_2020.category
ORDER BY 
  price_2021.avg_price_liter_2021 DESC
LIMIT 
  10;",,snowflake
278,bq218,iowa_liquor_sales,What are the top 5 items with the highest year-over-year growth percentage in total sales revenue for the year 2023?,"WITH AnnualSales AS (
  SELECT
    item_description,
    EXTRACT(YEAR FROM date) AS year,
    SUM(sale_dollars) AS total_sales_revenue,
    COUNT(DISTINCT invoice_and_item_number) AS unique_purchases
  FROM
    `bigquery-public-data.iowa_liquor_sales.sales`
  WHERE
    EXTRACT(YEAR FROM date) IN (2022, 2023)
    AND item_description IS NOT NULL
    AND sale_dollars IS NOT NULL
  GROUP BY
    item_description, year
),
YoYGrowth AS (
  SELECT
    curr.item_description,
    curr.year,
    curr.total_sales_revenue,
    curr.unique_purchases,
    LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) AS prev_year_sales_revenue,
    (curr.total_sales_revenue - LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year)) / LAG(curr.total_sales_revenue) OVER(PARTITION BY curr.item_description ORDER BY curr.year) * 100 AS yoy_growth_percentage
  FROM
    AnnualSales curr
),
total_info AS (
SELECT
  item_description,
  year,
  total_sales_revenue,
  unique_purchases,
  prev_year_sales_revenue,
  yoy_growth_percentage
FROM
  YoYGrowth
WHERE
  year = 2023
  AND prev_year_sales_revenue IS NOT NULL -- Exclude rows where there's no previous year data to calculate YoY growth
ORDER BY
  year, total_sales_revenue 
DESC
)

SELECT item_description
FROM total_info
order by yoy_growth_percentage
DESC
LIMIT 5",,snowflake
279,bq049,iowa_liquor_sales_plus,"Please show the monthly per capita Bourbon Whiskey sales during 2022 in Dubuque County for the zip code that ranks third in total Bourbon Whiskey sales, using only the population aged 21 and older.","WITH DUBUQUE_LIQUOR_CTE AS (
SELECT
  CASE
      WHEN UPPER(category_name) LIKE 'BUTTERSCOTCH SCHNAPPS' THEN 'All Other' --Edge case is not a scotch
      WHEN UPPER(category_name) LIKE '%WHISKIES' 
            AND UPPER(category_name) NOT LIKE '%RYE%'
            AND UPPER(category_name) NOT LIKE '%BOURBON%'
            AND UPPER(category_name) NOT LIKE '%SCOTCH%'     THEN 'Other Whiskey'
      WHEN UPPER(category_name) LIKE '%RYE%'                 THEN 'Rye Whiskey'
      WHEN UPPER(category_name) LIKE '%BOURBON%'             THEN 'Bourbon Whiskey'
      WHEN UPPER(category_name) LIKE '%SCOTCH%'              THEN 'Scotch Whiskey'
      ELSE 'All Other'
  END                              AS category_group,
  EXTRACT(MONTH FROM date)         AS month,    -- At the time of this query, there is only data until month 6.
  LEFT(CAST(zip_code AS string),5) AS zip_code, -- Casting to string necessary because zip_code has a mix of int & str types.
  ROUND(SUM(sale_dollars), 2)      AS sale_dollars_sum,

FROM 
  bigquery-public-data.iowa_liquor_sales.sales

WHERE
  UPPER(county)               = 'DUBUQUE'
  AND EXTRACT(YEAR FROM date) = 2022

GROUP BY
  category_group,
  month,
  zip_code
  
ORDER BY 
  category_group,
  month,
  zip_code
),

DUBUQUE_POPULATION_CTE AS (
SELECT
  zipcode,
  SUM(population) AS population_sum
FROM bigquery-public-data.census_bureau_usa.population_by_zip_2010
WHERE 
  minimum_age >= 21
GROUP BY 
  zipcode
),
MONTH_INFO AS (
SELECT 
  l.month,
  l.zip_code,
  l.sale_dollars_sum,
  ROUND(sale_dollars_sum/p.population_sum, 2) AS dollars_per_capita
FROM 
  DUBUQUE_LIQUOR_CTE AS l
  LEFT JOIN 
  DUBUQUE_POPULATION_CTE AS p
  ON l.zip_code = p.zipcode
WHERE
  category_group = 'Bourbon Whiskey'
GROUP BY 
  category_group,
  zip_code,
  month,
  sale_dollars_sum,
  zipcode,
  population_sum
ORDER BY
  zip_code,
  month
),
zip_code_sales AS (
    SELECT
        zip_code,
        SUM(sale_dollars_sum) AS total_sale_dollars_sum
    FROM MONTH_INFO
    GROUP BY zip_code
),
ranked_zip_codes AS (
    SELECT
        zip_code,
        total_sale_dollars_sum,
        ROW_NUMBER() OVER (ORDER BY total_sale_dollars_sum DESC) AS rank
    FROM zip_code_sales
)
SELECT
    t.month,
    t.zip_code,
    t.dollars_per_capita
FROM MONTH_INFO t
JOIN ranked_zip_codes r
ON t.zip_code = r.zip_code
WHERE r.rank = 3
ORDER BY t.month;",,snowflake
280,bq360,nppes,"Among healthcare providers whose practice location is in Mountain View, CA, and who have a specified specialization in the field healthcare provider taxonomy, identify the top 10 most common specializations based on the count of distinct NPIs. Then determine which of those top 10 has a count of distinct NPIs closest to the average count across those 10 specializations.","WITH specialist_counts AS (
  SELECT
    healthcare_provider_taxonomy_1_specialization,
    COUNT(DISTINCT npi) AS number_specialist
  FROM
    `bigquery-public-data.nppes.npi_optimized`
  WHERE
    provider_business_practice_location_address_city_name = ""MOUNTAIN VIEW""
    AND provider_business_practice_location_address_state_name = ""CA""
    AND healthcare_provider_taxonomy_1_specialization > """"
  GROUP BY
    healthcare_provider_taxonomy_1_specialization
),
top_10_specialists AS (
  SELECT
    healthcare_provider_taxonomy_1_specialization,
    number_specialist
  FROM
    specialist_counts
  ORDER BY
    number_specialist DESC
  LIMIT 10
),
average_value AS (
  SELECT
    AVG(number_specialist) AS average_specialist
  FROM
    top_10_specialists
),
closest_to_average AS (
  SELECT
    healthcare_provider_taxonomy_1_specialization,
    number_specialist,
    ABS(number_specialist - (SELECT average_specialist FROM average_value)) AS difference
  FROM
    top_10_specialists
)
SELECT
  healthcare_provider_taxonomy_1_specialization
FROM
  closest_to_average
ORDER BY
  difference
LIMIT 1;",,snowflake
281,bq286,usa_names,"Can you tell me the name of the most popular female baby in Wyoming for the year 2021, based on the proportion of female babies given that name compared to the total number of female babies given the same name across all states?","SELECT
  a.name AS name
FROM
  `bigquery-public-data.usa_names.usa_1910_current` a
JOIN (
  SELECT
    name,
    gender,
    year,
    SUM(number) AS total_number
  FROM
    `bigquery-public-data.usa_names.usa_1910_current`
  GROUP BY
    name,
    gender,
    year) b
ON
  a.name = b.name
  AND a.gender = b.gender
  AND a.year = b.year
WHERE 
    a.gender = 'F' AND
    a.state = 'WY' AND
    a.year = 2021
ORDER BY (a.number / b.total_number) DESC
LIMIT 1
",,snowflake
282,sf_bq044,TCGA,"For bladder cancer patients who have mutations in the CDKN2A (cyclin-dependent kinase inhibitor 2A) gene, using clinical data from the Genomic Data Commons Release 39, what types of mutations are they, what is their gender, vital status, and days to death - and for four downstream genes (MDM2 (MDM2 proto-oncogene), TP53 (tumor protein p53), CDKN1A (cyclin-dependent kinase inhibitor 1A), and CCNE1 (Cyclin E1)), what are the gene expression levels for each patient?",,"# TCGA Code Tables

This section contains tables of TCGA codes and abbreviations that are found throughout TCGA-related data and documentation. The tables are provided to assist users in understanding TCGA data. Users are advised that the GDC Data Dictionary contains the latest official set of GDC terms and definitions.


## TCGA Study Abbreviations

| Study Abbreviation | Study Name |
| ---- | ---- |
| LAML   | Acute Myeloid Leukemia                                           |
| ACC    | Adrenocortical carcinoma                                         |
| BLCA   | Bladder Urothelial Carcinoma                                     |
| LGG    | Brain Lower Grade Glioma                                         |
| BRCA   | Breast invasive carcinoma                                        |
| CESC   | Cervical squamous cell carcinoma and endocervical adenocarcinoma |
| CHOL   | Cholangiocarcinoma                                               |
| LCML   | Chronic Myelogenous Leukemia                                     |
| COAD   | Colon adenocarcinoma                                             |
| CNTL   | Controls                                                         |
| ESCA   | Esophageal carcinoma                                             |
| FPPP   | FFPE Pilot Phase II                                              |
| GBM    | Glioblastoma multiforme                                          |
| HNSC   | Head and Neck squamous cell carcinoma                            |
| KICH   | Kidney Chromophobe                                               |
| KIRC   | Kidney renal clear cell carcinoma                                |
| KIRP   | Kidney renal papillary cell carcinoma                            |
| LIHC   | Liver hepatocellular carcinoma                                   |
| LUAD   | Lung adenocarcinoma                                              |
| LUSC   | Lung squamous cell carcinoma                                     |
| DLBC   | Lymphoid Neoplasm Diffuse Large B-cell Lymphoma                  |
| MESO   | Mesothelioma                                                     |
| MISC   | Miscellaneous                                                    |
| OV     | Ovarian serous cystadenocarcinoma                                |
| PAAD   | Pancreatic adenocarcinoma                                        |
| PCPG   | Pheochromocytoma and Paraganglioma                               |
| PRAD   | Prostate adenocarcinoma                                          |
| READ   | Rectum adenocarcinoma                                            |
| SARC   | Sarcoma                                                          |
| SKCM   | Skin Cutaneous Melanoma                                          |
| STAD   | Stomach adenocarcinoma                                           |
| TGCT   | Testicular Germ Cell Tumors                                      |
| THYM   | Thymoma                                                          |
| THCA   | Thyroid carcinoma                                                |
| UCS    | Uterine Carcinosarcoma                                           |
| UCEC   | Uterine Corpus Endometrial Carcinoma                             |
| UVM    | Uveal Melanoma                                                   |
",snowflake
283,sf_bq043,TCGA,"What are the RNA expression levels of the genes MDM2, TP53, CDKN1A, and CCNE1, along with associated clinical information, in bladder cancer patients with CDKN2A mutations in the 'TCGA-BLCA' project?  Use clinical data from the Genomic Data Commons Release 39, data about somatic mutations derived from the hg19 human genome reference in Feb 2017.","SELECT
  genex.""case_barcode"" AS ""case_barcode"",
  genex.""sample_barcode"" AS ""sample_barcode"",
  genex.""aliquot_barcode"" AS ""aliquot_barcode"",
  genex.""HGNC_gene_symbol"" AS ""HGNC_gene_symbol"",
  clinical_info.""Variant_Type"" AS ""Variant_Type"",
  genex.""gene_id"" AS ""gene_id"",
  genex.""normalized_count"" AS ""normalized_count"",
  genex.""project_short_name"" AS ""project_short_name"",
  clinical_info.""demo__gender"" AS ""gender"",
  clinical_info.""demo__vital_status"" AS ""vital_status"",
  clinical_info.""demo__days_to_death"" AS ""days_to_death""
FROM ( 
  SELECT
    case_list.""Variant_Type"" AS ""Variant_Type"",
    case_list.""case_barcode"" AS ""case_barcode"",
    clinical.""demo__gender"",
    clinical.""demo__vital_status"",
    clinical.""demo__days_to_death""
  FROM
    (SELECT
      mutation.""case_barcode"",
      mutation.""Variant_Type""
    FROM
      ""TCGA"".""TCGA_VERSIONED"".""SOMATIC_MUTATION_HG19_DCC_2017_02"" AS mutation
    WHERE
      mutation.""Hugo_Symbol"" = 'CDKN2A'
      AND mutation.""project_short_name"" = 'TCGA-BLCA'
    GROUP BY
      mutation.""case_barcode"",
      mutation.""Variant_Type""
    ORDER BY
      mutation.""case_barcode""
    ) AS case_list /* end case_list */
  INNER JOIN
    ""TCGA"".""TCGA_VERSIONED"".""CLINICAL_GDC_R39"" AS clinical
  ON
    case_list.""case_barcode"" = clinical.""submitter_id"" /* end clinical annotation */ ) AS clinical_info
INNER JOIN
  ""TCGA"".""TCGA_VERSIONED"".""RNASEQ_HG19_GDC_2017_02"" AS genex
ON
  genex.""case_barcode"" = clinical_info.""case_barcode""
WHERE
  genex.""HGNC_gene_symbol"" IN ('MDM2', 'TP53', 'CDKN1A','CCNE1')
ORDER BY
  ""case_barcode"",
  ""HGNC_gene_symbol"";
",,snowflake
284,bq143,CPTAC_PDC,"Use CPTAC proteomics and RNAseq data for Clear Cell Renal Cell Carcinoma to select 'Primary Tumor' and 'Solid Tissue Normal' samples. Join the datasets on sample submitter IDs and gene symbols. Calculate the correlation between protein abundance (log2 ratio) and gene expression levels (log-transformed+1 FPKM) for each gene and sample type. Filter out correlations with an absolute value greater than 0.5, and compute the average correlation for each sample type.","WITH 
quant AS (
    SELECT 
        meta.sample_submitter_id, 
        meta.sample_type, 
        quant.case_id, 
        quant.aliquot_id, 
        quant.gene_symbol, 
        CAST(quant.protein_abundance_log2ratio AS FLOAT64) AS protein_abundance_log2ratio 
    FROM 
        `isb-cgc-bq.CPTAC.quant_proteome_CPTAC_CCRCC_discovery_study_pdc_current` AS quant
    JOIN 
        `isb-cgc-bq.PDC_metadata.aliquot_to_case_mapping_current` AS meta
        ON quant.case_id = meta.case_id
        AND quant.aliquot_id = meta.aliquot_id
        AND meta.sample_type IN ('Primary Tumor', 'Solid Tissue Normal')
),
gexp AS (
    SELECT DISTINCT 
        meta.sample_submitter_id, 
        meta.sample_type, 
        rnaseq.gene_name, 
        LOG(rnaseq.fpkm_unstranded + 1) AS HTSeq__FPKM   -- Confirm the correct column name here
    FROM 
        `isb-cgc-bq.CPTAC.RNAseq_hg38_gdc_current` AS rnaseq
    JOIN 
        `isb-cgc-bq.PDC_metadata.aliquot_to_case_mapping_current` AS meta
        ON meta.sample_submitter_id = rnaseq.sample_barcode
),
correlation AS (
    SELECT 
        quant.gene_symbol, 
        gexp.sample_type, 
        COUNT(*) AS n, 
        CORR(protein_abundance_log2ratio, HTSeq__FPKM) AS corr  -- Confirm the correct column name here
    FROM 
        quant 
    JOIN 
        gexp 
        ON quant.sample_submitter_id = gexp.sample_submitter_id
        AND gexp.gene_name = quant.gene_symbol
        AND gexp.sample_type = quant.sample_type
    GROUP BY 
        quant.gene_symbol, gexp.sample_type
),
pval AS (
    SELECT  
        gene_symbol, 
        sample_type, 
        n, 
        corr
    FROM 
        correlation
    WHERE 
        ABS(corr) <= 0.5
)
SELECT sample_type, AVG(corr)
FROM pval
GROUP BY sample_type;",,snowflake
285,sf_bq147,TCGA,"Can you identify the TCGA breast cancer cases from the RNA sequencing hg38 r35` where the protein_coding gene and the project TCGA-BRCA, and which have RNA sequencing samples of multiple tissue types—including ""Solid Tissue Normal""—within the same case?",,,snowflake
286,sf_bq148,TCGA,Could you identify the top five protein-coding genes that exhibit the highest variance in their expression levels (measured as fpkm_uq_unstranded) specifically within 'Solid Tissue Normal' samples? Please limit the analysis to TCGA-BRCA project cases that include at least one 'Solid Tissue Normal' sample type.,,,snowflake
287,sf_bq175,TCGA_MITELMAN,"Identify cytoband names on chromosome 1 in the TCGA-KIRC segment allelic dataset where the frequency of amplifications, gains, and heterozygous deletions each rank within the top 11. Calculate these rankings based on the maximum copy number observed across various genomic studies of kidney cancer, reflecting the severity of genetic alterations.",,"### Comprehensive Guide to Copy Number Variations in Cancer Genomics

#### **1. Introduction to Copy Number Variations (CNVs)**

Copy number variations (CNVs) are changes in the genome where regions have altered numbers of DNA segments. These variations include amplifications or deletions, significantly impacting genetic diversity and disease progression, particularly in cancer.

#### **2. The Role of CNVs in Cancer**

CNVs can drive cancer progression by amplifying oncogenes or deleting tumor suppressor genes, affecting gene dosage and cellular control mechanisms.

#### **3. TCGA-KIRC Project Overview**

The TCGA Kidney Renal Clear Cell Carcinoma (KIRC) project offers crucial CNV data to enhance our understanding of the molecular basis of kidney cancer.

#### **4. CytoBands and Their Genomic Significance**

CytoBands are chromosomal regions identified by staining patterns that help localize genetic functions and structural features.

#### **5. Data Sources for CNV Analysis**

- **TCGA CNV Data**: Provides genomic copy number changes in cancer tissues.
- **Mitelman Database (CytoBands_hg38)**: Offers detailed cytoband data for mapping CNVs to chromosomes.

#### **6. CNV Categories and Their Implications in Cancer**

- **Amplifications** (>3 copies): Lead to oncogene overexpression, accelerating tumor growth.
- **Gains** (=3 copies): Cause subtle changes in gene dosage, potentially enhancing cancer progression.
- **Homozygous Deletions** (0 copies): Result in the loss of both copies of tumor suppressor genes, promoting tumor development.
- **Heterozygous Deletions** (1 copy): Reduce the dosage of key regulatory genes, contributing to tumor progression.
- **Normal Diploid** (2 copies): Maintain standard genomic copies, serving as a baseline for comparative analysis.

#### **7. Methodology for Determining Overlaps**

To localize CNVs within specific cytobands, we use:

\[ \text{Overlap} = \max(0, \min(\text{end\_pos}, \text{hg38\_stop}) - \max(\text{start\_pos}, \text{hg38\_start})) \]

This formula ensures that the overlap measurement is the actual intersected length of the CNV and cytoband segments. It uses:
- `\min(\text{end\_pos}, \text{hg38\_stop})` to find the smallest endpoint between the CNV segment and the cytoband.
- `\max(\text{start\_pos}, \text{hg38\_start})` to find the largest start point between the CNV segment and the cytoband.
- The `max(0, ...)` function ensures that the overlap cannot be negative, which would indicate no actual overlap.


#### **8. Conclusion**

Analyzing CNVs is crucial for understanding cancer genetics and developing targeted therapies. Integrating CNV analysis with traditional markers enhances our insights into tumor biology.",snowflake
288,sf_bq176,TCGA_MITELMAN,"Identify the case barcodes from the TCGA-LAML study with the highest weighted average copy number in cytoband 15q11 on chromosome 15, using segment data and cytoband overlaps from TCGA's genomic and Mitelman databases.","WITH copy AS (
  SELECT 
    ""case_barcode"", 
    ""chromosome"", 
    ""start_pos"", 
    ""end_pos"", 
    MAX(""copy_number"") AS ""copy_number""
  FROM 
    ""TCGA_MITELMAN"".""TCGA_VERSIONED"".""COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23""
  WHERE 
    ""project_short_name"" = 'TCGA-LAML'
  GROUP BY 
    ""case_barcode"", 
    ""chromosome"", 
    ""start_pos"", 
    ""end_pos""
),
total_cases AS (
  SELECT COUNT(DISTINCT ""case_barcode"") AS ""total""
  FROM copy
),
cytob AS (
  SELECT 
    ""chromosome"", 
    ""cytoband_name"", 
    ""hg38_start"", 
    ""hg38_stop""
  FROM 
    ""TCGA_MITELMAN"".""PROD"".""CYTOBANDS_HG38""
),
joined AS (
  SELECT 
    cytob.""chromosome"", 
    cytob.""cytoband_name"", 
    cytob.""hg38_start"", 
    cytob.""hg38_stop"", 
    copy.""case_barcode"",
    (ABS(cytob.""hg38_stop"" - cytob.""hg38_start"") + ABS(copy.""end_pos"" - copy.""start_pos"") 
      - ABS(cytob.""hg38_stop"" - copy.""end_pos"") - ABS(cytob.""hg38_start"" - copy.""start_pos"")) / 2.0 AS ""overlap"", 
    copy.""copy_number""
  FROM 
    copy
  LEFT JOIN 
    cytob
  ON 
    cytob.""chromosome"" = copy.""chromosome""
  WHERE 
    (cytob.""hg38_start"" >= copy.""start_pos"" AND copy.""end_pos"" >= cytob.""hg38_start"")
    OR (copy.""start_pos"" >= cytob.""hg38_start"" AND copy.""start_pos"" <= cytob.""hg38_stop"")
),
INFO AS (
  SELECT 
    ""chromosome"", 
    ""cytoband_name"", 
    ""hg38_start"", 
    ""hg38_stop"", 
    ""case_barcode"",
    ROUND(SUM(""overlap"" * ""copy_number"") / SUM(""overlap"")) AS ""copy_number""
  FROM 
    joined
  GROUP BY 
    ""chromosome"", ""cytoband_name"", ""hg38_start"", ""hg38_stop"", ""case_barcode""
)

SELECT 
  ""case_barcode""
FROM 
  INFO
WHERE 
  ""chromosome"" = 'chr15' 
  AND ""cytoband_name"" = '15q11'
ORDER BY 
  ""copy_number"" DESC
LIMIT 1;
",,snowflake
289,sf_bq170,TCGA_MITELMAN,"For breast cancer cases (TCGA-BRCA) from Release 23 of the active GDC archive, identify and categorize copy number variations (CNVs) across all cytobands on every chromosome. For each cytoband and each case, determine the overlap between the cytoband region and the case's copy number segments, and compute the overlap-weighted average copy number for that cytoband in the case, rounding to the nearest whole number. Classify the rounded copy number into CNV types as follows: homozygous deletions (0), heterozygous deletions (1), normal diploid state (2), gains (3), and amplifications (greater than 3). For each cytoband, provide its name and start/end positions, and calculate the frequency of each CNV type across all cases as a percentage of the total number of cases, rounded to two decimal places.",,"# Copy Number Variations

Copy number variation (abbreviated CNV) refers to a circumstance in which the number of copies of a specific segment of DNA varies among different individuals’ genomes. The individual variants may be short or include thousands of bases. These structural differences may have come about through duplications, deletions or other changes and can affect long stretches of DNA. Such regions may or may not contain a gene(s).


## How Copy Number is Calculated

- For each case, we only care about the maximum copy number for a specific chromosome region;
- The overlap between a cytoband (a defined region on a chromosome) and a copy number segment is calculated using:
$$\text{overlap}=\frac{|\text{hg38\_stop}−\text{hg38\_start}| + |\text{end\_pos}−\text{start\_pos}| − |\text{hg38\_stop}−\text{end\_pos}| − |\text{hg38\_start}−\text{start\_pos}|}{2}$$
This measures how much of the copy number segment overlaps with the cytoband.
- The weighted average copy number for each cytoband is calculated by
$$\text{weighted copy number}=\frac{\sum (\text{overlap} \times \text{copy\_number})}{\sum \text{overlap}}$$
- Usually, the weighted average is rounded to produce the final copy mumber for each cytoband.


## Five Types of Genomic Alterations

Here’s an explanation of the five types of genomic alterations as represented by copy number variations (CNVs):

### Homozygous Deletions

- Definition: A homozygous deletion (often called a complete deletion) occurs when both copies of a gene are lost, resulting in a copy number of 0.
- Criteria: copy number = 0 identifies homozygous deletions.
- Biological Significance: Homozygous deletions can lead to the loss of tumor suppressor genes (genes that normally prevent cancer growth). Without these protective genes, cells can more easily progress toward cancer.


### Heterozygous Deletions

- Definition: A heterozygous deletion involves the loss of one copy of a gene, while the other copy remains intact. The copy number is 1 in this case.
- Criteria: copy number = 1 indicates a heterozygous deletion.
- Biological Significance: While less severe than a homozygous deletion, heterozygous deletions can still disrupt cellular function, especially if the remaining copy of the gene is mutated or insufficient to maintain normal function.


### Normal Diploid

- Definition: A normal diploid region has the standard two copies of a gene, as is typical in most healthy human cells.
- Criteria: copy number = 2 is used to identify regions with a normal diploid copy number.
- Biological Significance: Diploid regions are expected in non-cancerous cells and represent the baseline state of the genome, where two functional copies of each gene are present.


### Gains

- Definition: A gain is an increase in the number of copies of a region of the genome, but it's less severe than amplification. Gains involve having up to two additional copies beyond the diploid state, resulting in a copy number of 3.
- Criteria: One extra copy beyond the normal two.
- Biological Significance: Gains can be associated with increased expression of genes that promote cancer, but the effects may be more moderate compared to amplifications.


###  Amplifications

- Definition: Amplifications refer to regions of the genome where the number of copies of a gene is much higher than usual. For a diploid cell (a normal cell with two copies of each chromosome), amplifications involve having more than four copies of a gene.
- Criteria: There are at least 4 copies of a gene in the region.
- Biological Significance: Amplifications are common in cancer, where extra copies of oncogenes (genes that can promote cancer growth) may drive uncontrolled cell proliferation.


These categories help identify patterns in the genomic alterations present in cancer cells, providing insight into the molecular mechanisms driving cancer progression.",snowflake
290,sf_bq150,TCGA_HG19_DATA_V0,"Assess whether different genetic variants affect the log10-transformed TP53 expression levels in TCGA-BRCA samples using sequencing and mutation data. Provide the total number of samples, the number of mutation types, the mean square between groups, the mean square within groups, and the F-statistic.","WITH
cohortExpr AS (
  SELECT
    ""sample_barcode"",
    LOG(10, ""normalized_count"") AS ""expr""
  FROM
    ""TCGA_HG19_DATA_V0"".""TCGA_HG19_DATA_V0"".""RNASEQ_GENE_EXPRESSION_UNC_RSEM""
  WHERE
    ""project_short_name"" = 'TCGA-BRCA'
    AND ""HGNC_gene_symbol"" = 'TP53'
    AND ""normalized_count"" IS NOT NULL
    AND ""normalized_count"" > 0
),
cohortVar AS (
  SELECT
    ""Variant_Type"",
    ""sample_barcode_tumor"" AS ""sample_barcode""
  FROM
    ""TCGA_HG19_DATA_V0"".""TCGA_HG19_DATA_V0"".""SOMATIC_MUTATION_MC3""
  WHERE
    ""SYMBOL"" = 'TP53'
),
cohort AS (
  SELECT
    e.""sample_barcode"" AS ""sample_barcode"",
    v.""Variant_Type"" AS ""group_name"",
    e.""expr""
  FROM
    cohortExpr e
  JOIN
    cohortVar v
  ON
    e.""sample_barcode"" = v.""sample_barcode""
),
grandMeanTable AS (
  SELECT
    AVG(""expr"") AS ""grand_mean""
  FROM
    cohort
),
groupMeansTable AS (
  SELECT
    AVG(""expr"") AS ""group_mean"",
    ""group_name"",
    COUNT(""sample_barcode"") AS ""n""
  FROM
    cohort
  GROUP BY
    ""group_name""
),
ssBetween AS (
  SELECT
    g.""group_name"",
    g.""group_mean"",
    gm.""grand_mean"",
    g.""n"",
    g.""n"" * POW(g.""group_mean"" - gm.""grand_mean"", 2) AS ""n_diff_sq""
  FROM
    groupMeansTable g
  CROSS JOIN
    grandMeanTable gm
),
ssWithin AS (
  SELECT
    c.""group_name"" AS ""group_name"",
    c.""expr"",
    b.""group_mean"",
    b.""n"" AS ""n"",
    POW(c.""expr"" - b.""group_mean"", 2) AS ""s2""
  FROM
    cohort c
  JOIN
    ssBetween b
  ON
    c.""group_name"" = b.""group_name""
),
numerator AS (
  SELECT
    SUM(""n_diff_sq"") / (COUNT(""group_name"") - 1) AS ""mean_sq_between""
  FROM
    ssBetween
),
denominator AS (
  SELECT
    COUNT(DISTINCT ""group_name"") AS ""k"",
    COUNT(""group_name"") AS ""n"",
    SUM(""s2"") / (COUNT(""group_name"") - COUNT(DISTINCT ""group_name"")) AS ""mean_sq_within""
  FROM
    ssWithin
)

SELECT
  ""n"",
  ""k"",
  ""mean_sq_between"",
  ""mean_sq_within"",
  ""mean_sq_between"" / ""mean_sq_within"" AS ""F""
FROM
  numerator,
  denominator;
","### **Detailed Explanation of the ANOVA Calculation Process**

This document outlines a comprehensive approach to performing a one-way Analysis of Variance (ANOVA) on gene expression data, specifically examining the TP53 gene across different mutation types in the TCGA-BRCA project. The goal is to test the null hypothesis that **the mean gene expression levels are equal across different mutation types**.

#### **Objective**

- **Null Hypothesis (\( H_0 \))**: All group means are equal (\( \mu_1 = \mu_2 = \dots = \mu_k \)).
- **Alternative Hypothesis (\( H_A \))**: At least one group mean is different.

#### **Data Preparation**

1. **Select the Cohort**:
   - **Expression Data**: Extract samples with non-null and positive normalized expression levels of the TP53 gene.
   - **Mutation Data**: Identify mutation types for the TP53 gene in the same samples.
   - **Merge Datasets**: Assign each expression sample to its corresponding mutation type.

2. **Handle Missing Data**:
   - Exclude samples with missing expression or mutation data.
   - Ensure each sample is uniquely assigned to a mutation type.

#### **Calculation Steps**

1. **Compute Log-Transformed Expression Values**:
   - Apply a logarithmic transformation to normalize the distribution of expression levels:
     \[
     \text{log\_expression}_i = \log_{10}(\text{normalized\_count}_i)
     \]
     - Where \( \text{normalized\_count}_i \) is the expression count for sample \( i \).

2. **Calculate the Grand Mean (\( \bar{X} \))**:
   - Compute the overall mean of the log-transformed expression values across all samples:
     \[
     \bar{X} = \frac{1}{N} \sum_{i=1}^{N} X_i
     \]
     - \( N \) is the total number of samples.
     - \( X_i \) is the log-transformed expression value for sample \( i \).

3. **Calculate Group Means (\( \bar{X}_j \))**:
   - For each mutation type (group \( j \)), calculate the mean expression:
     \[
     \bar{X}_j = \frac{1}{n_j} \sum_{i=1}^{n_j} X_{ij}
     \]
     - \( n_j \) is the number of samples in group \( j \).
     - \( X_{ij} \) is the expression value for sample \( i \) in group \( j \).

4. **Compute the Sum of Squares Between Groups (SSB)**:
   - Measure the variation between group means and the grand mean, weighted by group sizes:
     \[
     \text{SSB} = \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2
     \]
     - \( k \) is the total number of groups.

5. **Compute the Sum of Squares Within Groups (SSW)**:
   - Measure the variation within each group:
     \[
     \text{SSW} = \sum_{j=1}^{k} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_j)^2
     \]

6. **Calculate Degrees of Freedom**:
   - Between Groups:
     \[
     \text{df}_{\text{Between}} = k - 1
     \]
   - Within Groups:
     \[
     \text{df}_{\text{Within}} = N - k
     \]

7. **Compute Mean Squares**:
   - Mean Square Between Groups (MSB):
     \[
     \text{MSB} = \frac{\text{SSB}}{\text{df}_{\text{Between}}}
     \]
   - Mean Square Within Groups (MSW):
     \[
     \text{MSW} = \frac{\text{SSW}}{\text{df}_{\text{Within}}}
     \]

8. **Calculate the F-Statistic**:
   - The F-statistic tests whether the group means are significantly different:
     \[
     F = \frac{\text{MSB}}{\text{MSW}}
     \]

9. **Interpret the Results**:
   - Compare the calculated F-statistic to the critical value from the F-distribution with \(\text{df}_{\text{Between}}\) and \(\text{df}_{\text{Within}}\) degrees of freedom.
   - Determine the p-value associated with the F-statistic.
   - **If** \( F \) is greater than the critical value **or** the p-value is less than the significance level (e.g., 0.05), **reject** the null hypothesis.

#### **Important Considerations**

- **Weighting by Sample Size**: When calculating SSB, it's crucial to weight the squared differences by the number of samples in each group (\( n_j \)) to account for varying group sizes.
- **Degrees of Freedom**: Correctly calculating degrees of freedom is essential for accurate MSB, MSW, and F-statistic computations.
- **Assumptions of ANOVA**:
  - **Independence**: Samples are independent of each other.
  - **Normality**: The distribution of residuals is approximately normal.
  - **Homogeneity of Variances**: The variances within each group are approximately equal.

#### **Mathematical Formulas Summary**

1. **Grand Mean**:
   \[
   \bar{X} = \frac{1}{N} \sum_{i=1}^{N} X_i
   \]

2. **Group Means**:
   \[
   \bar{X}_j = \frac{1}{n_j} \sum_{i=1}^{n_j} X_{ij}
   \]

3. **Sum of Squares Between (SSB)**:
   \[
   \text{SSB} = \sum_{j=1}^{k} n_j (\bar{X}_j - \bar{X})^2
   \]

4. **Sum of Squares Within (SSW)**:
   \[
   \text{SSW} = \sum_{j=1}^{k} \sum_{i=1}^{n_j} (X_{ij} - \bar{X}_j)^2
   \]

5. **Degrees of Freedom**:
   - Between Groups:
     \[
     \text{df}_{\text{Between}} = k - 1
     \]
   - Within Groups:
     \[
     \text{df}_{\text{Within}} = N - k
     \]

6. **Mean Squares**:
   - Between Groups:
     \[
     \text{MSB} = \frac{\text{SSB}}{\text{df}_{\text{Between}}}
     \]
   - Within Groups:
     \[
     \text{MSW} = \frac{\text{SSW}}{\text{df}_{\text{Within}}}
     \]

7. **F-Statistic**:
   \[
   F = \frac{\text{MSB}}{\text{MSW}}
   \]

#### **Avoiding Common Mistakes**

- **Ignoring Group Sizes**: Do not overlook the importance of \( n_j \) when calculating SSB. Each group's influence on the between-group variability should be proportional to its size.
- **Incorrect Variance Calculations**: Avoid averaging group variances without considering their degrees of freedom. The SSW should sum all individual squared deviations.
- **Degrees of Freedom Miscalculations**: Ensure that the degrees of freedom for both between and within groups are accurately computed, as they directly impact the MSB, MSW, and F-statistic.
- **Data Integrity**:
  - Verify that each sample is uniquely assigned to one group.
  - Check for and address any missing or duplicated data.

#### **Interpretation Guidelines**

- **High F-Statistic**: Suggests significant differences between group means.
- **P-Value**:
  - **Low p-value** (typically < 0.05): Reject the null hypothesis.
  - **High p-value**: Fail to reject the null hypothesis.
- **Post-Hoc Analysis**: If the null hypothesis is rejected, consider conducting post-hoc tests (e.g., Tukey's HSD) to identify which specific groups differ.

#### **Conclusion**

By meticulously following the outlined steps and carefully applying the mathematical formulas, one can accurately perform ANOVA to assess the effect of different mutation types on gene expression levels. This approach ensures the validity of statistical conclusions and prevents errors commonly made in incorrect analyses.

#### **Additional Notes**

- **Data Transformation**: The logarithmic transformation of expression data helps stabilize variance and meet ANOVA assumptions.
- **Assumption Checks**: Before finalizing results, perform tests for normality (e.g., Shapiro-Wilk test) and homogeneity of variances (e.g., Levene's test).",snowflake
291,sf_bq152,TCGA_HG38_DATA_V0,"For breast cancer cases (TCGA-BRCA) from Release 23 of the active GDC archive, identify and categorize copy number variations (CNVs) across all cytobands on every chromosome. For each cytoband and each case, determine the overlap between the cytoband region and the case's copy number segments, and compute the overlap-weighted average copy number for that cytoband in the case, rounding to the nearest whole number. Classify the rounded copy number into CNV types as follows: homozygous deletions (0), heterozygous deletions (1), normal diploid state (2), gains (3), and amplifications (greater than 3). For each cytoband, provide its name and start/end positions, and calculate the frequency of each CNV type across all cases as a percentage of the total number of cases, rounded to two decimal places.",,,snowflake
292,sf_bq155,TCGA_HG38_DATA_V0,"In the TCGA-BRCA cohort of patients who are 80 years old or younger at diagnosis and have a pathological stage of Stage I, Stage II, or Stage IIA, calculate the t-statistic derived from the Pearson correlation between the log10-transformed average RNA-Seq expression levels (using HTSeq__Counts + 1) of the gene SNORA31 and the average microRNA-Seq expression levels of all unique microRNAs, only considering pairs with more than 25 samples and where the absolute Pearson correlation coefficient is between 0.3 and 1.0","WITH cohort AS (
    SELECT ""case_barcode""
    FROM ""TCGA_HG38_DATA_V0"".""TCGA_BIOCLIN_V0"".""CLINICAL""
    WHERE ""project_short_name"" = 'TCGA-BRCA'
        AND ""age_at_diagnosis"" <= 80
        AND ""pathologic_stage"" IN ('Stage I', 'Stage II', 'Stage IIA')
),
table1 AS (
    SELECT
        ""symbol"",
        ""data"" AS ""rnkdata"",
        ""ParticipantBarcode""
    FROM (
        SELECT
            ""gene_name"" AS ""symbol"", 
            AVG(LOG(10, ""HTSeq__Counts"" + 1)) AS ""data"",
            ""case_barcode"" AS ""ParticipantBarcode""
        FROM ""TCGA_HG38_DATA_V0"".""TCGA_HG38_DATA_V0"".""RNASEQ_GENE_EXPRESSION""
        WHERE ""case_barcode"" IN (SELECT ""case_barcode"" FROM cohort)
            AND ""gene_name"" = 'SNORA31'
            AND ""HTSeq__Counts"" IS NOT NULL
        GROUP BY
            ""ParticipantBarcode"", ""symbol""
    )
),
table2 AS (
    SELECT
        ""symbol"",
        ""data"" AS ""rnkdata"",
        ""ParticipantBarcode""
    FROM (
        SELECT
            ""mirna_id"" AS ""symbol"", 
            AVG(""reads_per_million_miRNA_mapped"") AS ""data"",
            ""case_barcode"" AS ""ParticipantBarcode""
        FROM ""TCGA_HG38_DATA_V0"".""TCGA_HG38_DATA_V0"".""MIRNASEQ_EXPRESSION""
        WHERE ""case_barcode"" IN (SELECT ""case_barcode"" FROM cohort)
            AND ""mirna_id"" IS NOT NULL
            AND ""reads_per_million_miRNA_mapped"" IS NOT NULL
        GROUP BY
            ""ParticipantBarcode"", ""symbol""
    )
),
summ_table AS (
    SELECT 
        n1.""symbol"" AS ""symbol1"",
        n2.""symbol"" AS ""symbol2"",
        COUNT(n1.""ParticipantBarcode"") AS ""n"",
        CORR(n1.""rnkdata"", n2.""rnkdata"") AS ""correlation""
    FROM
        table1 AS n1
    INNER JOIN
        table2 AS n2
    ON
        n1.""ParticipantBarcode"" = n2.""ParticipantBarcode""
    GROUP BY
        ""symbol1"", ""symbol2""
)

SELECT 
    ""symbol1"", 
    ""symbol2"", 
    ABS(""correlation"") * SQRT(( ""n"" - 2 ) / (1 - ""correlation"" * ""correlation"")) AS ""t""
FROM 
    summ_table
WHERE 
    ""n"" > 25 
    AND ABS(""correlation"") >= 0.3 
    AND ABS(""correlation"") < 1.0;
",,snowflake
293,sf_bq141,TCGA_HG38_DATA_V0,"Using the TCGA-KIRP dataset, select patients from the 'TCGA_bioclin_v0.Clinical' table who have a non-null clinical_stage and a disease_code of 'KIRP.' Retrieve their gene expression data from the 'TCGA_hg38_data_v0.RNAseq_Gene_Expression' table for the genes 'MT-CO3,' 'MT-CO1,' and 'MT-CO2,' and randomly split the patients into a training set (90%) and a test set (10%) based on their case_barcode via the FARM_FINGERPRINT method. For each clinical stage in the training set, calculate the average HTSeq__FPKM_UQ expression of the three genes. For each patient in the test set, compute the Euclidean distance between the patient’s expression values and the stage-specific averages, and assign that patient to the clinical stage whose average is closest. Finally, output the case_barcode and the predicted clinical stage.",,,snowflake
294,bq046,TCGA_bioclin_v0,"Find case barcodes and their corresponding GDC file URLs for female patients aged 30 or younger diagnosed with breast cancer (BRCA) in TCGA database. The query should first identify patients from the Annotations table where entity_type is ""Patient"" and either category is ""History of unacceptable prior treatment related to a prior/other malignancy"" or classification is ""Redaction"". Also identify patients from the Clinical table with disease_code ""BRCA"", age_at_diagnosis less than or equal to 30, and gender ""FEMALE"". Perform a FULL JOIN between these two sets, but only keep patients where both categoryName and classificationName are NULL in the final result. Use GDC metadata from archive release 14 (specifically rel14_caseData, rel14_fileData_current, and rel14_GDCfileID_to_GCSurl_NEW tables) to get the corresponding file URLs. ",,,snowflake
295,sf_bq153,PANCANCER_ATLAS_1,"Calculate, for each histology type specified in the 'icd_o_3_histology' field (excluding those enclosed in square brackets), the average of the per-patient average log10(normalized_count + 1) expression levels of the IGF2 gene among LGG patients with valid IGF2 expression data. Match gene expression and clinical data using the ParticipantBarcode field.","WITH
    table1 AS (
        SELECT 
            ""Symbol"" AS ""symbol"", 
            AVG(LOG(10, ""normalized_count"" + 1)) AS ""data"", 
            ""ParticipantBarcode""
        FROM 
            PANCANCER_ATLAS_1.PANCANCER_ATLAS_FILTERED.EBPP_ADJUSTPANCAN_ILLUMINAHISEQ_RNASEQV2_GENEXP_FILTERED
        WHERE 
            ""Study"" = 'LGG' 
            AND ""Symbol"" = 'IGF2' 
            AND ""normalized_count"" IS NOT NULL
        GROUP BY 
            ""ParticipantBarcode"", ""symbol""
    ),
    table2 AS (
        SELECT
            ""symbol"",
            ""avgdata"" AS ""data"",
            ""ParticipantBarcode""
        FROM (
            SELECT
                'icd_o_3_histology' AS ""symbol"", 
                ""icd_o_3_histology"" AS ""avgdata"",
                ""bcr_patient_barcode"" AS ""ParticipantBarcode""
            FROM 
                PANCANCER_ATLAS_1.PANCANCER_ATLAS_FILTERED.CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED
            WHERE 
                ""acronym"" = 'LGG' 
                AND ""icd_o_3_histology"" IS NOT NULL  
                AND NOT REGEXP_LIKE(""icd_o_3_histology"", '^(\\[.*\\]$)')
        )
    ),
    table_data AS (
        SELECT 
            n1.""data"" AS ""data1"",
            n2.""data"" AS ""data2"",
            n1.""ParticipantBarcode""
        FROM 
            table1 AS n1
        INNER JOIN 
            table2 AS n2
        ON 
            n1.""ParticipantBarcode"" = n2.""ParticipantBarcode""
    ) 

SELECT 
    ""data2"" AS ""Histology_Type"", 
    AVG(""data1"") AS ""Average_Log_Expression""
FROM 
    table_data
GROUP BY 
    ""data2"";
",,snowflake
296,sf_bq154,PANCANCER_ATLAS_1,"Calculate the Kruskal-Wallis H-score among groups of LGG patients for IGF2 gene expression, where each patient’s IGF2 expression is determined by applying log10(normalized_count + 1) and then averaging across samples. Group the patients by ICD-O-3 histology codes, exclude any codes fully enclosed in square brackets, only include groups with more than one patient, and ensure that normalized count is not null. Finally, return the total number of groups, the total number of samples, and the Kruskal-Wallis H-score in descending order.",,"# Regulome Explorer Kruskal-Wallis test for numerical and categorical data

CCheck out more notebooks at our ['Regulome Explorer Repository'](https://github.com/isb-cgc/Community-Notebooks/tree/master/RegulomeExplorer)!

In this notebook we describe how Regulome Explorer uses Kruskal-Wallis test to compute the significance of associations between a numerical feature (Gene expression, Somatic copy number, etc.) and a categorical feature. Details of the Kruskal-Wallist test can be found in the following link: [https://en.wikipedia.org/wiki/Kruskal%E2%80%93Wallis_one-way_analysis_of_variance](https://en.wikipedia.org/wiki/Kruskal–Wallis_one-way_analysis_of_variance)

To describe the implementation of the test using BigQuery, we will use Gene expresion data of a user defined gene and a user defined clinical feature. This data is read from a BigQuery table in the pancancer-atlas dataset.

## Authenticate with Google (IMPORTANT)

The first step is to authorize access to BigQuery and the Google Cloud. For more information see ['Quick Start Guide to ISB-CGC'](https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/HowToGetStartedonISB-CGC.html) and alternative authentication methods can be found [here](https://googleapis.github.io/google-cloud-python/latest/core/auth.html).

#### Import Python libraries

In [1]:

```
%matplotlib inline
from google.cloud import bigquery
import numpy as np
import pandas as pd
from scipy import stats
from scipy.stats import mstats
import seaborn as sns
import re_module.bq_functions as regulome
```

## User defined Parameters

The parameters for this experiment are the cancer type, the name of gene for which gene expression data will be obtained, and the clinical feature name. Categorical groups with number of samples smaller than 'MinSampleSize' will be ignored in the test.

In [2]:

```
cancer_type = 'LGG'
gene_name = 'IGF2'
clinical_feature = 'icd_o_3_histology'
MinSampleSize = 26

bqclient = bigquery.Client()
```

## Data from BigQuery tables

**Gene expression data from the BigQuery:** The following query string retrieves the gene expression data of the user specified gene ('gene_name') from the 'Filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered' table available in pancancer-atlas dataset.

In [3]:

```

```

**Clinical data from the BigQuery:** The following string query will retrieve clinical data fromthe 'pancancer-atlas.Filtered.clinical_PANCAN_patient_with_followup_filtered' table available in pancancer-atlas dataset. It is worth noting that some of the values of the clinical feature may be 'indetermined' or 'not-evaluated'; typically these values are inside square brackets. The 'REGEXP_CONTAINS' command is used to avoid using those values in the test.

In [4]:

```

```

The following query combines the two tables based on Participant barcodes. T

In [5]:

```

```

At this point we can take a look at output table

In [6]:

```

 in runQuery ... 
    the results for this query were previously cached 
```

Out[6]:

|      |    data1 |  data2 | ParticipantBarcode |
| ---: | -------: | -----: | -----------------: |
|    1 | 2.359532 | 9382/3 |       TCGA-E1-A7YW |
|    2 | 2.868692 | 9382/3 |       TCGA-FG-7637 |
|    3 | 2.713119 | 9382/3 |       TCGA-TQ-A7RG |
|    4 | 3.064997 | 9382/3 |       TCGA-DB-5280 |
|    5 | 2.554518 | 9382/3 |       TCGA-IK-8125 |
|    6 | 2.799724 | 9382/3 |       TCGA-DU-8163 |
|    7 | 2.800062 | 9382/3 |       TCGA-HT-8018 |
|    8 | 2.558207 | 9382/3 |       TCGA-DU-7019 |
|    9 | 2.793514 | 9382/3 |       TCGA-DU-5852 |

We can use a 'violinplot' to visualize the populations in each category.

In [7]:

```

```

Out[7]:

```
<matplotlib.axes._subplots.AxesSubplot at 0x7f3e4421a5f8>
```

![img](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAXYAAAEQCAYAAACk818iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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%0ABu8AAAAASUVORK5CYII=)

## BigQuery to Compute statistical association

The Kruskal-Wallis score (H) is computed by using the following equation:
$$
H = \frac{(N-1)\sum_{i=1}^{g} n_i (\bar{r_{i}} -\bar{r} )^2 }{ \sum_{i=1}^{g} \sum_{j=1}^{n_i} (r_{ij}-\bar{r})^2   }
$$
where

- $n_{i}$ is the number of observations in category $i$
- $r_{ij}$ is the rank (among all participants) of the gene expression of participant jj that belongs to category $i$
- $N$ is the total number of participants considered in the test
- $\bar{r_i}$ is the average rank of gene expression values for participants in category $i$
- $\bar{r}$ is the average of all $r_{ij}$

To avoid reading that table multiple times, we rearranged the equations above as :
$$
H = (N-1)\frac{ \sum_{i=1}^{g}S_i^2/n_i - (\sum_{i=1}^{g}S_i)^2 / N }{ \sum_{i=1}^{g}Q_i - (\sum_{i=1}^{g}S_i)^2 / N }
$$
Where $S_i = \sum_{j=1}^{n_i}r_{ij}$ and $Q_i = \sum_{j=1}^{n_i}r_{ij}^2$

The following query string computes $S_i$ and $Q_i$:

In [8]:

```

```

The query above ingnores the categories that have a number of participants smaller or equal than 'MinSampleSize'. Moreover, the gene expression is ranked, assigning **average** of ranks to the similar values( https://docs.scipy.org/doc/scipy-0.16.0/reference/generated/scipy.stats.rankdata.html). Finally, The Kruskall-Wallis score ($H$) is computed by the following BigQuery string.

In [9]:

```

 in runQuery ... 
    the results for this query were previously cached 
```

Out[9]:

|      |      |      |      |
| ---: | ---: | ---: | ---: |
|      |      |      |      |

To test our implementation we can use the 'kruskalwallis' function available in python:

In [10]:

```

```",snowflake
297,sf_bq156,PANCANCER_ATLAS_1,"Compute the t-score (rounded to 2 decimals) to compare the difference in mean expression levels of the gene DRG2 between two groups (TP53 mutated vs. non-mutated) in the Lower Grade Glioma (LGG) study, where the expression levels are calculated as the average of log10(normalized_count + 1) for each participant, only considering samples with TP53 mutations that have a 'FILTER' status of 'PASS' in the mutation data, and ignoring any groups with fewer than 10 samples or with zero variance; refer to `t_score.md` for the method of computing the t-score.",,"# Welch's t-test

From Wikipedia, the free encyclopedia

In statistics, Welch's t-test, or unequal variances t-test, is a two-sample location test which is used to test the (null) hypothesis that two populations have equal means. It is named for its creator, Bernard Lewis Welch, and is an adaptation of Student's t-test, and is more reliable when the two samples have unequal variances and possibly unequal sample sizes. These tests are often referred to as ""unpaired"" or ""independent samples"" t-tests, as they are typically applied when the statistical units underlying the two samples being compared are non-overlapping. Given that Welch's t-test has been less popular than Student's t-test and may be less familiar to readers, a more informative name is ""Welch's unequal variances t-test"" — or ""unequal variances t-test"" for brevity.

## Assumptions

Student's t-test assumes that the sample means being compared for two populations are normally distributed, and that the populations have equal variances. Welch's t-test is designed for unequal population variances, but the assumption of normality is maintained. Welch's t-test is an approximate solution to the Behrens–Fisher problem.

## Calculations

Welch's t-test defines the statistic t by the following formula:

$$t=\frac{\triangle\overline{X}}{s_{\triangle\overline{X}}}=\frac{\overline{X}_1-\overline{X}_2}{\sqrt{s^2_{\overline{X}_1}+s^2_{\overline{X}_2}}}$$
$$s_{\overline{X}_i}=\frac{s_i}{\sqrt{N_i}}$$

where $\overline{X}_i$ and $s_{\overline{X}_i}$ are the $i$-th sample mean and its standard error, with $s_i$ denoting the corrected sample standard deviation, and sample size $N_i$. Unlike in Student's t-test, the denominator is not based on a pooled variance estimate.

The degrees of freedom $\nu$ associated with this variance estimate is approximated using the Welch–Satterthwaite equation:

$$\nu\approx\frac{(\frac{s^2_1}{N_1}+\frac{s^2_2}{N_2})^2}{\frac{s^4_1}{N^2_1\nu_1}+\frac{s^4_2}{N^2_2\nu_2}}$$

This expression can be simplified when $N_1=N_2$:
$$\nu\approx\frac{s^4_{\triangle\overline{X}}}{\nu^{-1}_1s^4_{\overline{X}_1}+\nu^{-1}_2s^4_{\overline{X}_2}}$$

Here, $\nu _{i}=N_{i}-1$ is the degrees of freedom associated with the $i$-th variance estimate.

The statistic is approximately from the t-distribution since we have an approximation of the chi-square distribution. This approximation is better done when both $N_{1}$ and $N_2$ are larger than 5.


## Example

The t-score, assuming the distributions of gene expression with and without mutation have unequal variances, is computed by using the following equation:

$$t=\frac{g_y-g_n}{\sqrt{\frac{s^2_y}{N_y}+\frac{s^2_n}{N_n}}}$$

where
- $g_y$ and $g_n$ are mean gene expression of participants with and without mutation;
- $N_y$ and $N_n$ are the number of participants in the group with and without mutation;
- $s^2_y$ and $s^2_n$ are the variance of gene expression for the participants with and without mutation, respectively.
Since the Somatic mutation table contains information of positive somatic mutation only, the averages and variances needed to compute the t-score are computed as a function $S_y=\sum_{i=1}^{N_y}g_i$ and $Q_y=\sum_{i=1}^{N_y}g^2_i$, the sums over the gene expression and squared gene expression for articipants with somatic mutation.

After computing $S_y$ and $Q_y$, we can compute the mean and the variance as:

$$g_y=\frac{S_y}{N_y}$$
$$s^2_y=\frac{1}{N_y-1}[Q_y-\frac{S^2_y}{N_y}]$$

To compute $S_n$ and $Q_n$, we first compute the sums of the gene expression and squeared gene expression using all the samples and then substract $S_y$ and $Q_y$. The following query uses this approach to compute the necessary variances and means, and then computes t-score. The t-score is only computed if the variances are greater than zero and if the number of participants in each group is greater than a user defined threshold (e.g., 10).


## Statistical test

Once $t$ and $\nu$ have been computed, these statistics can be used with the t-distribution to test one of two possible null hypotheses:

- that the two population means are equal, in which a two-tailed test is applied; or
- that one of the population means is greater than or equal to the other, in which a one-tailed test is applied.

The approximate degrees of freedom are real numbers $\nu \in \mathbb {R} ^{+}$ and used as such in statistics-oriented software, whereas they are rounded down to the nearest integer in spreadsheets.

## Advantages and limitations

Welch's t-test is more robust than Student's t-test and maintains type I error rates close to nominal for unequal variances and for unequal sample sizes under normality. Furthermore, the power of Welch's t-test comes close to that of Student's t-test, even when the population variances are equal and sample sizes are balanced. Welch's t-test can be generalized to more than 2-samples, which is more robust than one-way analysis of variance (ANOVA).

It is not recommended to pre-test for equal variances and then choose between Student's t-test or Welch's t-test. Rather, Welch's t-test can be applied directly and without any substantial disadvantages to Student's t-test as noted above. Welch's t-test remains robust for skewed distributions and large sample sizes. Reliability decreases for skewed distributions and smaller samples, where one could possibly perform Welch's t-test",snowflake
298,sf_bq157,PANCANCER_ATLAS_1,"Please compute the T-score to determine the statistical difference in the expression of the DRG2 gene between LGG patients with and without TP53 mutation: for each patient, calculate the average of log10(normalized_count + 1) of DRG2 expression across all their samples, using only samples present in the `MC3_MAF_V5_one_per_tumor_sample` table for the LGG study; identify patients with TP53 mutations from this table where `Hugo_Symbol` is 'TP53' and `FILTER` is 'PASS'; then perform a T-test comparing the mean averaged log-transformed DRG2 expression between patients with and without TP53 mutation.",,"# Regulome Explorer T test for numerical and binary data

Check out more notebooks at our ['Regulome Explorer Repository'](https://github.com/isb-cgc/Community-Notebooks/tree/master/RegulomeExplorer)!

In this notebook we describe the implementation of a student's t test to compute the significance of associations between a numerical feature (Gene expression, Somatic copy number, etc.) and Somatic mutation data which can be categorized into two groups according to the presence or absence of somatic mutations in a user defined gene. Detail of the test can be found the following link: [https://en.wikipedia.org/wiki/Welch%27s_t-test](https://en.wikipedia.org/wiki/Welch's_t-test)

To describe the implementation of the test using BigQuery, we will use Gene expresion data of a user defined gene and the precense or absence of somatic mutation of a user defined gene. This data is read from a BigQuery table in the pancancer-atlas dataset.

## Authenticate with Google (IMPORTANT)

The first step is to authorize access to BigQuery and the Google Cloud. For more information see ['Quick Start Guide to ISB-CGC'](https://isb-cancer-genomics-cloud.readthedocs.io/en/latest/sections/HowToGetStartedonISB-CGC.html) and alternative authentication methods can be found [here](https://googleapis.github.io/google-cloud-python/latest/core/auth.html).

#### Import Python libraries

In [1]:

```

```

## User defined Parameters

The parameters for this experiment are the cancer type, the name of gene for which gene expression data will be obtained, and the clinical feature name. Categorical groups with number of samples smaller than 'MinSampleSize' will be ignored in the test.

In [2]:

```

```

## Data from BigQuery tables

The first step is to select all participants in the selected study with a least one mutation.

In [3]:

```

```

**Gene expression data from the BigQuery:** The following query string retrieves the gene expression data of the user specified gene ('gene_expre') from the 'Filtered.EBpp_AdjustPANCAN_IlluminaHiSeq_RNASeqV2_genExp_filtered' table available in pancancer-atlas dataset. The gene expression of a participant is computed as the average gene expression of the tumor samples of the participant. Moreover, we are considering only samples with a least somatic mutation.

In [4]:

```

```

**Somatic mutation data from the BigQuery:** The following string query will retrieve a table with patients with at least one Somatic mutation in the user defined gene ('mutation_name'). This information is extracted from the 'pancancer-atlas.Filtered.MC3_MAF_V5_one_per_tumor_sample' table, available in pancancer-atlas dataset. Notice that we only use samples in which FILTER = 'PASS'.

In [5]:

```

```

At this point we can take a look at the combined data (Gene expression and Somatic mutation) by using a simple LEFT JOIN command. Participants with and without somatic mutations are labeled as 'YES' and 'NO' respectively.

In [6]:

```

 in runQuery ... 
    the results for this query were previously cached 
```

Out[6]:

|      |    data1 | data2 | ParticipantBarcode |
| ---: | -------: | ----: | -----------------: |
|    1 | 2.986545 |    NO |       TCGA-VW-A7QS |
|    2 | 2.398801 |   YES |       TCGA-DB-5280 |
|    3 | 2.513401 |   YES |       TCGA-HT-7611 |
|    4 | 3.063967 |    NO |       TCGA-DH-5141 |
|    5 | 2.549122 |    NO |       TCGA-HT-A616 |
|    6 | 2.769918 |    NO |       TCGA-HT-7694 |
|    7 | 2.527377 |   YES |       TCGA-CS-6290 |
|    8 | 2.832638 |    NO |       TCGA-HW-7495 |
|    9 | 2.373629 |   YES |       TCGA-WY-A85C |

To visualize the gene expression data in both groups with or without somatic mutation, we can use a 'violinplot' plot:

In [8]:

```

```

Out[8]:

```
<matplotlib.axes._subplots.AxesSubplot at 0x1a256eb2e8>
```

![img](data:image/png;base64,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)

## BigQuery to Compute statistical association

The T-score (T), assuming the distributions of gene expression with and without mutation have unequal variances, is computed by using the following equation:
$$
T = \frac{ \bar{g}_y - \bar{g}_n }{\sqrt{ \frac{s_y^2}{N_y} + \frac{s_n^2}{N_n}  }   }
$$
where

- $\bar{g}_y$ and $\bar{g}_n$ are mean gene expression of participants with and without mutation.
- $N_y$ and $N_n$ are the number of participants in the group with and without mutation.
- $s_y^2$ and $s_n^2$ are the variance of gene expression for the participants with and without mutation, respectively.

Since the Somatic mutation table contains information of positive somatic mutation only, the averages and variances needed to compute the $T$ score are computed as a function $S_y=\sum_{i=1}^{N_y}{g_i}$ and $Q_y=\sum_{i=1}^{N_y}{g_i^2}$, the summs over the gene expression and squared gene expression for articipants with somatic mutation. The following query string computes $S_y$ and $Q_y$:

In [9]:

```

```

After computing $S_y$ and $Q_y$ we can compute the mean and the variance as :
$$
\bar{g}_y =\frac{S_y}{N_y} \\
s_y^2 = (N_y-1)^{-1} \left[ Q_y - \frac{S_y^2}{ N_y} \right]
$$
To compute $S_n$ and $Q_n$, we first compute the sums of the gene expression and squeared gene expression using all the samples and then substract $S_y$ and $Q_y$. The following query uses this approach to compute the necessary variances and means, and then computes $T$ score. The $T$ score is only computed if the variances are greater than zero and if the number of participants in each group is greater than a user defined threshold.

In [10]:

```

 in runQuery ... 
    the results for this query were previously cached 
```

Out[10]:

|      |   Ny |   Nn |    avg_y |    avg_n |    tscore |
| ---: | ---: | ---: | -------: | -------: | --------: |
|    0 |  248 |  261 | 2.495134 | 2.894686 | 24.426213 |

To test our implementation we can use the 'ttest_ins' function available in python:

In [12]:

```

          DRG2      
          mean count
TP53                
NO    2.894686   261
YES   2.495134   248
Ttest_indResult(statistic=24.426213134587776, pvalue=4.38554327085175e-84)
```",snowflake
299,sf_bq158,PANCANCER_ATLAS_1,Which top five histological types of breast cancer (BRCA) in the PanCancer Atlas exhibit the highest percentage of CDH1 gene mutations?,"WITH
    table1 AS (
        SELECT
            ""histological_type"" AS ""data1"",
            ""bcr_patient_barcode"" AS ""ParticipantBarcode""
        FROM 
            ""PANCANCER_ATLAS_1"".""PANCANCER_ATLAS_FILTERED"".""CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED""
        WHERE 
            ""acronym"" = 'BRCA' 
            AND ""histological_type"" IS NOT NULL      
    ),
    table2 AS (
        SELECT
            ""Hugo_Symbol"" AS ""symbol"", 
            ""ParticipantBarcode""
        FROM 
            ""PANCANCER_ATLAS_1"".""PANCANCER_ATLAS_FILTERED"".""MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE""
        WHERE 
            ""Study"" = 'BRCA' 
            AND ""Hugo_Symbol"" = 'CDH1'
            AND ""FILTER"" = 'PASS'  
        GROUP BY
            ""ParticipantBarcode"", ""symbol""
    ),
    summ_table AS (
        SELECT 
            n1.""data1"",
            CASE 
                WHEN n2.""ParticipantBarcode"" IS NULL THEN 'NO' 
                ELSE 'YES' 
            END AS ""data2"",
            COUNT(*) AS ""Nij""
        FROM
            table1 AS n1
        LEFT JOIN
            table2 AS n2 
            ON n1.""ParticipantBarcode"" = n2.""ParticipantBarcode""
        GROUP BY
            n1.""data1"", ""data2""
    ),
    percentages AS (
        SELECT
            ""data1"",
            SUM(CASE WHEN ""data2"" = 'YES' THEN ""Nij"" ELSE 0 END) AS ""mutation_count"",
            SUM(""Nij"") AS ""total"",
            SUM(CASE WHEN ""data2"" = 'YES' THEN ""Nij"" ELSE 0 END) / SUM(""Nij"") AS ""mutation_percentage""
        FROM 
            summ_table
        GROUP BY 
            ""data1""
    )
SELECT 
    ""data1"" AS ""Histological_Type""
FROM 
    percentages
ORDER BY 
    ""mutation_percentage"" DESC
LIMIT 5;
",,snowflake
300,sf_bq159,PANCANCER_ATLAS_1,Calculate the chi-square value to assess the association between histological types and the presence of CDH1 gene mutations in BRCA patients using data from the PanCancer Atlas. Focus on patients with known histological types and consider only reliable mutation entries.  Exclude any histological types or mutation statuses with marginal totals less than or equal to 10. Match clinical and mutation data using ParticipantBarcode,"WITH
    table1 AS (
        SELECT
            ""symbol"",
            ""avgdata"" AS ""data"",
            ""ParticipantBarcode""
        FROM (
            SELECT
                'histological_type' AS ""symbol"", 
                ""histological_type"" AS ""avgdata"",
                ""bcr_patient_barcode"" AS ""ParticipantBarcode""
            FROM 
                ""PANCANCER_ATLAS_1"".""PANCANCER_ATLAS_FILTERED"".""CLINICAL_PANCAN_PATIENT_WITH_FOLLOWUP_FILTERED""
            WHERE 
                ""acronym"" = 'BRCA' 
                AND ""histological_type"" IS NOT NULL      
        )
    ),
    table2 AS (
        SELECT
            ""symbol"",
            ""ParticipantBarcode""
        FROM (
            SELECT
                ""Hugo_Symbol"" AS ""symbol"", 
                ""ParticipantBarcode"" AS ""ParticipantBarcode""
            FROM 
                ""PANCANCER_ATLAS_1"".""PANCANCER_ATLAS_FILTERED"".""MC3_MAF_V5_ONE_PER_TUMOR_SAMPLE""
            WHERE 
                ""Study"" = 'BRCA' 
                AND ""Hugo_Symbol"" = 'CDH1'
                AND ""FILTER"" = 'PASS'  
            GROUP BY
                ""ParticipantBarcode"", ""symbol""
        )
    ),
    summ_table AS (
        SELECT 
            n1.""data"" AS ""data1"",
            CASE 
                WHEN n2.""ParticipantBarcode"" IS NULL THEN 'NO' 
                ELSE 'YES' 
            END AS ""data2"",
            COUNT(*) AS ""Nij""
        FROM
            table1 AS n1
        LEFT JOIN
            table2 AS n2 
            ON n1.""ParticipantBarcode"" = n2.""ParticipantBarcode""
        GROUP BY
            n1.""data"", ""data2""
    ),
    expected_table AS (
        SELECT 
            ""data1"", 
            ""data2""
        FROM (     
            SELECT 
                ""data1"", 
                SUM(""Nij"") AS ""Ni""   
            FROM 
                summ_table
            GROUP BY 
                ""data1""
        ) AS Ni_table
        CROSS JOIN ( 
            SELECT 
                ""data2"", 
                SUM(""Nij"") AS ""Nj""
            FROM 
                summ_table
            GROUP BY 
                ""data2""
        ) AS Nj_table
        WHERE 
            Ni_table.""Ni"" > 10 
            AND Nj_table.""Nj"" > 10
    ),
    contingency_table AS (
        SELECT
            T1.""data1"",
            T1.""data2"",
            COALESCE(T2.""Nij"", 0) AS ""Nij"",
            (SUM(T2.""Nij"") OVER (PARTITION BY T1.""data1"")) * 
            (SUM(T2.""Nij"") OVER (PARTITION BY T1.""data2"")) / 
            SUM(T2.""Nij"") OVER () AS ""E_nij""
        FROM
            expected_table AS T1
        LEFT JOIN
            summ_table AS T2
        ON 
            T1.""data1"" = T2.""data1"" 
            AND T1.""data2"" = T2.""data2""
    )
SELECT
    SUM( ( ""Nij"" - ""E_nij"" ) * ( ""Nij"" - ""E_nij"" ) / ""E_nij"" ) AS ""Chi2""
FROM 
    contingency_table;
",,snowflake
301,bq161,pancancer_atlas_2,"Calculate the net difference between the number of pancreatic adenocarcinoma (PAAD) patients in TCGA's dataset who are confirmed to have mutations in both KRAS and TP53 genes, and those without mutations in either gene. Utilize patient clinical and follow-up data alongside genomic mutation details from TCGA’s cancer genomics database, focusing specifically on PAAD studies where the mutations have passed quality filters.","WITH
barcodes AS (
   SELECT bcr_patient_barcode AS ParticipantBarcode
   FROM `isb-cgc-bq.pancancer_atlas.Filtered_clinical_PANCAN_patient_with_followup`
   WHERE acronym = 'PAAD'
)
,table1 AS (
SELECT
   t1.ParticipantBarcode,
   IF( t2.ParticipantBarcode is null, 'NO', 'YES') as data
FROM
   barcodes AS t1
LEFT JOIN
   (
   SELECT
      ParticipantBarcode AS ParticipantBarcode
   FROM `isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample`
   WHERE Study = 'PAAD' AND Hugo_Symbol = 'KRAS'
         AND FILTER = 'PASS'
   GROUP BY ParticipantBarcode
   ) AS t2
ON t1.ParticipantBarcode = t2.ParticipantBarcode
)
,table2 AS (
SELECT
   t1.ParticipantBarcode,
   IF( t2.ParticipantBarcode is null, 'NO', 'YES') as data
FROM
   barcodes AS t1
LEFT JOIN
   (
   SELECT
      ParticipantBarcode AS ParticipantBarcode
   FROM `isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample`
   WHERE Study = 'PAAD' AND Hugo_Symbol = 'TP53'
         AND FILTER = 'PASS'
   GROUP BY ParticipantBarcode
   ) AS t2
ON t1.ParticipantBarcode = t2.ParticipantBarcode
),

INFO AS (
SELECT
   n1.data as data1,
   n2.data as data2,
   COUNT(*) as Nij
FROM
   table1 AS n1
INNER JOIN
   table2 AS n2
ON
   n1.ParticipantBarcode = n2.ParticipantBarcode
GROUP BY
  data1, data2
)

SELECT 
(SELECT Nij FROM INFO WHERE data1=""YES"" AND data2=""YES"")
-
(SELECT Nij FROM INFO WHERE data1=""NO"" AND data2=""NO"")

",,snowflake
302,bq151,pancancer_atlas_2,"Using TCGA dataset, calculate the chi-squared statistic to evaluate the association between KRAS and TP53 gene mutations in patients diagnosed with pancreatic adenocarcinoma (PAAD). Incorporate clinical follow-up data and high-quality mutation annotations to accurately determine the frequency of patients with co-occurring KRAS and TP53 mutations compared to those with each mutation occurring independently. Ensure that patient records are meticulously matched based on unique identifiers to maintain data integrity. This analysis aims to identify and quantify potential correlations between KRAS and TP53 genetic alterations within the PAAD patient population.","WITH
barcodes AS (
   SELECT bcr_patient_barcode AS ParticipantBarcode
   FROM isb-cgc-bq.pancancer_atlas.Filtered_clinical_PANCAN_patient_with_followup
   WHERE acronym = 'PAAD'
),
table1 AS (
SELECT
   t1.ParticipantBarcode,
   IF(t2.ParticipantBarcode IS NULL, 'NO', 'YES') AS data
FROM
   barcodes AS t1
LEFT JOIN
   (
   SELECT
      ParticipantBarcode AS ParticipantBarcode
   FROM isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample
   WHERE Study = 'PAAD' AND Hugo_Symbol = 'KRAS'
         AND FILTER = 'PASS'
   GROUP BY ParticipantBarcode
   ) AS t2
ON t1.ParticipantBarcode = t2.ParticipantBarcode
),
table2 AS (
SELECT
   t1.ParticipantBarcode,
   IF(t2.ParticipantBarcode IS NULL, 'NO', 'YES') AS data
FROM
   barcodes AS t1
LEFT JOIN
   (
   SELECT
      ParticipantBarcode AS ParticipantBarcode
   FROM isb-cgc-bq.pancancer_atlas.Filtered_MC3_MAF_V5_one_per_tumor_sample
   WHERE Study = 'PAAD' AND Hugo_Symbol = 'TP53'
         AND FILTER = 'PASS'
   GROUP BY ParticipantBarcode
   ) AS t2
ON t1.ParticipantBarcode = t2.ParticipantBarcode
),
summ_table AS (
SELECT
   n1.data AS data1,
   n2.data AS data2,
   COUNT(*) AS Nij
FROM
   table1 AS n1
INNER JOIN
   table2 AS n2
ON
   n1.ParticipantBarcode = n2.ParticipantBarcode
GROUP BY
  data1, data2
),
contingency_table AS (
SELECT
  MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) AS a,
  MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0)) AS b,
  MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0)) AS c,
  MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0)) AS d,
  (MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0))) AS row1_total,
  (MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0))) AS row2_total,
  (MAX(IF((data1 = 'YES') AND (data2 = 'YES'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'YES'), Nij, 0))) AS col1_total,
  (MAX(IF((data1 = 'YES') AND (data2 = 'NO'), Nij, 0)) + MAX(IF((data1 = 'NO') AND (data2 = 'NO'), Nij, 0))) AS col2_total,
  SUM(Nij) AS grand_total
FROM summ_table
)
SELECT
  POWER((a - (row1_total * col1_total) / grand_total), 2) / ((row1_total * col1_total) / grand_total) +
  POWER((b - (row1_total * col2_total) / grand_total), 2) / ((row1_total * col2_total) / grand_total) +
  POWER((c - (row2_total * col1_total) / grand_total), 2) / ((row2_total * col1_total) / grand_total) +
  POWER((d - (row2_total * col2_total) / grand_total), 2) / ((row2_total * col2_total) / grand_total) AS chi_square_statistic
FROM contingency_table
WHERE a IS NOT NULL AND b IS NOT NULL AND c IS NOT NULL AND d IS NOT NULL;
",,snowflake
303,bq162,HTAN_1,"Based on the 5th revision (r5) of the HTAN data, list the imaging assay types available at the HTAN WUSTL center that have Level2 data and any associated higher-level data (Level3, Level4) derived from them through 'entityId' relationships in the 'id_provenance_r5' table; exclude any records where the 'Component' is NULL or contains 'Auxiliary' or 'OtherAssay'; for each imaging assay type, provide the available data levels (Level2, Level3, Level4), and do not include Level1 data or Electron Microscopy assay types.",,,snowflake
304,sf_bq163,HTAN_2,Which 20 genes in the HTAN scRNAseq MSK-SCLC combined samples dataset show the greatest difference in average X_value expression between female and male epithelial cells specifically in cluster 41 of 74-year-old human stage patients? Please calculate the difference by subtracting male average X_value from female average X_value for each gene,,,snowflake
305,sf_bq164,HTAN_2,"Consolidate metadata from spatial transcriptomics and scRNAseq datasets—including levels 1 through 4 and auxiliary files—for the run ID 'HT264P1-S1H2Fc2U1Z1Bs1-H2Bs2-Test'. Include Filename, HTAN Parent Biospecimen ID, Component, File Format, Entity ID, and Run ID.",,,snowflake
306,sf_bq166,TCGA_MITELMAN,"Using segment-level copy number data from the copy_number_segment_allelic_hg38_gdc_r23 dataset restricted to 'TCGA-KIRC' samples, merge these segments with the cytogenetic band definitions in 'CytoBands_hg38' to identify each sample’s maximum copy number per cytoband. Classify these maximum copy numbers into amplifications (>3), gains (=3), homozygous deletions (=0), heterozygous deletions (=1), or normal (=2), then calculate the frequency of each subtype out of the total number of distinct cases, and finally present these frequencies as percentages sorted by chromosome and cytoband.","WITH copy AS (
  SELECT 
    ""case_barcode"", 
    ""chromosome"", 
    ""start_pos"", 
    ""end_pos"", 
    MAX(""copy_number"") AS ""copy_number""
  FROM 
    ""TCGA_MITELMAN"".""TCGA_VERSIONED"".""COPY_NUMBER_SEGMENT_ALLELIC_HG38_GDC_R23"" 
  WHERE  
    ""project_short_name"" = 'TCGA-KIRC'
  GROUP BY 
    ""case_barcode"", 
    ""chromosome"", 
    ""start_pos"", 
    ""end_pos""
),
total_cases AS (
  SELECT COUNT(DISTINCT ""case_barcode"") AS ""total""
  FROM copy 
),
cytob AS (
  SELECT 
    ""chromosome"", 
    ""cytoband_name"", 
    ""hg38_start"", 
    ""hg38_stop""
  FROM 
    ""TCGA_MITELMAN"".""PROD"".""CYTOBANDS_HG38""
),
joined AS (
  SELECT 
    cytob.""chromosome"", 
    cytob.""cytoband_name"", 
    cytob.""hg38_start"", 
    cytob.""hg38_stop"",
    copy.""case_barcode"",
    copy.""copy_number""  
  FROM 
    copy
  LEFT JOIN cytob
    ON cytob.""chromosome"" = copy.""chromosome"" 
  WHERE 
    (cytob.""hg38_start"" >= copy.""start_pos"" AND copy.""end_pos"" >= cytob.""hg38_start"")
    OR (copy.""start_pos"" >= cytob.""hg38_start"" AND copy.""start_pos"" <= cytob.""hg38_stop"")
),
cbands AS (
  SELECT 
    ""chromosome"", 
    ""cytoband_name"", 
    ""hg38_start"", 
    ""hg38_stop"", 
    ""case_barcode"",
    MAX(""copy_number"") AS ""copy_number""
  FROM 
    joined
  GROUP BY 
    ""chromosome"", 
    ""cytoband_name"", 
    ""hg38_start"", 
    ""hg38_stop"", 
    ""case_barcode""
),
aberrations AS (
  SELECT
    ""chromosome"",
    ""cytoband_name"",
    -- Amplifications: more than two copies for diploid > 4
    SUM( CASE WHEN ""copy_number"" > 3 THEN 1 ELSE 0 END ) AS ""total_amp"",
    -- Gains: at most two extra copies
    SUM( CASE WHEN ""copy_number"" = 3 THEN 1 ELSE 0 END ) AS ""total_gain"",
    -- Homozygous deletions, or complete deletions
    SUM( CASE WHEN ""copy_number"" = 0 THEN 1 ELSE 0 END ) AS ""total_homodel"",
    -- Heterozygous deletions, 1 copy lost
    SUM( CASE WHEN ""copy_number"" = 1 THEN 1 ELSE 0 END ) AS ""total_heterodel"",
    -- Normal for Diploid = 2
    SUM( CASE WHEN ""copy_number"" = 2 THEN 1 ELSE 0 END ) AS ""total_normal""
  FROM 
    cbands
  GROUP BY 
    ""chromosome"", 
    ""cytoband_name""
)
SELECT 
  aberrations.""chromosome"", 
  aberrations.""cytoband_name"",
  total_cases.""total"",  
  100 * aberrations.""total_amp"" / total_cases.""total"" AS ""freq_amp"", 
  100 * aberrations.""total_gain"" / total_cases.""total"" AS ""freq_gain"",
  100 * aberrations.""total_homodel"" / total_cases.""total"" AS ""freq_homodel"", 
  100 * aberrations.""total_heterodel"" / total_cases.""total"" AS ""freq_heterodel"", 
  100 * aberrations.""total_normal"" / total_cases.""total"" AS ""freq_normal""  
FROM 
  aberrations, 
  total_cases
ORDER BY 
  aberrations.""chromosome"", 
  aberrations.""cytoband_name"";
","### Comprehensive Guide to Copy Number Variations in Cancer Genomics

#### **1. Introduction to Copy Number Variations (CNVs)**

Copy number variations (CNVs) are changes in the genome where regions have altered numbers of DNA segments. These variations include amplifications or deletions, significantly impacting genetic diversity and disease progression, particularly in cancer.

#### **2. The Role of CNVs in Cancer**

CNVs can drive cancer progression by amplifying oncogenes or deleting tumor suppressor genes, affecting gene dosage and cellular control mechanisms.

#### **3. TCGA-KIRC Project Overview**

The TCGA Kidney Renal Clear Cell Carcinoma (KIRC) project offers crucial CNV data to enhance our understanding of the molecular basis of kidney cancer.

#### **4. CytoBands and Their Genomic Significance**

CytoBands are chromosomal regions identified by staining patterns that help localize genetic functions and structural features.

#### **5. Data Sources for CNV Analysis**

- **TCGA CNV Data**: Provides genomic copy number changes in cancer tissues.
- **Mitelman Database (CytoBands_hg38)**: Offers detailed cytoband data for mapping CNVs to chromosomes.

#### **6. CNV Categories and Their Implications in Cancer**

- **Amplifications** (>3 copies): Lead to oncogene overexpression, accelerating tumor growth.
- **Gains** (=3 copies): Cause subtle changes in gene dosage, potentially enhancing cancer progression.
- **Homozygous Deletions** (0 copies): Result in the loss of both copies of tumor suppressor genes, promoting tumor development.
- **Heterozygous Deletions** (1 copy): Reduce the dosage of key regulatory genes, contributing to tumor progression.
- **Normal Diploid** (2 copies): Maintain standard genomic copies, serving as a baseline for comparative analysis.

#### **7. Methodology for Determining Overlaps**

To localize CNVs within specific cytobands, we use:

\[ \text{Overlap} = \max(0, \min(\text{end\_pos}, \text{hg38\_stop}) - \max(\text{start\_pos}, \text{hg38\_start})) \]

This formula ensures that the overlap measurement is the actual intersected length of the CNV and cytoband segments. It uses:
- `\min(\text{end\_pos}, \text{hg38\_stop})` to find the smallest endpoint between the CNV segment and the cytoband.
- `\max(\text{start\_pos}, \text{hg38\_start})` to find the largest start point between the CNV segment and the cytoband.
- The `max(0, ...)` function ensures that the overlap cannot be negative, which would indicate no actual overlap.


#### **8. Conclusion**

Analyzing CNVs is crucial for understanding cancer genetics and developing targeted therapies. Integrating CNV analysis with traditional markers enhances our insights into tumor biology.",snowflake
307,bq165,mitelman,"Can you use CytoConverter genomic coordinates to calculate the frequency of chromosomal gains and losses across a cohort of breast cancer (morphology='3111') and adenocarcinoma (topology='0401') samples? Concretely, please include the number and frequency (2 decimals in percentage) of amplifications (gains of more than 1 copy), gains (1 extra copy), losses (1 copy) and homozygous deletions (loss of 2 copies) for each chromosomal band. And sort the result by the ordinal of each chromosome and the starting-ending base-pair position of each band in ascending order.",,,snowflake
308,bq169,mitelman,"Retrieve distinct case references, case numbers, investigation numbers, and clone information where a single clone simultaneously exhibits all three of the following genetic alterations: (1) a loss on chromosome 13 between positions 48,303,751 and 48,481,890, (2) a loss on chromosome 17 between positions 7,668,421 and 7,687,490, and (3) a gain on chromosome 11 between positions 108,223,067 and 108,369,102. For each matching clone, display the chromosomal details for each of these three regions (including chromosome number represented by ChrOrd, start position, and end position) and the corresponding karyotype short description from the KaryClone table. Use the CytoConverted and KaryClone.",,,snowflake
309,bq111,mitelman,"Could you compute, by chromosome, the Pearson correlation between the frequency of copy number aberrations (including amplifications, gains, losses, and deletions) from the Mitelman database for cases with morph = 3111 and topo = 0401, and those computed from TCGA data, returning correlation coefficients and corresponding p-values for each aberration type, ensuring only results with at least five matching records are shown.",,"# Correlations between Mitelman and TCGA datasets

Check out other notebooks at our [Community Notebooks Repository](https://github.com/isb-cgc/Community-Notebooks)!

- **Title:** Correlations between Mitelman DB and TCGA datasets
- **Author:** Boris Aguilar
- **Created:** 04-23-2022
- **Purpose:** Compare Mitelman DB and TCGA datasets
- **URL:**

This notebook demonstrates how to compute correlations between Mitelman DB and TCGa datasets. The Mitelman DB is hosted by ISB-CGC and can be accessed at this URL: https://mitelmandatabase.isb-cgc.org/. This notebook replicates some of the analyses from the paper by Denomy et al: https://cancerres.aacrjournals.org/content/79/20/5181. Note, however that results are not replicated exactly as some of the underlying data has changed since publication.



# Calculate Frequency of Gains and Losses of breast cancer samples in Mitelman DB

We can use CytoConverter genomic coordinates to calculate the frequency of chromosomal gains and losses across a cohort of samples, e.g., across all breast cancer samples.

In [ ]:

```
# Set parameters for this query
cancer_type = 'BRCA' # Cancer type for TCGA
bq_project = 'mitelman-db'  # project name of Mitelman-DB BigQuery table
bq_dataset = 'prod' # Name of the dataset containing Mitelman-DB BigQuery tables
morphology = '3111' # Breast cancer
topology = '0401' # Adenocarcinoma
```

First, we identify all Mitelman DB cases related to the morphology and topology of interest.

This query was copied from the new feature of the MitelmanDB interface: View Overall Gain/Loss in chromosome.

```
case_query = """"""
# sql here
""""""

# Run the query and put results in a data frame
mysql = ( ""WITH "" + case_query + """"""
SELECT *
FROM mitelman
"""""" )
final_mitelman = client.query(mysql).result().to_dataframe()
```

# Calculate Frequency of TCGA Copy Number Gains and Losses in breast cancer samples.

As a comparison to Mitelman DB gain and loss frequency, we can calculate similar frequencies using TCGA Copy Number data.

```
cnv_query = """"""
# sql here
""""""

# Execute query and put results into a data frame
mysql = ( ""WITH "" + cnv_query + """"""
SELECT *
FROM tcga
"""""" )
cnv = client.query(mysql).result().to_dataframe()
```

|      | chromosome | cytoband_name | hg38_start | hg38_stop | total |  freq_amp | freq_gain | freq_homodel | freq_heterodel | freq_normal |
| ---: | ---------: | ------------: | ---------: | --------: | ----: | --------: | --------: | -----------: | -------------: | ----------: |
|    0 |       chr1 |          1p36 |          0 |  27600000 |  1067 | 11.902530 | 19.962512 |     0.000000 |      13.120900 |   55.014058 |
|    1 |       chr1 |          1p35 |   27600000 |  34300000 |  1067 | 13.214620 | 21.462043 |     0.000000 |       9.372071 |   55.951265 |
|    2 |       chr1 |          1p34 |   34300000 |  46300000 |  1067 | 18.650422 | 21.743205 |     0.000000 |       5.716963 |   53.889410 |
|    3 |       chr1 |          1p33 |   46300000 |  50200000 |  1067 | 17.525773 | 22.774133 |     0.000000 |       6.373008 |   53.327085 |
|    4 |       chr1 |          1p32 |   50200000 |  60800000 |  1067 | 19.119025 | 21.462043 |     0.000000 |       6.279288 |   53.139644 |
|  ... |        ... |           ... |        ... |       ... |   ... |       ... |       ... |          ... |            ... |         ... |
|  300 |       chrX |          Xq27 |  138900000 | 148000000 |  1067 | 24.273664 | 14.058107 |     0.281162 |      10.496720 |   50.890347 |
|  301 |       chrX |          Xq28 |  148000000 | 156040895 |  1067 | 23.711340 | 14.526710 |     0.187441 |      10.309278 |   51.265230 |
|  302 |       chrY |          Yp11 |          0 |  10400000 |  1067 |  0.374883 |  0.281162 |    96.438613 |       2.624180 |    0.281162 |
|  303 |       chrY |          Yq11 |   10400000 |  26600000 |  1067 |  0.281162 |  0.281162 |    97.469541 |       1.593252 |    0.374883 |
|  304 |       chrY |          Yq12 |   26600000 |  57227415 |  1067 |  0.281162 |  0.187441 |    96.438613 |       2.811621 |    0.281162 |

305 rows × 10 columns

# Compute Pearson correlation and p-values

The following query compute Pearson correlation for each chromosome comparing Mitelman DB frequencies with those computed from TCGA. Moreover, for each correlation values, its respective p-values is computed by using the BigQuery function `isb-cgc-bq.functions.corr_pvalue_current`. The minimum number of cases for correlation computation was 5.

```
mysql = ( ""WITH "" + case_query + "","" + cnv_query + """"""
# sql here
"""""")
```

The non a value results (NaN) represent cases in which the computed frequencies of TCGA are zero for all the cytobands.

# Conclusion

This notebook demonstrated usage of the Mitelman BigQuery dataset, which includes CytoConverter chromosomal coordinate data, in combination with TCGA BigQuery tables for a comparative analysis. Specifically, the notebook computes correlation (Pearson) coefficients between gains and losses obtained with Mitelam DB and TCGA datasets.

We observed that the mayority (but not all) of the significan correlation shown in Denomy et al. paper (Table 1, https://doi.org/10.1158/0008-5472.CAN-19-0585) are also significan in this analysis.",snowflake
310,sf_bq451,_1000_GENOMES,"Extract genotype data for single nucleotide polymorphisms (SNPs) on chromosome X, excluding positions where the `start` value is between 59999 and 2699519 or between 154931042 and 155260559. For each sample, identify genotype calls where the genotype array has at least one allele. Classify each genotype call into one of the following categories: homozygous reference alleles (both alleles are 0), homozygous alternate alleles (both alleles are the same and greater than 0), or heterozygous alleles (alleles are different, or any allele is null, and at least one allele is greater than 0). Compute the total number of callable sites (the sum of all three genotype categories), the number of homozygous reference, homozygous alternate, and heterozygous genotype calls, the total number of single nucleotide variants (SNVs) as the sum of homozygous alternate and heterozygous genotype calls, the percentage of heterozygous genotype calls among all SNVs, and the percentage of homozygous alternate genotype calls among all SNVs. Output the sample ID along with these computed counts and percentages, and order the results by the percentage of heterozygous genotype calls among SNVs in descending order, then by sample ID.",,"# Explanation of Alleles Types

## homozygous reference alleles
- When both alleles are 0.

## homozygous alternate alleles
- When both alleles are identical and greater than 0.

## heterozygous alternate alleles
- When alleles are different or one is NULL with at least one greater than 0.

## callable sites
- Including homozygous reference, homozygous alternate, and heterozygous alleles.

## Single Nucleotide Variants (SNVs)
- Including homozygous and heterozygous alternate alleles.",snowflake
311,sf_bq452,_1000_GENOMES,"Identify variants on chromosome 12 and, for each variant, calculate the chi-squared score using allele counts in cases and controls, where cases are individuals from the 'EAS' super population and controls are individuals from all other super populations. Apply Yates's correction for continuity in the chi-squared calculation, ensuring that the expected counts for each allele in both groups are at least 5. Return the start position, end position, and chi-squared score of the top variants where the chi-squared score is no less than 29.71679.",,,snowflake
312,sf_bq453,_1000_GENOMES,"In chromosome 17 between positions 41196311 and 41277499, what are the reference names, start and end positions, reference bases, distinct alternate bases, variant types, and the chi-squared scores (calculated from Hardy-Weinberg equilibrium) along with the total number of genotypes, their observed and expected counts for homozygous reference, heterozygous, and homozygous alternate genotypes, as well as allele frequencies (including those from 1KG), for each variant?",,,snowflake
313,sf_bq454,_1000_GENOMES,"For the 1000 Genomes dataset, analyze common autosomal variants (those with an allele frequency of at least 0.05) across different super populations. For each super population, count how many variants are shared by each specific number of samples within that super population. Include in your results the total population size of each super population, whether the variant is common (allele frequency ≥ 0.05), the number of samples having each variant, and the total count of variants shared by that many samples. Only include autosomal variants by explicitly excluding sex chromosomes (X, Y) and mitochondrial DNA (MT) from the analysis. Consider only samples that have at least one alternate allele (non-reference) for the variant.",,,snowflake
314,sf_bq415,HUMAN_GENOME_VARIANTS,List the top 10 samples in the genome data that have the highest number of positions where there is exactly one alternate allele and the sample's genotype is homozygous for the reference allele (both alleles are 0). Order the results in descending order of these counts.,,"# Homozygous Reference Genotype

## Definition
A **homozygous reference genotype** (often abbreviated as **hom_RR**) is a genetic condition where both alleles at a specific locus on a chromosome are identical to the reference allele. The reference allele is the variant of a gene that is most common in a population or considered the ""standard"" sequence. In genomic studies, this reference sequence serves as a baseline to identify variations or mutations in an individual's genome.

## Characteristics
- **Alleles**: In a diploid organism, each individual has two alleles at each locus—one inherited from the mother and one from the father. In the case of a homozygous reference genotype, both of these alleles match the reference sequence.
- **Genotype Representation**: Homozygous reference genotypes are typically represented as `0/0` or `0|0`, where `0` denotes the reference allele. The slashes (`/`) or pipes (`|`) are used to separate the two alleles.
- **No Variation**: The hom_RR genotype indicates that there is no variation from the reference sequence at this specific locus in the individual's genome. This is the opposite of a **homozygous alternate genotype** (hom_AA), where both alleles differ from the reference, or a **heterozygous genotype** (het_RA), where one allele matches the reference and the other does not.

## Biological Significance
Homozygous reference genotypes are crucial in understanding genetic variation within populations. By comparing individual genotypes to the reference genome:
- Researchers can identify loci that are conserved across a population (where most individuals are hom_RR).
- It helps in pinpointing specific loci where mutations occur, potentially leading to genetic diseases or contributing to phenotypic diversity.

## Use in Genomic Analysis
In genomic studies, identifying homozygous reference genotypes is a critical step in filtering and analyzing genetic data. It provides a baseline to:
- **Calculate mutation frequencies**: By determining how often a specific locus deviates from the reference genotype.
- **Perform association studies**: To link genetic variants with particular traits or diseases.
- **Understand evolutionary conservation**: Hom_RR loci may indicate regions of the genome under strong evolutionary pressure to remain unchanged.

## Example Scenario
Consider a genomic variant dataset where each record contains information about a specific locus:
- If a sample's genotype for a given locus is `0/0`, this sample is considered to have a homozygous reference genotype at that locus.
- This status can be used to count the number of hom_RR genotypes across all loci for a given individual, or to compare the frequency of hom_RR genotypes between populations.

## Summary
The homozygous reference genotype plays a foundational role in genetic analysis, serving as a benchmark for identifying genetic variations. It is a key concept for understanding the genetic makeup of individuals and populations in relation to a reference genome.
",snowflake
315,bq279,austin,Can you provide the number of distinct active and closed bike share stations for each year 2013 and 2014?,"SELECT
    t.year,
    CASE 
        WHEN t.year = 2013 THEN (
                                  SELECT 
                                    COUNT(DISTINCT station_id)
                                  FROM 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t
                                  INNER JOIN 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s
                                  ON 
                                    t.start_station_id = s.station_id
                                  WHERE 
                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2013
                                 ) 
        WHEN t.year = 2014 THEN (
                                  SELECT 
                                    COUNT(DISTINCT station_id)
                                  FROM 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t
                                  INNER JOIN 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s
                                  ON 
                                    t.start_station_id = s.station_id
                                  WHERE 
                                    s.status = 'active' AND EXTRACT(YEAR FROM start_time) = 2014
                                 )
    END
    AS number_status_active,
    CASE 
        WHEN t.year = 2013 THEN (
                                  SELECT 
                                   COUNT(DISTINCT station_id)
                                  FROM 
                                  `bigquery-public-data.austin_bikeshare.bikeshare_trips` t
                                  INNER JOIN 
                                  `bigquery-public-data.austin_bikeshare.bikeshare_stations` s
                                  ON 
                                   t.start_station_id = s.station_id
                                  WHERE 
                                   s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2013
                                 ) 
        WHEN t.year = 2014 THEN (
                                  SELECT 
                                  COUNT(DISTINCT station_id)
                                  FROM 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_trips` t
                                  INNER JOIN 
                                    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s
                                  ON 
                                    t.start_station_id = s.station_id
                                  WHERE 
                                    s.status = 'closed' AND EXTRACT(YEAR FROM start_time) = 2014
                                 )
    END
    AS number_status_closed
FROM
    (
      SELECT 
         EXTRACT(YEAR FROM start_time) AS year,
         start_station_id
      FROM
         `bigquery-public-data.austin_bikeshare.bikeshare_trips`
    ) 
    AS t
INNER JOIN
    `bigquery-public-data.austin_bikeshare.bikeshare_stations` s
ON
    t.start_station_id = s.station_id
WHERE
    t.year BETWEEN 2013 AND 2014
GROUP BY
    t.year
ORDER BY
    t.year",,snowflake
316,bq281,austin,"What is the highest number of electric bike rides lasting more than 10 minutes taken by subscribers with 'Student Membership' in a single day, excluding rides starting or ending at 'Mobile Station' or 'Repair Shop'?","SELECT
  COUNT(1) AS num_rides
FROM
  `bigquery-public-data.austin_bikeshare.bikeshare_trips` 
WHERE 
start_station_name 
    NOT IN ('Mobile Station', 'Repair Shop')
AND
end_station_name 
    NOT IN ('Mobile Station', 'Repair Shop')
AND 
subscriber_type = 'Student Membership'
AND
bike_type = 'electric'
AND
duration_minutes > 10
GROUP BY 
    EXTRACT(YEAR from start_time), 
    EXTRACT(MONTH from start_time), 
    EXTRACT(DAY from start_time)
ORDER BY num_rides DESC
LIMIT 1",,snowflake
317,bq282,austin,"Can you tell me the numeric value of the active council district in Austin which has the highest number of bike trips that start and end within the same district, but not at the same station?","SELECT 
  district
FROM (
  SELECT
    S.starting_district AS district,
    T.start_station_id,
    T.end_station_id
  FROM
    `bigquery-public-data.austin_bikeshare.bikeshare_trips` AS T
  INNER JOIN (
    SELECT
      station_id,
      council_district AS starting_district
    FROM
      `bigquery-public-data.austin_bikeshare.bikeshare_stations`
    WHERE
      status = ""active""
  ) AS S ON T.start_station_id = S.station_id
  WHERE
    S.starting_district IN (
      SELECT council_district
      FROM `bigquery-public-data.austin_bikeshare.bikeshare_stations`
      WHERE
        status = ""active"" AND
        station_id = SAFE_CAST(T.end_station_id AS INT64)
    )
    AND T.start_station_id != SAFE_CAST(T.end_station_id AS INT64)
) 
GROUP BY district
ORDER BY COUNT(*) DESC
LIMIT 1;
",,snowflake
318,bq006,austin,What is the date with the second highest Z-score for daily counts of 'PUBLIC INTOXICATION' incidents in Austin for the year 2016? List the date in the format of '2016-xx-xx'.,"WITH incident_stats AS (
  SELECT 
    COUNT(descript) AS total_pub_intox
  FROM 
    `bigquery-public-data.austin_incidents.incidents_2016` 
  WHERE 
    descript = 'PUBLIC INTOXICATION' 
  GROUP BY 
    date
),
average_and_stddev AS (
  SELECT 
    AVG(total_pub_intox) AS avg, 
    STDDEV(total_pub_intox) AS stddev 
  FROM 
    incident_stats
),
daily_z_scores AS (
  SELECT 
    date, 
    COUNT(descript) AS total_pub_intox, 
    ROUND((COUNT(descript) - a.avg) / a.stddev, 2) AS z_score
  FROM 
    `bigquery-public-data.austin_incidents.incidents_2016`,
    (SELECT avg, stddev FROM average_and_stddev) AS a
  WHERE 
    descript = 'PUBLIC INTOXICATION'
  GROUP BY 
    date, avg, stddev
)

SELECT 
  date
FROM 
  daily_z_scores
ORDER BY 
  z_score DESC
LIMIT 1
OFFSET 1",,snowflake
319,sf_bq283,AUSTIN,"Among all stations that are currently active, identify those that rank in the top 15 (including ties) based on the total number of trips that start at each station. For each of these stations, return the station ID, the total number of starting trips, the percentage of those trips out of the overall starting trips from active stations, and the average trip duration in minutes. Order the results by the station’s rank.",,,snowflake
320,bq284,bbc,"Can you provide a breakdown of the total number of articles into different categories and the percentage of those articles that mention ""education"" within each category from the BBC News?","SELECT 
  category,
  COUNT(*) AS number_total_by_category,  
  CASE 
    WHEN category = 'tech' THEN 
          (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE (LOWER(body) LIKE '%education%') AND category = 'tech') * 100 /
                (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE category = 'tech')
    WHEN category = 'sport' THEN 
          (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE (LOWER(body) LIKE '%education%') AND category = 'sport') * 100 /
                (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE category = 'sport')
    WHEN category = 'business' THEN 
          (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE (LOWER(body) LIKE '%education%') AND category = 'business') * 100 /
                (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE category = 'business')
    WHEN category = 'politics' THEN 
          (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE (LOWER(body) LIKE '%education%') AND category = 'politics') * 100 /
                (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE category = 'politics')
    WHEN category = 'entertainment' THEN 
          (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE (LOWER(body) LIKE '%education%') AND category = 'entertainment') * 100 /
                (SELECT count(*)
                FROM `bigquery-public-data.bbc_news.fulltext`
                WHERE category = 'entertainment')
  END AS percent_education
FROM `bigquery-public-data.bbc_news.fulltext`
GROUP BY
  category;",,snowflake
321,bq413,dimensions_ai_covid19,"Retrieve the venue titles of publications that have a `date_inserted` from the year 2021 onwards and are associated with a grid whose address city is 'Qianjiang'. For each publication, prioritize the venue title by selecting the journal title first if it exists; if not, then the proceedings title; if that's also unavailable, then the book title; and finally, if none of those are available, the book series title.","SELECT
   COALESCE(p.journal.title, p.proceedings_title.preferred, p.book_title.preferred, p.book_series_title.preferred) AS venue,
FROM
   `bigquery-public-data.dimensions_ai_covid19.publications` p
LEFT JOIN
   UNNEST(research_orgs) AS research_orgs_grids
LEFT JOIN
   `bigquery-public-data.dimensions_ai_covid19.grid` grid
ON
   grid.id=research_orgs_grids
WHERE
   EXTRACT(YEAR FROM date_inserted) >= 2021
   AND
   grid.address.city = 'Qianjiang'",,snowflake
322,bq425,ebi_chembl,"Using data from ChEMBL Release 23, retrieve all distinct molecules associated with the company 'SanofiAventis,' listing the trade name and the most recent approval date for each molecule. Make sure to keep only the latest approval date per molecule and ensure the company field precisely matches 'SanofiAventis' without relying on other fields.","SELECT *
  FROM (
  SELECT
  molregno,
  comp.company,
  prod.trade_name,
  prod.approval_date,
  ROW_NUMBER() OVER(PARTITION BY molregno ORDER BY PARSE_DATE('%Y-%m-%d', prod.approval_date) DESC) rn
  FROM bigquery-public-data.ebi_chembl.compound_records_23 AS cmpd_rec
  JOIN bigquery-public-data.ebi_chembl.molecule_synonyms_23 AS ms USING (molregno)
  JOIN bigquery-public-data.ebi_chembl.research_companies_23 AS comp USING (res_stem_id)
  JOIN bigquery-public-data.ebi_chembl.formulations_23 AS form USING (molregno)
  JOIN bigquery-public-data.ebi_chembl.products_23 AS prod USING (product_id)
  ) as subq
 WHERE rn = 1 AND company = 'SanofiAventis'",,snowflake
323,bq430,ebi_chembl,"Find pairs of different molecules tested in the same assay and standard type, where both have 10–15 heavy atoms, fewer than 5 activities in that assay, fewer than 2 duplicate activities, non-null standard values, and pChEMBL values over 10. For each pair, report the maximum heavy atom count, the latest publication date (calculated based on the document's rank within the same journal and year, and map it to a synthetic month and day), the highest document ID, classify the change in standard values as 'increase', 'decrease', or 'no-change' based on their values and relations, and generate UUIDs from their activity IDs and canonical SMILES.","select 
  -- *, 
  greatest(heavy_atoms_1, heavy_atoms_2) as heavy_atoms_greatest,
  greatest(publication_date_1, publication_date_2) as publication_date_greatest,
  greatest(doc_id_1, doc_id_2) as doc_id_greatest,
  case 
    when 
      standard_value_1 > standard_value_2 and 
      standard_relation_1 not in ('<', '<<') and 
      standard_relation_2 not in ('>', '>>')
    then 'decrease'
    when
      standard_value_1 < standard_value_2 and 
      standard_relation_1 not in ('>', '>>') and 
      standard_relation_2 not in ('<', '<<') 
    then 'increase'
    when
      standard_value_1 = standard_value_2 and 
      standard_relation_1 in ('=', '~') and 
      standard_relation_2 in ('=', '~') 
    then 'no-change'
    else null
  end as standard_change,
  to_hex(md5(to_json_string(struct(activity_id_1, activity_id_2)))) as mmp_delta_uuid,
  to_hex(md5(to_json_string(struct(canonical_smiles_1, canonical_smiles_2, 5)))) as mmp_search_uuid
from (
  select 
    act.assay_id,
    act.standard_type,
    act.activity_id as activity_id_1,
    cast(act.standard_value as numeric) as standard_value_1,
    act.standard_relation as standard_relation_1,
    cast(act.pchembl_value as numeric) as pchembl_value_1,
    count(*) over (partition by act.assay_id) as count_activities_1,
    count(*) over (partition by act.assay_id, act.molregno) as duplicate_activities_1,
    act.molregno as molregno_1,
    com.canonical_smiles as canonical_smiles_1,
    cast(cmp.heavy_atoms as int64) as heavy_atoms_1,
    cast(d.doc_id as int64) as doc_id_1,
    date(
      coalesce(cast(d.year as int64), 1970), 
      coalesce(cast(floor(percent_rank() over (
        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 11) as int64) + 1, 1),
      coalesce(mod(cast(floor(percent_rank() over (
        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 308) as int64), 28) + 1, 1)) as publication_date_1
  FROM `bigquery-public-data.ebi_chembl.activities_29` act
  join `bigquery-public-data.ebi_chembl.compound_structures_29` com using (molregno)
  join `bigquery-public-data.ebi_chembl.compound_properties_29` cmp using (molregno)
  left join `bigquery-public-data.ebi_chembl.docs_29` d using (doc_id)
  where standard_type in (select distinct standard_type from`bigquery-public-data.ebi_chembl.activities_29` where pchembl_value is not null)
  ) a1
join (
  select 
    act.assay_id,
    act.standard_type,
    act.activity_id as activity_id_2,
    cast(act.standard_value as numeric) as standard_value_2,
    act.standard_relation as standard_relation_2,
    cast(act.pchembl_value as numeric) as pchembl_value_2,
    count(*) over (partition by act.assay_id) as count_activities_2,
    count(*) over (partition by act.assay_id, act.molregno) as duplicate_activities_2, 
    act.molregno as molregno_2,
    com.canonical_smiles as canonical_smiles_2, 
    cast(cmp.heavy_atoms as int64) as heavy_atoms_2,
    cast(d.doc_id as int64) as doc_id_2,
    date(
      coalesce(cast(d.year as int64), 1970), 
      coalesce(cast(floor(percent_rank() over (
        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 11) as int64) + 1, 1),
      coalesce(mod(cast(floor(percent_rank() over (
        partition by d.journal, d.year order by SAFE_CAST(d.first_page as int64)) * 308) as int64), 28) + 1, 1)) as publication_date_2
  FROM `bigquery-public-data.ebi_chembl.activities_29` act
  join `bigquery-public-data.ebi_chembl.compound_structures_29` com using (molregno)
  join `bigquery-public-data.ebi_chembl.compound_properties_29` cmp using (molregno)
  left join `bigquery-public-data.ebi_chembl.docs_29` d using (doc_id)
  where standard_type in (select distinct standard_type from`bigquery-public-data.ebi_chembl.activities_29` where pchembl_value is not null)
  ) a2 using (assay_id, standard_type)
where 
  a1.molregno_1 != a2.molregno_2 and
  a1.count_activities_1 < 5 and 
  a2.count_activities_2 < 5 and 
  a1.heavy_atoms_1 between 10 and 15 and
  a2.heavy_atoms_2 between 10 and 15 and
  a1.standard_value_1 is not null and 
  a2.standard_value_2 is not null and
  a1.duplicate_activities_1 < 2 and
  a2.duplicate_activities_2 < 2 and
  a1.pchembl_value_1 > 10 and
  a2.pchembl_value_2 > 10

","### Data Sources:
Part tables of ChEMBL database:
- activity data: patents-public-data.ebi_chembl.activities_29
- compound structures: patents-public-data.ebi_chembl.compound_structures_29 
- compound properties: patents-public-data.ebi_chembl.compound_properties_29 
- publication documents: patents-public-data.ebi_chembl.docs_29 

### UUID Generation:
Activity Pair UUID (mmp_delta_uuid):
Generated using the MD5 hash of the JSON string of the pair's activity IDs:
to_hex(md5(to_json_string(struct(A, B))))
Both A and B can be activity id or canonical_smiles

### Standard Change Classification:
Determines whether the standard value between two molecules has increased, decreased, or stayed the same:
'decrease': If standard_value_1 >(>>) standard_value_2 and measurement relations do not conflict.
'increase': If standard_value_1 <(<<) standard_value_2 and measurement relations do not conflict.
'no-change': If standard_value_1 =(~) standard_value_2 and both standard relations indicate equality.

### How to Got the publication Date
To compute a publication date for each document, we use the following method, which assigns a date based on the document's relative position within its journal and year, ordered by its first page number.

#### 1. Year Calculation

- **Year**: Use the document's publication year as the year in the date.
  - If the document's year is known, use that year.
  - If the year is missing or unavailable, default to **1970**.

#### 2. Month Calculation

- **Grouping**: For all documents within the same **journal** and **year**, group them together.
- **Ordering**: Within each group, order the documents by their `first_page` number (converted to an integer).
- **Percent Rank Computation**:
  - Calculate the **percent rank** of each document in the ordered list.
  - **Percent Rank Formula**:

    $$\text{Percent Rank} = \frac{\text{Rank of Document} - 1}{\text{Total Documents in Group} - 1}$$

    - The rank starts at 1 for the first document.
    - The percent rank ranges from 0 to 1.
- **Month Assignment**:
  - Scale the percent rank to months by multiplying it by 11:

    $$\text{Scaled Value} = \text{Percent Rank} \times 11$$

  - Take the integer part of the scaled value (floor it):

    $$\text{Floor Value} = \left\lfloor \text{Scaled Value} \right\rfloor$$

  - Add 1 to get the month number (since months are from 1 to 12):

    $$\text{Month} = \text{Floor Value} + 1$$

  - **Note**: If the computed month is not available (e.g., due to missing data), default the month to **1**.

#### 3. Day Calculation

- **Using the Same Percent Rank**: Use the percent rank computed in the month calculation.
- **Day Assignment**:
  - Scale the percent rank to days by multiplying it by **308**:

    $$\text{Scaled Value} = \text{Percent Rank} \times 308$$

    - The number 308 is chosen because it is the product of 11 months and 28 days (11 × 28), representing the total number of days in an 11-month period with 28 days per month.
  - Take the integer part of the scaled value (floor it):

    $$\text{Floor Value} = \left\lfloor \text{Scaled Value} \right\rfloor$$

  - Compute the modulus of the floor value with 28:

    $$\text{Modulo Value} = \text{Floor Value} \bmod 28$$

  - Add 1 to get the day number (since days are from 1 to 28):

    $$\text{Day} = \text{Modulo Value} + 1$$

  - **Note**: If the computed day is not available, default the day to **1**.

### 4. Constructing the publication Date

- Combine the computed **Year**, **Month**, and **Day** to form the publication date.

  $$\text{publication Date} : \text{Year}-\text{Month}-\text{Day}$$

  For example, 2002-06-15.
",snowflake
324,bq023,fec,"Using the 2018 5-Year American Community Survey (ACS) for median incomes at the census tract level and the 2020 Federal Election Commission (FEC) individual contributions dataset filtered for donors in New York, matched to census tract geographies via a ZIP code to census tract crosswalk, calculate and list the average political donation amount and the median income for each census tract located in Kings County (Brooklyn), New York. ",,,snowflake
325,bq094,fec,"Please provide a list of all 2016 committees that supported at least one candidate and received a total amount of individual contributions between $0 and $200 (inclusive of more than $0 and less than $200) where these small-dollar contributions sum to more than $0 overall. For each qualifying committee, include its name, the number of unique candidates it supported, the candidates’ names in alphabetical order (separated by commas), and the total sum of these small-dollar donations received by the committee.",,,snowflake
326,bq287,fda,"Among all Utah ZIP codes, what is the 2017 American Community Survey employment rate for the population aged 16 or older in the ZIP code that has the fewest FDIC-insured bank locations?",,,snowflake
327,bq432,fda,"Please provide the food events data where both \""date_created\"" and \""date_started\"" are between January 1 and January 31, 2015, apply the following data cleansing steps: split reactions and outcomes fields into arrays by commas, handle special numeric patterns in the products_brand_name field (where a digit is followed by comma and another digit) by preserving those numeric patterns while replacing other "", "" with "" -- "", replace "", "" with "" -- "" in products_industry_code, products_role, and products_industry_name fields, and calculate industry_code_length and brand_name_length as the array lengths after splitting. ",,"#### Food Events Cleansing

1. **`report_number`**: A unique identifier for each reported food event.



2. **`reactions`**: A list of reactions associated with the food event.

   \- Each reaction is separated into individual entries from a comma-delimited string to facilitate easier analysis and access.



3. **`outcomes`**: A list of outcomes associated with the food event.

   \- Outcomes are individually listed by breaking down the comma-separated values into distinct elements for clear interpretation and usage.


4. **`products_brand_name`**: A list of product brand names, modified to address patterns where numeric sequences followed by commas could disrupt list separation.
   - Commas followed by a space within these patterns are replaced with double hyphens (` -- `) to ensure accurate parsing and to maintain the integrity of brand names.

5. **`products_industry_code`**: List of industry codes.
   - Commas followed by a space, which typically separate these codes, are replaced with double hyphens (` -- `) to ensure each code is distinctly recognized.

6. **`products_industry_role`**: List of roles associated with the product.
   - To ensure clear delineation, commas followed by a space that typically separate these roles are replaced with double hyphens (` -- `).

7. **`products_industry_name`**: List of industry names.
   - Commas followed by a space, which typically separate these names, are replaced with double hyphens (` -- `) to prevent any misinterpretation of industry names.




8. **`date_created`**: The date the event was created in the system.



9. **`date_started`**: The date when the adverse event started.



10. **`consumer_gender`**: Gender of the consumer involved in the event.



11. **`consumer_age`**: Age of the consumer involved in the event.



12. **`consumer_age_unit`**: Unit for the consumer's age (e.g., year, month).



13. **`industry_code_length`**: Represents the total number of distinct industry codes associated with the product.
    - The count is derived by transforming the `products_industry_code` field. Commas followed by a space are replaced with double hyphens (` -- `) to ensure that composite codes are not incorrectly split. The total number of industry codes is then determined by measuring the length of the resulting array.

14. **`brand_name_length`**: Reflects the number of unique brand names listed for the product.
    - This value is computed by first modifying the `products_brand_name` to accommodate special formats, such as numeric sequences that may include commas followed by a space. These commas are replaced with double hyphens (` -- `) to ensure proper parsing. The total count of brand names is obtained by calculating the length of the array after this transformation.
",snowflake
328,bq285,fda,Could you provide me with the zip code of the location that has the highest number of bank institutions in Florida?,"with _fips AS
    (
        SELECT
            state_fips_code
        FROM
            `bigquery-public-data.census_utility.fips_codes_states`
        WHERE
            state_name = ""Florida""
    )

    ,_zip AS
    (
        SELECT
            z.zip_code,
            z.zip_code_geom,
        FROM
            `bigquery-public-data.geo_us_boundaries.zip_codes` z, _fips u
        WHERE
            z.state_fips_code = u.state_fips_code
    )

    ,locations AS
    (
        SELECT
            COUNT(i.institution_name) AS count_locations,
            l.zip_code
        FROM
            `bigquery-public-data.fdic_banks.institutions` i
        JOIN
            `bigquery-public-data.fdic_banks.locations` l 
        USING (fdic_certificate_number)
        WHERE
            l.state IS NOT NULL
        AND 
            l.state_name IS NOT NULL
        GROUP BY 2
    )

    SELECT
        z.zip_code
    FROM
        _zip z
    JOIN
        locations l 
    USING (zip_code)
    GROUP BY
        z.zip_code
    ORDER BY
        SUM(l.count_locations) DESC
    LIMIT 1;",,snowflake
329,bq288,fda,"What is the total number of all banking institutions in the state that has the highest sum of assets from banks established between January 1, 1900, and December 31, 2000, with institution names starting with 'Bank'?",,,snowflake
330,sf_bq412,GOOGLE_ADS,"Please retrieve the page URLs, first shown time, last shown time, removal reason, violation category, and the lower and upper bounds of times shown for the five most recently removed ads in the Croatia region (region code 'HR'), where the times shown availability date is null, the times shown lower bound exceeds 10,000, the times shown upper bound is below 25,000, and the ads used at least one non-unused audience selection approach among demographics, geographic location, contextual signals, customer lists, or topics of interest, ordering the resulting ads by their last shown time in descending order.","SELECT
    ""creative_page_url"",
    TO_TIMESTAMP(GET(""region_stat"".value, 'first_shown')) AS ""first_shown"",
    TO_TIMESTAMP(GET(""region_stat"".value, 'last_shown')) AS ""last_shown"",
    REPLACE(REPLACE(""disapproval""[0].""removal_reason"", '""""', '""'), '""', '') AS ""removal_reason"", 
    REPLACE(REPLACE(""disapproval""[0].""violation_category"", '""""', '""'), '""', '') AS ""violation_category"",
    GET(""region_stat"".value, 'times_shown_lower_bound') AS ""times_shown_lower"",
    GET(""region_stat"".value, 'times_shown_upper_bound') AS ""times_shown_upper""
FROM
    ""GOOGLE_ADS"".""GOOGLE_ADS_TRANSPARENCY_CENTER"".""REMOVED_CREATIVE_STATS"",
    LATERAL FLATTEN(input => ""region_stats"") AS ""region_stat""
WHERE
    GET(""region_stat"".value, 'region_code') = 'HR' 
    AND GET(""region_stat"".value, 'times_shown_availability_date') IS NULL 
    AND GET(""region_stat"".value, 'times_shown_lower_bound') > 10000 
    AND GET(""region_stat"".value, 'times_shown_upper_bound') < 25000
    AND (
        GET(""audience_selection_approach_info"", 'demographic_info') != 'CRITERIA_UNUSED' 
        OR GET(""audience_selection_approach_info"", 'geo_location') != 'CRITERIA_UNUSED' 
        OR GET(""audience_selection_approach_info"", 'contextual_signals') != 'CRITERIA_UNUSED' 
        OR GET(""audience_selection_approach_info"", 'customer_lists') != 'CRITERIA_UNUSED' 
        OR GET(""audience_selection_approach_info"", 'topics_of_interest') != 'CRITERIA_UNUSED'
    )
ORDER BY
    ""last_shown"" DESC
LIMIT 5;
",,snowflake
331,sf_bq423,GOOGLE_ADS,"Between January 1, 2023, and January 1, 2024, which image-type advertisement on the topic of Health, published by a verified advertiser located in Cyprus, was shown in Croatia, has times_shown_availability_date as NULL (meaning the times shown data is available), utilized demographic information, geo-location targeting, contextual signals, customer lists, and topics of interest without any of these selection methods being unused, and additionally had its first shown date strictly after January 1, 2023, and last shown date strictly before January 1, 2024? Among such ads, provide the page URL of the one with the highest upper bound of times shown.",,,snowflake
332,sf_bq070,IDC,"Could you provide a clean, structured dataset from dicom_all table that only includes SM images marked as VOLUME from the TCGA-LUAD and TCGA-LUSC collections, excluding any slides with compression type “other,” where the specimen preparation step explicitly has “Embedding medium” set to “Tissue freezing medium,” and ensuring that the tissue type is only “normal” or “tumor” and the cancer subtype is reported accordingly?","WITH
  sm_images AS (
    SELECT
      ""SeriesInstanceUID"" AS ""digital_slide_id"", 
      ""StudyInstanceUID"" AS ""case_id"",
      ""ContainerIdentifier"" AS ""physical_slide_id"",
      ""PatientID"" AS ""patient_id"",
      ""TotalPixelMatrixColumns"" AS ""width"", 
      ""TotalPixelMatrixRows"" AS ""height"",
      ""collection_id"",
      ""crdc_instance_uuid"",
      ""gcs_url"", 
      CAST(
        ""SharedFunctionalGroupsSequence""[0].""PixelMeasuresSequence""[0].""PixelSpacing""[0] AS FLOAT
      ) AS ""pixel_spacing"", 
      CASE ""TransferSyntaxUID""
          WHEN '1.2.840.10008.1.2.4.50' THEN 'jpeg'
          WHEN '1.2.840.10008.1.2.4.91' THEN 'jpeg2000'
          ELSE 'other'
      END AS ""compression""
    FROM
      IDC.IDC_V17.DICOM_ALL
    WHERE
      ""Modality"" = 'SM' 
      AND ""ImageType""[2] = 'VOLUME'
  ),

  tissue_types AS (
    SELECT DISTINCT *
    FROM (
      SELECT
        ""SeriesInstanceUID"" AS ""digital_slide_id"",
        CASE ""steps_unnested2"".value:""CodeValue""::STRING
            WHEN '17621005' THEN 'normal' -- meaning: 'Normal' (i.e., non-neoplastic)
            WHEN '86049000' THEN 'tumor'  -- meaning: 'Neoplasm, Primary'
            ELSE 'other'                 -- meaning: 'Neoplasm, Metastatic'
        END AS ""tissue_type""
      FROM
        IDC.IDC_V17.DICOM_ALL
        CROSS JOIN
          LATERAL FLATTEN(input => ""SpecimenDescriptionSequence""[0].""PrimaryAnatomicStructureSequence"") AS ""steps_unnested1""
        CROSS JOIN
          LATERAL FLATTEN(input => ""steps_unnested1"".value:""PrimaryAnatomicStructureModifierSequence"") AS ""steps_unnested2""
    )
  ),

  specimen_preparation_sequence_items AS (
    SELECT DISTINCT *
    FROM (
      SELECT
        ""SeriesInstanceUID"" AS ""digital_slide_id"",
        ""steps_unnested2"".value:""ConceptNameCodeSequence""[0].""CodeMeaning""::STRING AS ""item_name"",
        ""steps_unnested2"".value:""ConceptCodeSequence""[0].""CodeMeaning""::STRING AS ""item_value""
      FROM
        IDC.IDC_V17.DICOM_ALL
        CROSS JOIN
          LATERAL FLATTEN(input => ""SpecimenDescriptionSequence""[0].""SpecimenPreparationSequence"") AS ""steps_unnested1""
        CROSS JOIN
          LATERAL FLATTEN(input => ""steps_unnested1"".value:""SpecimenPreparationStepContentItemSequence"") AS ""steps_unnested2""
    )
  )

SELECT
  a.*,
  b.""tissue_type"",
  REPLACE(REPLACE(a.""collection_id"", 'tcga_luad', 'luad'), 'tcga_lusc', 'lscc') AS ""cancer_subtype""
FROM 
  sm_images AS a
  JOIN tissue_types AS b 
    ON b.""digital_slide_id"" = a.""digital_slide_id""
  JOIN specimen_preparation_sequence_items AS c 
    ON c.""digital_slide_id"" = a.""digital_slide_id""
WHERE
  (a.""collection_id"" = 'tcga_luad' OR a.""collection_id"" = 'tcga_lusc')
  AND a.""compression"" != 'other'
  AND (b.""tissue_type"" = 'normal' OR b.""tissue_type"" = 'tumor')
  AND (c.""item_name"" = 'Embedding medium' AND c.""item_value"" = 'Tissue freezing medium')
ORDER BY 
  a.""crdc_instance_uuid"";
","Detailed requirements include:
- The slides must belong to the TCGA-LUAD or TCGA-LUSC collections;
- The slides must be JPEG or JPEG2000 compressed;
- The slides must be digital images and exclude non-volume ones;
- The slides must contain either normal (`17621005`) or tumor (`86049000`) tissue, identified by specific DICOM codes;
- The slides must be prepared using the ""Tissue freezing medium"" embedding method;

With respect to the output, it should contain the following basic metadata and attributes related to slide microscopy images. Concretely,
- digital slide ID: unique numeric identifier of a digital slide, i.e., a digital image of a physical slide;
- case ID: unique numeric identifier of the study in the context of which the ditial slide was created;
- physical slide ID: unique numeric identifier of the physical slide as prepared in the wet lab; 
- patient ID: unique numeric identifier of the patient from whose tissue the physical slide was obtained;
- collection ID: numeric or character sequence describing the dataset the physical slide is part of;
- instance ID: universally unique identifier of the DICOM instance;
- GCS URL: how to access the DICOM file;
- width/height: image width and height in pixels, respectively;
- pixel spacing: image pixel spacing in mm/px;
- compression type (either value `jpeg`, `jpeg2000`, or `other`).

And the target two labels are:
- tissue_type: either `normal` or `tumor`;
- cancer_subtype: either `luad` or `lscc`.
Sort the results according to instance ID in ascending order.",snowflake
333,sf_bq320,IDC,"In the dicom_pivot table, how many unique StudyInstanceUID values exactly match the SegmentedPropertyTypeCodeSequence of ""15825003"" (case-insensitive) and also have a collection_id of either ""Community"" or ""nsclc_radiomics""?","SELECT
  COUNT(*) AS ""total_count""
FROM
  IDC.IDC_V17.DICOM_PIVOT AS ""dicom_pivot""
WHERE
  ""StudyInstanceUID"" IN (
    SELECT
      ""StudyInstanceUID""
    FROM
      IDC.IDC_V17.DICOM_PIVOT AS ""dicom_pivot""
    WHERE
      ""StudyInstanceUID"" IN (
        SELECT
          ""StudyInstanceUID""
        FROM
          IDC.IDC_V17.DICOM_PIVOT AS ""dicom_pivot""
        WHERE
          LOWER(""dicom_pivot"".""SegmentedPropertyTypeCodeSequence"") LIKE LOWER('15825003')
        GROUP BY
          ""StudyInstanceUID""
        INTERSECT
        SELECT
          ""StudyInstanceUID""
        FROM
          IDC.IDC_V17.DICOM_PIVOT AS ""dicom_pivot""
        WHERE
          ""dicom_pivot"".""collection_id"" IN ('Community', 'nsclc_radiomics')
        GROUP BY
          ""StudyInstanceUID""
      )
    GROUP BY
      ""StudyInstanceUID""
  );
",,snowflake
334,sf_bq321,IDC,"How many unique StudyInstanceUIDs are there from the DWI, T2 Weighted Axial, Apparent Diffusion Coefficient series, and T2 Weighted Axial Segmentations in the 'qin_prostate_repeatability' collection?","WITH relevant_series AS (
  SELECT 
    DISTINCT ""StudyInstanceUID""
  FROM 
    IDC.IDC_V17.DICOM_ALL
  WHERE 
    ""collection_id"" = 'qin_prostate_repeatability'
    AND ""SeriesDescription"" IN (
      'DWI',
      'T2 Weighted Axial',
      'Apparent Diffusion Coefficient',
      'T2 Weighted Axial Segmentations',
      'Apparent Diffusion Coefficient Segmentations'
    )    
),
t2_seg_lesion_series AS (
  SELECT 
    DISTINCT ""StudyInstanceUID""
  FROM 
    IDC.IDC_V17.DICOM_ALL
  CROSS JOIN LATERAL FLATTEN(input => ""SegmentSequence"") AS segSeq
  WHERE 
    ""collection_id"" = 'qin_prostate_repeatability'
    AND ""SeriesDescription"" = 'T2 Weighted Axial Segmentations'
)

SELECT 
    COUNT(DISTINCT ""StudyInstanceUID"") AS ""total_count""
FROM (
  SELECT 
    ""StudyInstanceUID"" 
  FROM relevant_series
  UNION ALL
  SELECT 
    ""StudyInstanceUID""
  FROM t2_seg_lesion_series
);
",,snowflake
335,sf_bq323,IDC,"Within the 'prostatex' collection, for MRI sequences where the Modality is 'MR', assign the label 't2w_prostateX' to sequences whose SeriesDescription contains 't2_tse_tra' and 'adc_prostateX' to sequences whose SeriesDescription contains 'ADC'. For all sequences labeled as 't2w_prostateX' or 'adc_prostateX', calculate the average Repetition Time, the average Echo Time, and the average Slice Thickness, and then compute the sum of these averages to obtain the combined overall average.",,,snowflake
336,sf_bq417,IDC,"Please provide identification details, study and series information, storage location, and total size in MB for the medical images belonging to male patients who are exactly 18 years old based on the numeric portion of the PatientAge field, where the BodyPartExamined is set to 'MEDIASTINUM' and the study date is strictly after September 1, 2014.",,"### Data Model Description

**Collection ID**
- **Description**: Identifies the data collection to which the imaging series belongs. This ID helps in categorizing the series within a larger dataset or repository.

**Patient ID**
- **Description**: A unique identifier for the patient who underwent the imaging procedure. This ID is crucial for linking the imaging data with patient records while maintaining confidentiality.

**Series Instance UID**
- **Description**: A unique identifier for the imaging series, ensuring each series can be distinctly recognized and accessed within the data system.

**Study Instance UID**
- **Description**: A unique identifier for the study under which the imaging series was produced, linking all series and images that were part of the same diagnostic examination.

**Source DOI**
- **Description**: The Digital Object Identifier for the source of the imaging data, providing a persistent link to the source document or dataset.

**Patient Age**
- **Description**: Represents the age of the patient at the time the imaging study was conducted, providing clinical context to the imaging data.

**Patient Sex**
- **Description**: Indicates the biological sex of the patient, which is relevant in the clinical analysis and diagnostic process.

**Study Date**
- **Description**: The date on which the imaging study was performed, important for chronological medical records and tracking patient history.

**Study Description**
- **Description**: Provides a description of the imaging study, offering insights into the purpose and scope of the study.

**Body Part Examined**
- **Description**: Specifies the part of the body that was examined in the imaging study, critical for aligning the imaging data with clinical assessments.

**Modality**
- **Description**: The type of modality used to produce the imaging series, such as MRI or CT, crucial for understanding the imaging technique and its clinical implications.

**Manufacturer**
- **Description**: The company that manufactured the imaging equipment, which can be relevant for assessing image quality and technological specifics.

**Manufacturer Model Name**
- **Description**: The model name of the imaging equipment used to produce the series, providing further details on the technology and capabilities of the equipment.

**Series Date**
- **Description**: The date on which the specific imaging series was created, helping to contextualize the imaging data within the patient's medical timeline.

**Series Description**
- **Description**: A detailed description of the imaging series, providing clinical context and details about the specific focus or protocol of the series.

**Series Number**
- **Description**: A number that uniquely identifies the series within a study, used to order and reference the series systematically.

**Instance Count**
- **Description**: The count of individual image instances within the series, useful for understanding the volume of data and comprehensiveness of the imaging series.

**License Short Name**
- **Description**: The licensing terms under which the imaging data is released, important for legal use and distribution of the data.

**Series AWS URL**
- **Description**: Construct a URL that provides direct access to the series data stored on Amazon Web Services (AWS). This URL should be composed of the standard S3 prefix followed by the extracted bucket name from the provided AWS URL, the unique series identifier, and a wildcard to include all related files. This enables straightforward access to the series for downloading or analysis purposes.

**Series Size in MB**
- **Description**: Calculate the total size of all the imaging files within a series in megabytes. This involves summing the sizes of individual image instances initially provided in bytes, converting this total into megabytes by dividing by 1,000,000, and rounding the result to two decimal places. This metric is crucial for understanding the data volume associated with the series, which aids in effective storage and processing planning.

This comprehensive data model serves as a robust framework for managing, querying, and analyzing medical imaging data, ensuring that each element is properly cataloged and accessible for clinical and research purposes.",snowflake
337,sf_bq455,IDC,"Identify the top five CT scan series by size (in MiB), including their SeriesInstanceUID, series number, patient ID, and series size. These series must be from the CT modality and not part of the 'nlst' collection. Exclude any series where the ImageType is classified as 'LOCALIZER' or where the TransferSyntaxUID is either '1.2.840.10008.1.2.4.70' or '1.2.840.10008.1.2.4.51' (i.e., JPEG compressed). The selected series must have consistent slice intervals, exposure levels, image orientation (with only one unique ImageOrientationPatient value), pixel spacing, image positions (both z-axis and xy positions), and pixel dimensions (rows and columns). Ensure that the number of images matches the number of unique z-axis positions, indicating no duplicate slices. Additionally, the z-axis component of the cross product of the x and y direction cosines from ImageOrientationPatient must have an absolute value between 0.99 and 1.01, ensuring alignment with the expected imaging plane. Finally, order the results by series size in descending order and limit the output to the top five series satisfying these conditions.","WITH
  -- Create a common table expression (CTE) named localizerAndJpegCompressedSeries
  localizerAndJpegCompressedSeries AS (
    SELECT 
      ""SeriesInstanceUID""
    FROM 
      IDC.IDC_V17.""DICOM_ALL"" AS bid
    WHERE 
      ""ImageType"" = 'LOCALIZER' OR
      ""TransferSyntaxUID"" IN ('1.2.840.10008.1.2.4.70', '1.2.840.10008.1.2.4.51')
  ),
  
  -- Create a common table expression (CTE) for x_vector calculation (first three elements)
  imageOrientation AS (
    SELECT
      ""SeriesInstanceUID"",
      ARRAY_AGG(CAST(part.value AS FLOAT)) AS ""x_vector""
    FROM 
      IDC.IDC_V17.""DICOM_ALL"" AS bid,
      LATERAL FLATTEN(input => bid.""ImageOrientationPatient"") AS part
    WHERE
      part.index BETWEEN 0 AND 2
    GROUP BY ""SeriesInstanceUID""
  ),
  
  -- Create a common table expression (CTE) for y_vector calculation (next three elements)
  imageOrientationY AS (
    SELECT
      ""SeriesInstanceUID"",
      ARRAY_AGG(CAST(part.value AS FLOAT)) AS ""y_vector""
    FROM 
      IDC.IDC_V17.""DICOM_ALL"" AS bid,
      LATERAL FLATTEN(input => bid.""ImageOrientationPatient"") AS part
    WHERE
      part.index BETWEEN 3 AND 5
    GROUP BY ""SeriesInstanceUID""
  ),
  
  -- Create a common table expression (CTE) named nonLocalizerRawData
  nonLocalizerRawData AS (
    SELECT
      bid.""SeriesInstanceUID"",  -- Added table alias bid
      bid.""StudyInstanceUID"",
      bid.""PatientID"",
      bid.""SOPInstanceUID"",
      bid.""SliceThickness"",
      bid.""ImageType"",
      bid.""TransferSyntaxUID"",
      bid.""SeriesNumber"",
      bid.""aws_bucket"",
      bid.""crdc_series_uuid"",
      CAST(bid.""Exposure"" AS FLOAT) AS ""Exposure"",  -- Use CAST directly
      CAST(ipp.value AS FLOAT) AS ""zImagePosition"", -- Use CAST directly
      CONCAT(ipp2.value, '/', ipp3.value) AS ""xyImagePosition"",
      LEAD(CAST(ipp.value AS FLOAT)) OVER (PARTITION BY bid.""SeriesInstanceUID"" ORDER BY CAST(ipp.value AS FLOAT)) - CAST(ipp.value AS FLOAT) AS ""slice_interval"",
      ARRAY_TO_STRING(bid.""ImageOrientationPatient"", '/') AS ""iop"",
      bid.""PixelSpacing"",
      bid.""Rows"" AS ""pixelRows"",
      bid.""Columns"" AS ""pixelColumns"",
      bid.""instance_size"" AS ""instanceSize""
    FROM
      IDC.IDC_V17.""DICOM_ALL"" AS bid
    LEFT JOIN LATERAL FLATTEN(input => bid.""ImagePositionPatient"") AS ipp
    LEFT JOIN LATERAL FLATTEN(input => bid.""ImagePositionPatient"") AS ipp2
    LEFT JOIN LATERAL FLATTEN(input => bid.""ImagePositionPatient"") AS ipp3
    WHERE
      bid.""collection_id"" != 'nlst'
      AND bid.""Modality"" = 'CT'
      AND ipp.index = 2
      AND ipp2.index = 0
      AND ipp3.index = 1
      AND bid.""SeriesInstanceUID"" NOT IN (SELECT ""SeriesInstanceUID"" FROM localizerAndJpegCompressedSeries)
  ),
  
  -- Cross product calculation
  crossProduct AS (
    SELECT
      nld.""SOPInstanceUID"",  -- Added table alias nld
      nld.""SeriesInstanceUID"",  -- Added table alias nld
      OBJECT_CONSTRUCT(
        'x', (""x_vector""[1] * ""y_vector""[2] - ""x_vector""[2] * ""y_vector""[1]),
        'y', (""x_vector""[2] * ""y_vector""[0] - ""x_vector""[0] * ""y_vector""[2]),
        'z', (""x_vector""[0] * ""y_vector""[1] - ""x_vector""[1] * ""y_vector""[0])
      ) AS ""xyCrossProduct""
    FROM 
      nonLocalizerRawData AS nld  -- Added alias for nonLocalizerRawData
    JOIN imageOrientation AS io ON nld.""SeriesInstanceUID"" = io.""SeriesInstanceUID""
    JOIN imageOrientationY AS ioy ON nld.""SeriesInstanceUID"" = ioy.""SeriesInstanceUID""
  ),
  
  -- Cross product elements extraction and row numbering
  crossProductElements AS (
    SELECT
      cp.""SOPInstanceUID"",  
      cp.""SeriesInstanceUID"",  
      elem.value,
      ROW_NUMBER() OVER (PARTITION BY cp.""SOPInstanceUID"", cp.""SeriesInstanceUID"" ORDER BY elem.value) AS rn
    FROM 
      crossProduct AS cp  
    -- Use LATERAL FLATTEN to explode the cross product object into individual 'x', 'y', and 'z'
    JOIN LATERAL FLATTEN(input => ARRAY_CONSTRUCT(
          cp.""xyCrossProduct""['x'],
          cp.""xyCrossProduct""['y'],
          cp.""xyCrossProduct""['z']
    )) AS elem -- Simplified 'elem.value' reference here
  ),
  
  -- Dot product calculation
  dotProduct AS (
    SELECT
      cpe.""SOPInstanceUID"",  
      cpe.""SeriesInstanceUID"",  
      SUM(
        CASE 
          WHEN cpe.rn = 1 THEN cpe.value * 0  -- x * 0
          WHEN cpe.rn = 2 THEN cpe.value * 0  -- y * 0
          WHEN cpe.rn = 3 THEN cpe.value * 1  -- z * 1
        END
      ) AS ""xyDotProduct""
    FROM 
      crossProductElements AS cpe
    GROUP BY 
      cpe.""SOPInstanceUID"",  
      cpe.""SeriesInstanceUID""
  ),
  
  -- Geometry checks for series consistency
  geometryChecks AS (
    SELECT
      gc.""SeriesInstanceUID"",  -- Added table alias gc
      gc.""SeriesNumber"",
      gc.""aws_bucket"",
      gc.""crdc_series_uuid"",
      gc.""StudyInstanceUID"",
      gc.""PatientID"",
      ARRAY_AGG(DISTINCT gc.""slice_interval"") AS ""sliceIntervalDifferences"",
      ARRAY_AGG(DISTINCT gc.""Exposure"") AS ""distinctExposures"",
      COUNT(DISTINCT gc.""iop"") AS ""iopCount"",
      COUNT(DISTINCT gc.""PixelSpacing"") AS ""pixelSpacingCount"",
      COUNT(DISTINCT gc.""zImagePosition"") AS ""positionCount"",
      COUNT(DISTINCT gc.""xyImagePosition"") AS ""xyPositionCount"",
      COUNT(DISTINCT gc.""SOPInstanceUID"") AS ""sopInstanceCount"",
      COUNT(DISTINCT gc.""SliceThickness"") AS ""sliceThicknessCount"",
      COUNT(DISTINCT gc.""Exposure"") AS ""exposureCount"",
      COUNT(DISTINCT gc.""pixelRows"") AS ""pixelRowCount"",
      COUNT(DISTINCT gc.""pixelColumns"") AS ""pixelColumnCount"",
      dp.""xyDotProduct"",  -- Added xyDotProduct from dotProduct
      SUM(gc.""instanceSize"") / 1024 / 1024 AS ""seriesSizeInMiB""
    FROM 
      nonLocalizerRawData AS gc  -- Added table alias gc
    JOIN dotProduct AS dp ON gc.""SeriesInstanceUID"" = dp.""SeriesInstanceUID"" 
    AND gc.""SOPInstanceUID"" = dp.""SOPInstanceUID""
    GROUP BY
      gc.""SeriesInstanceUID"", 
      gc.""SeriesNumber"",
      gc.""aws_bucket"",
      gc.""crdc_series_uuid"",
      gc.""StudyInstanceUID"",
      gc.""PatientID"",
      dp.""xyDotProduct""  -- Include xyDotProduct in GROUP BY
    HAVING
      COUNT(DISTINCT gc.""iop"") = 1 
      AND COUNT(DISTINCT gc.""PixelSpacing"") = 1  
      AND COUNT(DISTINCT gc.""SOPInstanceUID"") = COUNT(DISTINCT gc.""zImagePosition"") 
      AND COUNT(DISTINCT gc.""xyImagePosition"") = 1
      AND COUNT(DISTINCT gc.""pixelRows"") = 1 
      AND COUNT(DISTINCT gc.""pixelColumns"") = 1 
      AND ABS(dp.""xyDotProduct"") BETWEEN 0.99 AND 1.01
  )

SELECT
  geometryChecks.""SeriesInstanceUID"",  -- Added table alias
  geometryChecks.""SeriesNumber"",  -- Added table alias
  geometryChecks.""PatientID"",  -- Added table alias
  geometryChecks.""seriesSizeInMiB""
FROM
  geometryChecks
ORDER BY
  geometryChecks.""seriesSizeInMiB"" DESC
LIMIT 5;
",,snowflake
338,sf_bq456,IDC,"Please retrieve from the dicom_all table each PatientID, StudyInstanceUID, StudyDate, and the CodeMeaning of the FindingSite for patients whose StudyDate is in the year 2001, along with the maximum values of each of the following measurements identified by their CodeMeaning (Elongation, Flatness, Least Axis in 3D Length, Major Axis in 3D Length, Maximum 3D Diameter of a Mesh, Minor Axis in 3D Length, Sphericity, Surface Area of Mesh, Surface to Volume Ratio, Volume from Voxel Summation, and Volume of Mesh), ensuring that the quantitative_measurements table is joined on segmentationInstanceUID matching the SOPInstanceUID in dicom_all, and grouping by PatientID, StudyInstanceUID, StudyDate, and FindingSite CodeMeaning.",,,snowflake
339,sf_bq324,IDC,How many frames in total are present across all whole slide microscopy images from the TCGA-BRCA collection that use the SM modality and include an eosin-based staining step in their SpecimenPreparationSequence?,,,snowflake
340,bq418,targetome_reactome,"Determine which three lowest-level Reactome pathways (with TAS evidence) have the highest chi-squared statistics, considering only Homo sapiens targets associated with sorafenib under the conditions that the median assay value is ≤ 100 and both low and high assay values are ≤ 100 or null. For each of these three pathways, how many of these targets and non-targets lie within the pathway and outside it?",,,snowflake
341,bq330,fda,"Which Colorado zip code has the highest concentration of bank locations per block group, based on the overlap between zip codes and block groups?","WITH _fips AS (
    SELECT
        state_fips_code
    FROM
        `bigquery-public-data.census_utility.fips_codes_states`
    WHERE
        state_name = ""Colorado""
),

_bg AS (
    SELECT
        b.geo_id,
        b.blockgroup_geom,
        ST_AREA(b.blockgroup_geom) AS bg_size
    FROM
        `bigquery-public-data.geo_census_blockgroups.us_blockgroups_national` b
    JOIN
        _fips u ON b.state_fips_code = u.state_fips_code
),

_zip AS (
    SELECT
        z.zip_code,
        z.zip_code_geom
    FROM
        `bigquery-public-data.geo_us_boundaries.zip_codes` z
    JOIN
        _fips u ON z.state_fips_code = u.state_fips_code
),

bq_zip_overlap AS (
    SELECT
        b.geo_id,
        z.zip_code,
        ST_AREA(ST_INTERSECTION(b.blockgroup_geom, z.zip_code_geom)) / b.bg_size AS overlap_size,
        b.blockgroup_geom
    FROM
        _zip z
    JOIN
        _bg b ON ST_INTERSECTS(b.blockgroup_geom, z.zip_code_geom)
),

locations AS (
    SELECT
        SUM(overlap_size * count_locations) AS locations_per_bg,
        l.zip_code
    FROM (
        SELECT
            COUNT(CONCAT(institution_name, "" : "", branch_name)) AS count_locations,
            zip_code
        FROM
            `bigquery-public-data.fdic_banks.locations`
        WHERE
            state IS NOT NULL
            AND state_name IS NOT NULL
        GROUP BY
            zip_code
    ) l
    JOIN
        bq_zip_overlap ON l.zip_code = bq_zip_overlap.zip_code
    GROUP BY
        l.zip_code
)

SELECT
    l.zip_code
FROM
    locations l
GROUP BY
    l.zip_code
ORDER BY
    MAX(locations_per_bg) DESC
LIMIT 1;","# Calculation Method: Overlap Ratio and Bank Location Data

This document describes the method used to calculate the number of bank institutions per postal code area (ZIP code) by combining geospatial data and bank location data, focusing on the overlap between postal code areas and census block groups.

## 1. Geospatial Intersection of Postal Code Areas and Census Block Groups
We are using two geographical units:
- **ZIP Code Areas**: Represented by geometries from the ZIP code boundaries dataset.
- **Census Block Groups**: Represented by geometries from the national census block groups dataset.

### Key Calculation:
- The method calculates the area of overlap between each ZIP code's geometry and each block group's geometry.
- The ratio of this intersection area to the total block group area is computed as the **overlap ratio**.

This overlap ratio represents the proportion of a block group that falls within a given ZIP code.

## 2. Bank Location Distribution Based on Overlap Ratio
The next step involves distributing the number of bank locations to the overlapping census block groups based on the calculated overlap ratio.

### Key Calculation:
- For each block group, the number of bank locations is proportionally assigned based on the overlap size. The total number of bank locations in a ZIP code is distributed to the block groups using the overlap ratio.

This provides the number of bank institutions for each block group, adjusted for the overlap with ZIP code areas.

## 3. Aggregation by ZIP Code Area
Finally, the results are aggregated by ZIP code to determine which postal code has the highest number of bank institutions.

### Key Calculation:
- The process involves grouping by ZIP code and finding the maximum number of bank locations per block group.
",snowflake
342,bq398,world_bank,What are the top three debt indicators for Russia based on the highest debt values?,"WITH russia_Data as (
SELECT distinct 
  id.country_name,
  id.value, --format in DataStudio
  id.indicator_name
FROM (
  SELECT
    country_code,
    region
  FROM
    bigquery-public-data.world_bank_intl_debt.country_summary
  WHERE
    region != """" ) cs --aggregated countries do not have a region
INNER JOIN (
  SELECT
    country_code,
    country_name,
    value, 
    indicator_name
  FROM
    bigquery-public-data.world_bank_intl_debt.international_debt
  WHERE true
    and country_code = 'RUS'  
     ) id
ON
  cs.country_code = id.country_code
WHERE value is not null
ORDER BY
  id.value DESC
)
SELECT 
    indicator_name
FROM russia_data
LIMIT 3;",,snowflake
343,bq399,world_bank,"Which high-income country had the highest average crude birth rate respectively in each region, and what are their corresponding average birth rate, during the 1980s?","WITH country_data AS ( 
  SELECT 
    country_code, 
    short_name AS country,
    region, 
    income_group 
  FROM 
    bigquery-public-data.world_bank_wdi.country_summary
)
, birth_rate_data AS (
  SELECT 
    data.country_code, 
    country_data.country,
    country_data.region,
    AVG(value) AS avg_birth_rate
  FROM 
    bigquery-public-data.world_bank_wdi.indicators_data data 
  LEFT JOIN 
    country_data 
  ON 
    data.country_code = country_data.country_code
  WHERE 
    indicator_code = ""SP.DYN.CBRT.IN"" -- Birth Rate
    AND EXTRACT(YEAR FROM PARSE_DATE('%Y', CAST(year AS STRING))) BETWEEN 1980 AND 1989 -- 1980s
    AND country_data.income_group = ""High income"" -- High-income group
  GROUP BY 
    data.country_code, 
    country_data.country,
    country_data.region
)
, ranked_birth_rates AS (
  SELECT
    region,
    country,
    avg_birth_rate,
    RANK() OVER(PARTITION BY region ORDER BY avg_birth_rate DESC) AS rank
  FROM
    birth_rate_data
)
SELECT 
  region, 
  country, 
  avg_birth_rate
FROM 
  ranked_birth_rates
WHERE 
  rank = 1
ORDER BY 
  region;",,snowflake
344,bq457,libraries_io,"Get details of repositories that use specific feature toggle libraries. For each repository, include the full name with owner, hosting platform type, size in bytes, primary programming language, fork source name (if any), last update timestamp, the artifact and library names of the feature toggle used, and the library's programming languages. Include repositories that depend on the specified feature toggle libraries, defined by their artifact names, library names, platforms, and languages.",,"# Feature Toggle Libraries

## Libraries and Their Details

### .NET Libraries
- **Library:** unleash-client-dotnet
  - **Artifact Name:** Unleash.FeatureToggle.Client
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** unleash-client
  - **Artifact Name:** unleash.client
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** launchdarkly
  - **Artifact Name:** LaunchDarkly.Client
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** NFeature
  - **Artifact Name:** NFeature
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** FeatureToggle
  - **Artifact Name:** FeatureToggle
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** FeatureSwitcher
  - **Artifact Name:** FeatureSwitcher
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

- **Library:** Toggler
  - **Artifact Name:** Toggler
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

### Go Libraries
- **Library:** launchdarkly
  - **Artifact Name:** github.com/launchdarkly/go-client
  - **Platform:** Go
  - **Languages:** Go

- **Library:** Toggle
  - **Artifact Name:** github.com/xchapter7x/toggle
  - **Platform:** Go
  - **Languages:** Go

- **Library:** dcdr
  - **Artifact Name:** github.com/vsco/dcdr
  - **Platform:** Go
  - **Languages:** Go

- **Library:** unleash-client-go
  - **Artifact Name:** github.com/unleash/unleash-client-go
  - **Platform:** Go
  - **Languages:** Go

### JavaScript/TypeScript Libraries
- **Library:** unleash-client-node
  - **Artifact Name:** unleash-client
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** launchdarkly
  - **Artifact Name:** ldclient-js
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** ember-feature-flags
  - **Artifact Name:** ember-feature-flags
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** feature-toggles
  - **Artifact Name:** feature-toggles
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** React Feature Toggles
  - **Artifact Name:** @paralleldrive/react-feature-toggles
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** launchdarkly
  - **Artifact Name:** ldclient-node
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** flipit
  - **Artifact Name:** flipit
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** fflip
  - **Artifact Name:** fflip
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** Bandiera
  - **Artifact Name:** bandiera-client
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** flopflip
  - **Artifact Name:** @flopflip/react-redux
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

- **Library:** flopflip
  - **Artifact Name:** @flopflip/react-broadcast
  - **Platform:** NPM
  - **Languages:** JavaScript, TypeScript

### Maven/Kotlin/Java Libraries
- **Library:** launchdarkly
  - **Artifact Name:** com.launchdarkly:launchdarkly-android-client
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** toggle
  - **Artifact Name:** cc.soham:toggle
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** unleash-client-java
  - **Artifact Name:** no.finn.unleash:unleash-client-java
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** launchdarkly
  - **Artifact Name:** com.launchdarkly:launchdarkly-client
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** Togglz
  - **Artifact Name:** org.togglz:togglz-core
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** FF4J
  - **Artifact Name:** org.ff4j:ff4j-core
  - **Platform:** Maven
  - **Languages:** Kotlin, Java

- **Library:** Flip
  - **Artifact Name:** com.tacitknowledge.flip:core
  - **Platform:** Maven
  - **Languages:** Kotlin, Java



### .NET Libraries
- **Library:** unleash-client-dotnet
  - **Artifact Name:** Unleash.FeatureToggle.Client
  - **Platform:** NuGet
  - **Languages:** C#, Visual Basic

### iOS Libraries (CocoaPods, Carthage)
- **Library:** launchdarkly
  - **Artifact Name:** LaunchDarkly
  - **Platform:** CocoaPods
  - **Languages:** Objective-C, Swift

- **Library:** launchdarkly
  - **Artifact Name:** launchdarkly/ios-client
  - **Platform:** Carthage
  - **Languages:** Objective-C, Swift

### PHP Libraries (Packagist)
- **Library:** launchdarkly
  - **Artifact Name:** launchdarkly/launchdarkly-php
  - **Platform:** Packagist
  - **Languages:** PHP

- **Library:** Symfony FeatureFlagsBundle
  - **Artifact Name:** dzunke/feature-flags-bundle
  - **Platform:** Packagist
  - **Languages:** PHP

- **Library:** rollout
  - **Artifact Name:** opensoft/rollout
  - **Platform:** Packagist
  - **Languages:** PHP

- **Library:** Bandiera
  - **Artifact Name:** npg/bandiera-client-php
  - **Platform:** Packagist
  - **Languages:** PHP

### Python Libraries (Pypi)
- **Library:** unleash-client-python
  - **Artifact Name:** UnleashClient
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** launchdarkly
  - **Artifact Name:** ldclient-py
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Flask FeatureFlags
  - **Artifact Name:** Flask-FeatureFlags
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Gutter
  - **Artifact Name:** gutter
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Feature Ramp
  - **Artifact Name:** feature_ramp
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** flagon
  - **Artifact Name:** flagon
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Waffle
  - **Artifact Name:** django-waffle
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Gargoyle
  - **Artifact Name:** gargoyle
  - **Platform:** Pypi
  - **Languages:** Python

- **Library:** Gargoyle
  - **Artifact Name:** gargoyle-yplan
  - **Platform:** Pypi
  - **Languages:** Python

### Ruby Libraries (Rubygems)
- **Library:** unleash-client-ruby
  - **Artifact Name:** unleash
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** launchdarkly
  - **Artifact Name:** ldclient-rb
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** rollout
  - **Artifact Name:** rollout
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** FeatureFlipper
  - **Artifact Name:** feature_flipper
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** Flip
  - **Artifact Name:** flip
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** Setler
  - **Artifact Name:** setler
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** Bandiera
  - **Artifact Name:** bandiera-client
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** Feature
  - **Artifact Name:** feature
  - **Platform:** Rubygems
  - **Languages:** Ruby

- **Library:** Flipper
  - **Artifact Name:** flipper
  - **Platform:** Rubygems
  - **Languages:** Ruby

### Scala Libraries (Maven)
- **Library:** Bandiera
  - **Artifact Name:** com.springernature:bandiera-client-scala_2.12
  - **Platform:** Maven
  - **Languages:** Scala

- **Library:** Bandiera
  - **Artifact Name:** com.springernature:bandiera-client-scala_2.11
  - **Platform:** Maven
  - **Languages:** Scala",snowflake
345,bq227,london,"Could you provide the annual percentage shares, rounded to two decimal places, of the top 5 minor crime categories from 2008 in London's total crimes, with each year displayed in one row?","WITH top5_categories AS (
  SELECT minor_category
  FROM `bigquery-public-data.london_crime.crime_by_lsoa`
  WHERE year = 2008
  GROUP BY minor_category
  ORDER BY SUM(value) DESC
  LIMIT 5
),

total_crimes_per_year AS (
  SELECT 
    year, 
    SUM(value) AS total_crimes_year
  FROM `bigquery-public-data.london_crime.crime_by_lsoa`
  GROUP BY year
),

top5_crimes_per_year AS (
  SELECT
    year,
    minor_category,
    SUM(value) AS total_crimes_category_year
  FROM `bigquery-public-data.london_crime.crime_by_lsoa`
  WHERE minor_category IN (SELECT minor_category FROM top5_categories)
  GROUP BY year, minor_category
)

SELECT
  t.year,
  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 0) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 1`,
  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 1) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 2`,
  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 2) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 3`,
  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 3) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 4`,
  ROUND(SUM(CASE WHEN t.minor_category = (SELECT minor_category FROM top5_categories LIMIT 1 OFFSET 4) THEN t.total_crimes_category_year ELSE 0 END) / y.total_crimes_year * 100, 2) AS `Category 5`
FROM
  top5_crimes_per_year t
JOIN
  total_crimes_per_year y ON t.year = y.year
GROUP BY
  t.year, y.total_crimes_year
ORDER BY
  t.year;
",,snowflake
346,bq232,london,Could you provide the total number of 'Other Theft' incidents within the 'Theft and Handling' category for each year in the Westminster borough?,"WITH borough_data AS (
    SELECT 
        year, 
        month, 
        borough, 
        major_category, 
        minor_category, 
        SUM(value) AS total,
    CASE 
        WHEN 
            major_category = 'Theft and Handling' 
        THEN 
            'Theft and Handling'
        ELSE 
            'Other' 
    END AS major_division,
    CASE 
        WHEN 
            minor_category = 'Other Theft' THEN minor_category
        ELSE 
            'Other'
    END AS minor_division,
    FROM 
        bigquery-public-data.london_crime.crime_by_lsoa
    GROUP BY 1,2,3,4,5
    ORDER BY 1,2
)

SELECT year, SUM(total) AS year_total
FROM borough_data
WHERE 
    borough = 'Westminster'
AND
    major_division != 'Other'
AND 
    minor_division != 'Other'
GROUP BY year, major_division, minor_division
ORDER BY year;",,snowflake
347,bq228,london,"Please provide a list of the top three major crime categories in the borough of Barking and Dagenham, along with the number of incidents in each category.","WITH ranked_crimes AS (
    SELECT
        borough,
        major_category,
        RANK() OVER(PARTITION BY borough ORDER BY SUM(value) DESC) AS rank_per_borough,
        SUM(value) AS no_of_incidents
    FROM
        `bigquery-public-data.london_crime.crime_by_lsoa`
    GROUP BY
        borough,
        major_category
)

SELECT
    borough,
    major_category,
    rank_per_borough,
    no_of_incidents
FROM
    ranked_crimes
WHERE
    rank_per_borough <= 3
AND 
    borough = 'Barking and Dagenham'
ORDER BY
    borough,
    rank_per_borough;",,snowflake
348,bq229,open_images,"Using the bigquery-public-data.open_images dataset, can you provide a count of how many distinct image URLs are categorized as 'cat' (where the image has label '/m/01yrx' with confidence=1) and how many distinct image URLs are categorized as 'other' (meaning they have no cat label '/m/01yrx' at all)?",,,snowflake
349,bq230,usda_nass_agriculture,"Using the crops dataset, find the total 2022 production figures, measured in bushels, for corn from the 'FIELD CROPS' category and mushrooms from the 'HORTICULTURE' group for each U.S. state. Only include data rows where 'statisticcat_desc' is 'PRODUCTION', 'agg_level_desc' is 'STATE', 'value' is not null, and ensure that for corn the 'unit_desc' is 'BU'. Combine both results so that each state’s 2022 corn and mushroom totals are presented.",,,snowflake
350,bq326,world_bank,"Based on the World Bank global population dataset and the World Bank health nutrition population dataset, how many countries experienced an increase of more than 1% from the previous year to 2018 in both their total population and per capita current health expenditure (PPP)?",,,snowflake
351,bq424,world_bank,"List the top 10 countries with respect to the total amount of long-term external debt in descending order, excluding those without a specified region.","SELECT DISTINCT
  id.country_name,
  --cs.region,
  id.value AS debt,
  --id.indicator_code
FROM (
  SELECT
    country_code,
    region
  FROM
    `bigquery-public-data.world_bank_intl_debt.country_summary`
  WHERE
    region != """" ) cs
INNER JOIN (
  SELECT
    country_code,
    country_name,
    value,
    indicator_code
  FROM
    `bigquery-public-data.world_bank_intl_debt.international_debt`
  WHERE
    indicator_code = ""DT.AMT.DLXF.CD"") id

ON
  cs.country_code = id.country_code
ORDER BY
  id.value DESC
  LIMIT 10",,snowflake
352,bq327,world_bank,"How many debt indicators for Russia have a value of 0, excluding NULL values?","WITH russia_Data AS (
  SELECT DISTINCT 
    id.country_name,
    id.value, -- Format in DataStudio
    id.indicator_name
  FROM (
    SELECT
      country_code,
      region
    FROM
      bigquery-public-data.world_bank_intl_debt.country_summary
    WHERE
      region != """" -- Aggregated countries do not have a region
  ) cs -- Aggregated countries do not have a region
  INNER JOIN (
    SELECT
      country_code,
      country_name,
      value, 
      indicator_name
    FROM
      bigquery-public-data.world_bank_intl_debt.international_debt
    WHERE
      country_code = 'RUS'
  ) id
  ON
    cs.country_code = id.country_code
  WHERE value IS NOT NULL
)
-- Count the number of indicators with a value of 0 for Russia
SELECT 
  COUNT(*) AS number_of_indicators_with_zero
FROM 
  russia_Data
WHERE 
  value = 0;",,snowflake
353,bq328,world_bank,Which region has the highest median GDP (constant 2015 US$) value?,"WITH country_data AS (
  -- CTE for country descriptive data
  SELECT 
    country_code, 
    short_name AS country,
    region, 
    income_group 
  FROM 
    `bigquery-public-data.world_bank_wdi.country_summary`
),

gdp_data AS (
  -- Filter data to only include GDP values
  SELECT 
    data.country_code, 
    country,
    region,
    value AS gdp_value
  FROM 
    `bigquery-public-data.world_bank_wdi.indicators_data` data
  LEFT JOIN country_data
    ON data.country_code = country_data.country_code
  WHERE indicator_code = ""NY.GDP.MKTP.KD"" -- GDP Indicator
    AND country_data.region IS NOT NULL
    AND country_data.income_group IS NOT NULL
),

cal_median_gdp AS (
  -- Calculate the median GDP value for each region
  SELECT 
    region,
    APPROX_QUANTILES(gdp_value, 2)[OFFSET(1)] AS median_gdp
  FROM gdp_data
  GROUP BY region
)
-- Select the regions with their median GDP values
SELECT 
  region
FROM 
  cal_median_gdp
ORDER BY median_gdp DESC
LIMIT 1;",,snowflake
354,sf_bq331,META_KAGGLE,"Find the top three users who have authored the first message in forum topics, ranked in descending order by their message scores, where a message score is defined as the number of distinct users who voted on that message. For each of these users, provide their username and the absolute difference between their message score and the average message score across all first messages in forum topics.",,,snowflake
355,sf_bq380,META_KAGGLE,"Using the data from Forum Message Votes and Users tables, find the top three users who have received the most distinct upvotes on the Kaggle forum. For each of these users, list their username, the total number of distinct upvotes they have received (based on the ToUserId field), and the total number of distinct upvotes they have given (based on the FromUserId field), sorted by the number of upvotes received in descending order and restricted to only the top three results.",,,snowflake
356,sf_bq370,WIDE_WORLD_IMPORTERS,"How many customers have orders and invoices that match at the line-item level and, when aggregated, result in each customer having an equal count of orders and invoices as well as an identical total value for the orders and invoices?",,,snowflake
357,sf_bq371,WIDE_WORLD_IMPORTERS,"In the year 2013, considering each invoice’s total value as the product of unit price and quantity and grouping by the quarter (Q1, Q2, Q3, Q4) in which the invoice date occurs, what is the difference between the maximum and minimum average invoice values across these quarters?",,,snowflake
358,sf_bq372,WIDE_WORLD_IMPORTERS,"Among all orders that do not appear in the invoice table, for each customer category calculate the maximum lost order value, then determine which customer category’s maximum lost order value is closest to the overall average of these maximum lost order values across all categories?",,,snowflake
359,sf_bq373,WIDE_WORLD_IMPORTERS,"Using the invoice date to determine each month of the year 2014, and summing the total invoice line amounts for each customer across these months, what is the median of the resulting average monthly spending across all customers?",,,snowflake
360,bq393,hacker_news,"I want to identify users who had activity followed by inactivity. Specifically, find the user ID and their corresponding month number (counting from their first activity month) for the user with the highest month number who became inactive (no activity recorded) after their last recorded activity month. For this analysis, only consider data up until September 10, 2024, and ensure the month number represents the count of months since the user's first activity. The user should have at least one month where they were expected to be active (within their activity span) but actually had no records.",,,snowflake
361,bq403,irs_990,Which three years in 2012-2017 have the smallest absolute difference between median revenue and median functional expenses for organizations filing IRS 990 forms? Please output three years and respective differences.,"WITH RankedData AS (
    SELECT
        CONCAT(""20"", _TABLE_SUFFIX) AS year_filed,
        totrevenue,
        totfuncexpns,
        ROW_NUMBER() OVER (PARTITION BY CONCAT(""20"", _TABLE_SUFFIX) ORDER BY totrevenue) 
        AS revenue_rank,
        ROW_NUMBER() OVER (PARTITION BY CONCAT(""20"", _TABLE_SUFFIX) ORDER BY totfuncexpns) 
        AS expense_rank,
        COUNT(*) OVER (PARTITION BY CONCAT(""20"", _TABLE_SUFFIX)) AS total_count
    FROM 
        `bigquery-public-data.irs_990.irs_990_20*`
),

YearlyMedians AS (
    SELECT
        year_filed,
        IF(MOD(total_count, 2) = 1, 
           MAX(CASE WHEN revenue_rank = (total_count + 1) / 2 THEN totrevenue END),
           AVG(CASE WHEN revenue_rank IN ((total_count / 2), (total_count / 2) + 1) THEN totrevenue END)
        ) AS median_revenue,
        IF(MOD(total_count, 2) = 1, 
           MAX(CASE WHEN expense_rank = (total_count + 1) / 2 THEN totfuncexpns END),
           AVG(CASE WHEN expense_rank IN ((total_count / 2), (total_count / 2) + 1) THEN totfuncexpns END)
        ) AS median_expense
    FROM
        RankedData
    GROUP BY
        year_filed, total_count
),

DifferenceCalculations AS (
    SELECT
        year_filed,
        median_revenue,
        median_expense,
        ABS(median_revenue - median_expense) AS difference
    FROM
        YearlyMedians
)

SELECT
    year_filed,
    difference
FROM
    DifferenceCalculations
WHERE
    year_filed BETWEEN '2012' AND '2017'
ORDER BY
    difference ASC
LIMIT 3;",,snowflake
362,bq397,ecommerce,"After removing any duplicate records from the rev_transactions dataset, identify each channel grouping that has transactions from more than one country. For each such channel grouping, find the country with the highest total number of transactions and report both the country name and the sum of transactions for that channel grouping.","WITH tmp AS (
  SELECT DISTINCT *
  FROM `data-to-insights.ecommerce.rev_transactions`
  -- Removing duplicated values
),
tmp1 AS (
  SELECT 
    tmp.channelGrouping,
    tmp.geoNetwork_country,
    SUM(tmp.totals_transactions) AS tt
  FROM tmp
  GROUP BY 1, 2
),
tmp2 AS (
  SELECT 
    channelGrouping,
    geoNetwork_country,
    SUM(tt) AS TotalTransaction,
    COUNT(DISTINCT geoNetwork_country) OVER (PARTITION BY channelGrouping) AS CountryCount
  FROM tmp1
  GROUP BY channelGrouping, geoNetwork_country
),
tmp3 AS (
  SELECT
    channelGrouping,
    geoNetwork_country AS Country,
    TotalTransaction,
    RANK() OVER (PARTITION BY channelGrouping ORDER BY TotalTransaction DESC) AS rnk
  FROM tmp2
  WHERE CountryCount > 1
)
SELECT
  channelGrouping,
  Country,
  TotalTransaction
FROM tmp3
WHERE rnk = 1;",,snowflake
363,bq402,ecommerce,"Calculate the conversion rate from unique visitors to purchasers by using data exclusively from the `web_analytics` table in the `data-to-insights.ecommerce` dataset. A visitor is defined as a unique `fullVisitorId` present in the table, while a purchaser is a visitor who has at least one transaction recorded (`totals.transactions` is not null). The conversion rate is computed by dividing the number of unique purchasers by the total number of unique visitors. Additionally, calculate the average number of transactions per purchaser, considering only those visitors who have made at least one transaction.","WITH visitors AS (
  SELECT
    COUNT(DISTINCT fullVisitorId) AS total_visitors
  FROM 
    `data-to-insights.ecommerce.web_analytics`
),

purchasers AS (
  SELECT
    COUNT(DISTINCT fullVisitorId) AS total_purchasers
  FROM 
    `data-to-insights.ecommerce.web_analytics`
  WHERE 
    totals.transactions IS NOT NULL
),

transactions AS (
  SELECT
    COUNT(*) AS total_transactions,
    AVG(totals.transactions) AS avg_transactions_per_purchaser
  FROM 
    `data-to-insights.ecommerce.web_analytics`
  WHERE 
    totals.transactions IS NOT NULL
)

SELECT
  p.total_purchasers / v.total_visitors AS conversion_rate,
  a.avg_transactions_per_purchaser AS avg_transactions_per_purchaser
FROM
  visitors v,
  purchasers p,
  transactions a;",,snowflake
364,sf_bq160,META_KAGGLE,"Please provide the creation date, title, parent forum title, reply count, distinct user replies count, total upvotes, and total views for the earliest five forum topics that belong to any sub-forum under the parent forum titled ""General"". The reply count should be calculated as the total number of messages associated with each topic, and the distinct user replies count should be the number of unique users who have posted messages in the topic. The upvotes should be calculated as the total number of upvotes on all messages within each topic. If any values are missing or None, please treat them as zero",,,snowflake
365,sf_bq167,META_KAGGLE,"Identify the pair of Kaggle users involved in ForumMessageVotes such that one user has given the other the greatest distinct number of upvotes, then also display how many upvotes that recipient returned. Present the usernames of both users, the total distinct upvotes one received from the other, and the upvotes they gave back, sorting by the highest received count and then by the highest given count, and show only the top result.","WITH UserPairUpvotes AS (
  SELECT
    ToUsers.""UserName"" AS ""ToUserName"",
    FromUsers.""UserName"" AS ""FromUserName"",
    COUNT(DISTINCT ""ForumMessageVotes"".""Id"") AS ""UpvoteCount""
  FROM META_KAGGLE.META_KAGGLE.FORUMMESSAGEVOTES AS ""ForumMessageVotes""
  INNER JOIN META_KAGGLE.META_KAGGLE.USERS AS FromUsers
    ON FromUsers.""Id"" = ""ForumMessageVotes"".""FromUserId""
  INNER JOIN META_KAGGLE.META_KAGGLE.USERS AS ToUsers
    ON ToUsers.""Id"" = ""ForumMessageVotes"".""ToUserId""
  GROUP BY
    ToUsers.""UserName"",
    FromUsers.""UserName""
),
TopPairs AS (
  SELECT
    ""ToUserName"",
    ""FromUserName"",
    ""UpvoteCount"",
    ROW_NUMBER() OVER (ORDER BY ""UpvoteCount"" DESC) AS ""Rank""
  FROM UserPairUpvotes
),
ReciprocalUpvotes AS (
  SELECT
    t.""ToUserName"",
    t.""FromUserName"",
    t.""UpvoteCount"" AS ""UpvotesReceived"",
    COALESCE(u.""UpvoteCount"", 0) AS ""UpvotesGiven""
  FROM TopPairs t
  LEFT JOIN UserPairUpvotes u
    ON t.""ToUserName"" = u.""FromUserName"" AND t.""FromUserName"" = u.""ToUserName""
  WHERE t.""Rank"" = 1
)
SELECT
  ""ToUserName"" AS ""UpvotedUserName"",
  ""FromUserName"" AS ""UpvotingUserName"",
  ""UpvotesReceived"" AS ""UpvotesReceivedByUpvotedUser"",
  ""UpvotesGiven"" AS ""UpvotesGivenByUpvotedUser""
FROM ReciprocalUpvotes
ORDER BY ""UpvotesReceived"" DESC, ""UpvotesGiven"" DESC;
",,snowflake
366,sf_bq171,META_KAGGLE,"Whose Forum message upvotes are closest to the average in 2019? If there’s a tie, tell me the one with the alphabetically first username.",,,snowflake
367,sf_bq118,DEATH,"Among individuals identified as white, how much higher is the average number of deaths from ICD-10 codes whose descriptions contain the word “discharge” (specifically excluding “Urethral discharge,” “Discharge of firework,” and “Legal intervention involving firearm discharge”) compared to the average number of deaths from ICD-10 codes whose descriptions contain the word “vehicle,” when aggregated by age groups?",,,snowflake
368,sf_bq072,DEATH,"Please provide, for each age from 12 through 18 (inclusive), the total number of deaths and the number of deaths among individuals identified as Black (based on race descriptions containing the word ‘black’), specifically for deaths associated with ICD-10 codes whose descriptions include the word ‘vehicle’ and for deaths associated with ICD-10 codes whose descriptions include the word ‘firearm.’ Use the EntityAxisConditions table to determine which ICD-10 codes were involved in each death, rather than joining ICD-10 code information directly on the death records.","WITH BlackRace AS (
    SELECT CAST(""Code"" AS INT) AS CODE
    FROM DEATH.DEATH.RACE
    WHERE LOWER(""Description"") LIKE '%black%'
)
SELECT 
    v.""Age"", 
    v.""Total"" AS ""Vehicle_Total"", 
    v.""Black"" AS ""Vehicle_Black"",
    g.""Total"" AS ""Gun_Total"", 
    g.""Black"" AS ""Gun_Black""
FROM (
    SELECT 
        ""Age"", 
        COUNT(*) AS ""Total"", 
        COUNT_IF(""Race"" IN (SELECT CODE FROM BlackRace)) AS ""Black""
    FROM DEATH.DEATH.DEATHRECORDS d
    JOIN (
        SELECT 
            DISTINCT e.""DeathRecordId"" AS ""id""
        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e
        JOIN (
            SELECT * 
            FROM DEATH.DEATH.ICD10CODE 
            WHERE LOWER(""Description"") LIKE '%vehicle%'
        ) c 
        ON e.""Icd10Code"" = c.""Code""
    ) f
    ON d.""Id"" = f.""id""
    WHERE ""Age"" BETWEEN 12 AND 18
    GROUP BY ""Age""
) v  -- Vehicle

JOIN (
    SELECT 
        ""Age"", 
        COUNT(*) AS ""Total"", 
        COUNT_IF(""Race"" IN (SELECT CODE FROM BlackRace)) AS ""Black""
    FROM DEATH.DEATH.DEATHRECORDS d
    JOIN (
        SELECT 
            DISTINCT e.""DeathRecordId"" AS ""id""
        FROM DEATH.DEATH.ENTITYAXISCONDITIONS e
        JOIN (
            SELECT 
                ""Code"", ""Description"" 
            FROM DEATH.DEATH.ICD10CODE
            WHERE ""Description"" LIKE '%firearm%'
        ) c 
        ON e.""Icd10Code"" = c.""Code""
    ) f
    ON d.""Id"" = f.""id""
    WHERE ""Age"" BETWEEN 12 AND 18
    GROUP BY ""Age""
) g
ON g.""Age"" = v.""Age"";
",,snowflake
369,ga001,ga4,I want to know the preferences of customers who purchased the Google Navy Speckled Tee in December 2020. What other product was purchased with the highest total quantity alongside this item?,"WITH
  Params AS (
    SELECT 'Google Navy Speckled Tee' AS selected_product
  ),
  PurchaseEvents AS (
    SELECT
      user_pseudo_id,
      items
    FROM
      `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
    WHERE
      _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'
      AND event_name = 'purchase'
  ),
  ProductABuyers AS (
    SELECT DISTINCT
      user_pseudo_id
    FROM
      Params,
      PurchaseEvents,
      UNNEST(items) AS items
    WHERE
      items.item_name = selected_product
  )
SELECT
  items.item_name AS item_name,
  SUM(items.quantity) AS item_quantity
FROM
  Params,
  PurchaseEvents,
  UNNEST(items) AS items
WHERE
  user_pseudo_id IN (SELECT user_pseudo_id FROM ProductABuyers)
  AND items.item_name != selected_product
GROUP BY 1
ORDER BY item_quantity DESC
LIMIT 1;",,snowflake
370,ga002,ga4,Tell me the most purchased other products and their quantities by customers who bought the Google Red Speckled Tee each month for the three months starting from November 2020.,"WITH
Params AS (
  SELECT 'Google Red Speckled Tee' AS selected_product
),
DateRanges AS (
  SELECT '20201101' AS start_date, '20201130' AS end_date, '202011' AS period UNION ALL
  SELECT '20201201', '20201231', '202012' UNION ALL
  SELECT '20210101', '20210131', '202101'
),
PurchaseEvents AS (
  SELECT
    period,
    user_pseudo_id,
    items
  FROM
    DateRanges
  JOIN
    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
    ON _TABLE_SUFFIX BETWEEN start_date AND end_date
  WHERE
    event_name = 'purchase'
),
ProductABuyers AS (
  SELECT DISTINCT
    period,
    user_pseudo_id
  FROM
    Params,
    PurchaseEvents,
    UNNEST(items) AS items
  WHERE
    items.item_name = selected_product
),
TopProducts AS (
  SELECT
    pe.period,
    items.item_name AS item_name,
    SUM(items.quantity) AS item_quantity
  FROM
    Params,
    PurchaseEvents pe,
    UNNEST(items) AS items
  WHERE
    user_pseudo_id IN (SELECT user_pseudo_id FROM ProductABuyers pb WHERE pb.period = pe.period)
    AND items.item_name != selected_product
  GROUP BY
    pe.period, items.item_name
),
TopProductPerPeriod AS (
  SELECT
    period,
    item_name,
    item_quantity
  FROM (
    SELECT
      period,
      item_name,
      item_quantity,
      RANK() OVER (PARTITION BY period ORDER BY item_quantity DESC) AS rank
    FROM
      TopProducts
  )
  WHERE
    rank = 1
)
SELECT
  period,
  item_name,
  item_quantity
FROM
  TopProductPerPeriod
ORDER BY
  period;
",,snowflake
371,ga003,firebase,"I'm trying to evaluate which board types were most effective on September 15, 2018. Can you find out the average scores for each board type from the quick play mode completions on that day?","WITH EventData AS (
    SELECT 
        user_pseudo_id, 
        event_timestamp, 
        param
    FROM 
        `firebase-public-project.analytics_153293282.events_20180915`,
        UNNEST(event_params) AS param
    WHERE 
        event_name = ""level_complete_quickplay""
        AND (param.key = ""value"" OR param.key = ""board"")
),
ProcessedData AS (
    SELECT 
        user_pseudo_id, 
        event_timestamp, 
        MAX(IF(param.key = ""value"", param.value.int_value, NULL)) AS score,
        MAX(IF(param.key = ""board"", param.value.string_value, NULL)) AS board_type
    FROM 
        EventData
    GROUP BY 
        user_pseudo_id, 
        event_timestamp
)
SELECT 
    ANY_VALUE(board_type) AS board, 
    AVG(score) AS average_score
FROM 
    ProcessedData
GROUP BY 
    board_type
",,snowflake
372,ga004,ga4,Can you figure out the average difference in pageviews between users who bought something and those who didn’t in December 2020? Just label anyone who was involved in purchase events as a purchaser.,"WITH
  UserInfo AS (
    SELECT
      user_pseudo_id,
      COUNTIF(event_name = 'page_view') AS page_view_count,
      COUNTIF(event_name IN ('in_app_purchase', 'purchase')) AS purchase_event_count
    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
    WHERE _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'
    GROUP BY 1
  ),
  Averages AS (
    SELECT
      (purchase_event_count > 0) AS purchaser,
      COUNT(*) AS user_count,
      SUM(page_view_count) AS total_page_views,
      SUM(page_view_count) / COUNT(*) AS avg_page_views
    FROM UserInfo
    GROUP BY 1
  )

SELECT
  MAX(CASE WHEN purchaser THEN avg_page_views ELSE 0 END) -
  MAX(CASE WHEN NOT purchaser THEN avg_page_views ELSE 0 END) AS avg_page_views_difference
FROM Averages;",,snowflake
373,ga008,ga4,"Could you provide the total number of page views for each day in November 2020 as well as the average number of page views per user on those days, restricted to users who made at least one purchase in November 2020?","WITH
  UserInfo AS (
    SELECT
      user_pseudo_id,
      PARSE_DATE('%Y%m%d', event_date) AS event_date,
      COUNTIF(event_name = 'page_view') AS page_view_count,
      COUNTIF(event_name = 'purchase') AS purchase_event_count
    FROM `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
    WHERE _TABLE_SUFFIX BETWEEN '20201101' AND '20201130'
    GROUP BY 1, 2
  )
SELECT
  event_date,
  SUM(page_view_count) / COUNT(*) AS avg_page_views,
  SUM(page_view_count)
FROM UserInfo
WHERE purchase_event_count > 0
GROUP BY event_date
ORDER BY event_date;",,snowflake
374,ga017,ga4,How many distinct users viewed the most frequently visited page during January 2021?,"WITH unnested_events AS (
  SELECT
    MAX(CASE WHEN event_params.key = 'page_location' THEN event_params.value.string_value END) AS page_location,
    user_pseudo_id,
    event_timestamp
  FROM
    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`,
    UNNEST(event_params) AS event_params
  WHERE
    _TABLE_SUFFIX BETWEEN '20210101' AND '20210131'
    AND event_name = 'page_view'
  GROUP BY user_pseudo_id,event_timestamp
),
temp AS (
    SELECT
    page_location,
    COUNT(*) AS event_count,
    COUNT(DISTINCT user_pseudo_id) AS users
    FROM
    unnested_events
    GROUP BY page_location
    ORDER BY event_count DESC
)

SELECT users 
FROM
temp
LIMIT 1",,snowflake
375,ga007,ga4,"Please find out what percentage of the page views on January 2, 2021, were for PDP type pages.",,"### Refined Page Classification Criteria

#### Overview
To enhance our understanding of user engagement on our e-commerce platform, we differentiate between two types of pages based on the URL structure: Product Listing Pages (PLPs) and Product Detail Pages (PDPs). These classifications are crucial for analyzing user behavior and improving site navigation efficiency.

#### Product Listing Pages (PLPs)
PLPs are identified by specific characteristics in the URL:
- The URL must be divided into at least five segments.
- Neither the fourth nor the fifth segment contains a '+' sign, ensuring these are not detail views.
- The fourth or fifth segment must contain one of the following category names, indicating a broader category or collection page rather than a specific product focus:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

#### Product Detail Pages (PDPs)
PDPs, which focus on individual products, are marked by:
- A URL split into at least five segments, akin to PLPs.
- The presence of a '+' sign in the last segment, a common marker for detailed product pages.
- The fourth or fifth segment must also include one of the specified category names, ensuring that the detail being viewed pertains to one of the recognized product categories:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

### Note
Please note that the page classification keywords are case-insensitive. Additionally, when identifying URLs as described above, spaces are typically replaced by the '+' sign.
",snowflake
376,ga013,ga4,"I want to know all the pages visited by user 1402138.5184246691 on January 2, 2021. Please show the names of these pages and adjust the names to PDP or PLP where necessary.",,"### Refined Page Classification Criteria

#### Overview
To enhance our understanding of user engagement on our e-commerce platform, we differentiate between two types of pages based on the URL structure: Product Listing Pages (PLPs) and Product Detail Pages (PDPs). These classifications are crucial for analyzing user behavior and improving site navigation efficiency.

#### Product Listing Pages (PLPs)
PLPs are identified by specific characteristics in the URL:
- The URL must be divided into at least five segments.
- Neither the fourth nor the fifth segment contains a '+' sign, ensuring these are not detail views.
- The fourth or fifth segment must contain one of the following category names, indicating a broader category or collection page rather than a specific product focus:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

#### Product Detail Pages (PDPs)
PDPs, which focus on individual products, are marked by:
- A URL split into at least five segments, akin to PLPs.
- The presence of a '+' sign in the last segment, a common marker for detailed product pages.
- The fourth or fifth segment must also include one of the specified category names, ensuring that the detail being viewed pertains to one of the recognized product categories:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

### Note
Please note that the page classification keywords are case-insensitive. Additionally, when identifying URLs as described above, spaces are typically replaced by the '+' sign.
",snowflake
377,ga018,ga4,"On January 2nd, 2021, I want to determine the percentage of times users transition from a product list page (PLP) view to a product detail page (PDP) view within the same session, using only page_view events. Could you calculate how many PLP views eventually led to a PDP view in the same session on that date, and then provide the resulting percentage of PLP-to-PDP transitions?","WITH base_table AS (
  SELECT
    event_name,
    event_date,
    event_timestamp,
    user_pseudo_id,
    user_id,
    device,
    geo,
    traffic_source,
    event_params,
    user_properties
  FROM
    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
  WHERE
    _table_suffix = '20210102'
  AND event_name IN ('page_view')
)
, unnested_events AS (
-- unnests event parameters to get to relevant keys and values
  SELECT
    event_date AS date,
    event_timestamp AS event_timestamp_microseconds,
    user_pseudo_id,
    MAX(CASE WHEN c.key = 'ga_session_id' THEN c.value.int_value END) AS visitID,
    MAX(CASE WHEN c.key = 'ga_session_number' THEN c.value.int_value END) AS visitNumber,
    MAX(CASE WHEN c.key = 'page_title' THEN c.value.string_value END) AS page_title,
    MAX(CASE WHEN c.key = 'page_location' THEN c.value.string_value END) AS page_location
  FROM 
    base_table,
    UNNEST (event_params) c
  GROUP BY 1,2,3
)

, unnested_events_categorised AS (
-- categorizing Page Titles into PDPs and PLPs
  SELECT
  *,
  CASE WHEN ARRAY_LENGTH(SPLIT(page_location, '/')) >= 5 
            AND
            CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+')
            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN 
                                        ('accessories','apparel','brands','campus+collection','drinkware',
                                          'electronics','google+redesign',
                                          'lifestyle','nest','new+2015+logo','notebooks+journals',
                                          'office','shop+by+brand','small+goods','stationery','wearables'
                                          )
                  OR
                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN 
                                        ('accessories','apparel','brands','campus+collection','drinkware',
                                          'electronics','google+redesign',
                                          'lifestyle','nest','new+2015+logo','notebooks+journals',
                                          'office','shop+by+brand','small+goods','stationery','wearables'
                                          )
            )
            THEN 'PDP'
            WHEN NOT(CONTAINS_SUBSTR(ARRAY_REVERSE(SPLIT(page_location, '/'))[SAFE_OFFSET(0)], '+'))
            AND (LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(4)]) IN 
                                        ('accessories','apparel','brands','campus+collection','drinkware',
                                          'electronics','google+redesign',
                                          'lifestyle','nest','new+2015+logo','notebooks+journals',
                                          'office','shop+by+brand','small+goods','stationery','wearables'
                                          )
                  OR 
                  LOWER(SPLIT(page_location, '/')[SAFE_OFFSET(3)]) IN 
                                          ('accessories','apparel','brands','campus+collection','drinkware',
                                            'electronics','google+redesign',
                                            'lifestyle','nest','new+2015+logo','notebooks+journals',
                                            'office','shop+by+brand','small+goods','stationery','wearables'
                                            )
            )
            THEN 'PLP'
        ELSE page_title
        END AS page_title_adjusted 

  FROM 
    unnested_events
)


, ranked_screens AS (
  SELECT
    *,
    LAG(page_title_adjusted,1) OVER (PARTITION BY  user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC) previous_page,
    LEAD(page_title_adjusted,1) OVER (PARTITION BY  user_pseudo_id, visitID ORDER BY event_timestamp_microseconds ASC)  next_page
  FROM 
    unnested_events_categorised

)

,PLPtoPDPTransitions AS (
  SELECT
    user_pseudo_id,
    visitID
  FROM
    ranked_screens
  WHERE
    page_title_adjusted = 'PLP' AND next_page = 'PDP'
)

,TotalPLPViews AS (
  SELECT
    COUNT(*) AS total_plp_views
  FROM
    ranked_screens
  WHERE
    page_title_adjusted = 'PLP'
)

,TotalTransitions AS (
  SELECT
    COUNT(*) AS total_transitions
  FROM
    PLPtoPDPTransitions
)

SELECT
  (total_transitions * 100.0) / total_plp_views AS percentage
FROM
  TotalTransitions, TotalPLPViews;","### Refined Page Classification Criteria

#### Overview
To enhance our understanding of user engagement on our e-commerce platform, we differentiate between two types of pages based on the URL structure: Product Listing Pages (PLPs) and Product Detail Pages (PDPs). These classifications are crucial for analyzing user behavior and improving site navigation efficiency.

#### Product Listing Pages (PLPs)
PLPs are identified by specific characteristics in the URL:
- The URL must be divided into at least five segments.
- Neither the fourth nor the fifth segment contains a '+' sign, ensuring these are not detail views.
- The fourth or fifth segment must contain one of the following category names, indicating a broader category or collection page rather than a specific product focus:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

#### Product Detail Pages (PDPs)
PDPs, which focus on individual products, are marked by:
- A URL split into at least five segments, akin to PLPs.
- The presence of a '+' sign in the last segment, a common marker for detailed product pages.
- The fourth or fifth segment must also include one of the specified category names, ensuring that the detail being viewed pertains to one of the recognized product categories:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

### Note
Please note that the page classification keywords are case-insensitive. Additionally, when identifying URLs as described above, spaces are typically replaced by the '+' sign.
",snowflake
378,ga032,ga4,"Can you generate the navigation flow for user with pseudo_id '1362228.4966015575' on January 28th 2021, showing only the page_view events? Please connect the page titles with '>>' between each step, convert product detail pages to 'PDP' and product listing pages to 'PLP' based on the URL structure, and merge adjacent identical pages so they only appear once in the sequence. I need to understand how this specific user navigated through our website on that day.",,"### Refined Page Classification Criteria

#### Overview
To enhance our understanding of user engagement on our e-commerce platform, we differentiate between two types of pages based on the URL structure: Product Listing Pages (PLPs) and Product Detail Pages (PDPs). These classifications are crucial for analyzing user behavior and improving site navigation efficiency.

#### Product Listing Pages (PLPs)
PLPs are identified by specific characteristics in the URL:
- The URL must be divided into at least five segments.
- Neither the fourth nor the fifth segment contains a '+' sign, ensuring these are not detail views.
- The fourth or fifth segment must contain one of the following category names, indicating a broader category or collection page rather than a specific product focus:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

#### Product Detail Pages (PDPs)
PDPs, which focus on individual products, are marked by:
- A URL split into at least five segments, akin to PLPs.
- The presence of a '+' sign in the last segment, a common marker for detailed product pages.
- The fourth or fifth segment must also include one of the specified category names, ensuring that the detail being viewed pertains to one of the recognized product categories:
  - Accessories
  - Apparel
  - Brands
  - Campus Collection
  - Drinkware
  - Electronics
  - Google Redesign
  - Lifestyle
  - Nest
  - New 2015 Logo
  - Notebooks Journals
  - Office
  - Shop by Brand
  - Small Goods
  - Stationery
  - Wearables

### Note
Please note that the page classification keywords are case-insensitive. Additionally, when identifying URLs as described above, spaces are typically replaced by the '+' sign.
",snowflake
379,ga031,ga4,"I want to know the user session conversion rate on January 2nd, 2021, using only 'page_view' events. The conversion rate should be calculated as the percentage of user visits that reached both the Home and Checkout Confirmation pages in one session, relative to those that landed on the Home page.",,,snowflake
380,ga006,ga4,"For the date range November 1–30, 2020, can you retrieve each user_pseudo_id and its average purchase revenue in USD per session for users who had more than one purchase session, considering only events with event_name='purchase' and a non-null ecommerce.purchase_revenue_in_usd, grouping sessions by the ga_session_id from event_params",,,snowflake
381,ga009,ga4,"Could you tell me the average number of engaged sessions per user for December 2020, counting only those sessions where the event parameter 'session_engaged' is equal to '1' and using 'user_pseudo_id' combined with the 'ga_session_id' to identify distinct sessions?",,,snowflake
382,ga010,ga4,Can you give me an overview of our website traffic for December 2020? I'm particularly interested in the channel with the fourth highest number of sessions.,"WITH prep AS (
  SELECT
    user_pseudo_id,
    (SELECT value.int_value FROM UNNEST(event_params) WHERE key = 'ga_session_id') AS session_id,
    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'source') IGNORE NULLS 
              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS source,
    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'medium') IGNORE NULLS 
              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS medium,
    ARRAY_AGG((SELECT value.string_value FROM UNNEST(event_params) WHERE key = 'campaign') IGNORE NULLS 
              ORDER BY event_timestamp)[SAFE_OFFSET(0)] AS campaign
  FROM
    `bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_*`
  WHERE
    _TABLE_SUFFIX BETWEEN '20201201' AND '20201231'
  GROUP BY
    user_pseudo_id,
    session_id
)
SELECT
  -- session default channel grouping (dimension | the channel group associated with a session) 
  CASE 
    WHEN source = '(direct)' AND (medium IN ('(not set)','(none)')) THEN 'Direct'
    WHEN REGEXP_CONTAINS(campaign, 'cross-network') THEN 'Cross-network'
    WHEN (REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')
        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$'))
        AND REGEXP_CONTAINS(medium, '^(.*cp.*|ppc|paid.*)$') THEN 'Paid Shopping'
    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')
        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Search'
    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')
        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Social'
    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')
        AND REGEXP_CONTAINS(medium,'^(.*cp.*|ppc|paid.*)$') THEN 'Paid Video'
    WHEN medium IN ('display', 'banner', 'expandable', 'interstitial', 'cpm') THEN 'Display'
    WHEN REGEXP_CONTAINS(source,'alibaba|amazon|google shopping|shopify|etsy|ebay|stripe|walmart')
        OR REGEXP_CONTAINS(campaign, '^(.*(([^a-df-z]|^)shop|shopping).*)$') THEN 'Organic Shopping'
    WHEN REGEXP_CONTAINS(source,'badoo|facebook|fb|instagram|linkedin|pinterest|tiktok|twitter|whatsapp')
        OR medium IN ('social','social-network','social-media','sm','social network','social media') THEN 'Organic Social'
    WHEN REGEXP_CONTAINS(source,'dailymotion|disneyplus|netflix|youtube|vimeo|twitch|vimeo|youtube')
        OR REGEXP_CONTAINS(medium,'^(.*video.*)$') THEN 'Organic Video'
    WHEN REGEXP_CONTAINS(source,'baidu|bing|duckduckgo|ecosia|google|yahoo|yandex')
        OR medium = 'organic' THEN 'Organic Search'
    WHEN REGEXP_CONTAINS(source,'email|e-mail|e_mail|e mail')
        OR REGEXP_CONTAINS(medium,'email|e-mail|e_mail|e mail') THEN 'Email'
    WHEN medium = 'affiliate' THEN 'Affiliates'
    WHEN medium = 'referral' THEN 'Referral'
    WHEN medium = 'audio' THEN 'Audio'
    WHEN medium = 'sms' THEN 'SMS'
    WHEN medium LIKE '%push'
        OR REGEXP_CONTAINS(medium,'mobile|notification') THEN 'Mobile Push Notifications'
    ELSE 'Unassigned' 
  END AS channel_grouping_session
FROM
  prep
GROUP BY
  channel_grouping_session
ORDER BY
  COUNT(DISTINCT CONCAT(user_pseudo_id, session_id)) DESC
LIMIT 1 OFFSET 3","# Channel Group

| Channel                | Description                                                                                                                                                                |
|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Affiliates             | Affiliates is the channel by which users arrive at your site/app via links on affiliate sites.                                                                              |
| Audio                  | Audio is the channel by which users arrive at your site/app via ads on audio platforms (e.g., podcast platforms).                                                           |
| Cross-network          | Cross-network is the channel by which users arrive at your site/app via ads that appear on a variety of networks (e.g., Search and Display).                                 |
| Direct                 | Direct is the channel by which users arrive at your site/app via a saved link or by entering your URL.                                                                      |
| Display                | Display is the channel by which users arrive at your site/app via display ads, including ads on the Google Display Network.                                                 |
| Email                  | Email is the channel by which users arrive at your site/app via links in email.                                                                                             |
| Mobile Push Notifications | Mobile Push Notifications is the channel by which users arrive at your site/app via links in mobile-device messages when they're not actively using the app.                 |
| Organic Search         | Organic Search is the channel by which users arrive at your site/app via non-ad links in organic-search results.                                                            |
| Organic Shopping       | Organic Shopping is the channel by which users arrive at your site/app via non-ad links on shopping sites like Amazon or eBay.                                              |
| Organic Social         | Organic Social is the channel by which users arrive at your site/app via non-ad links on social sites like Facebook or Twitter.                                             |
| Organic Video          | Organic Video is the channel by which users arrive at your site/app via non-ad links on video sites like YouTube, TikTok, or Vimeo.                                         |
| Paid Search            | Paid Search is the channel by which users arrive at your site/app via ads on search-engine sites like Bing, Baidu, or Google.                                               |
| Paid Shopping          | Paid Shopping is the channel by which users arrive at your site/app via paid ads on shopping sites like Amazon or eBay or on individual retailer sites.                     |
| Paid Social            | Paid Social is the channel by which users arrive at your site/app via ads on social sites like Facebook and Twitter.                                                        |
| Paid Video             | Paid Video is the channel by which users arrive at your site/app via ads on video sites like TikTok, Vimeo, and YouTube.                                                   |
| Referral               | Referral is the channel by which users arrive at your site via non-ad links on other sites/apps (e.g., blogs, news sites).                                                  |
| SMS                    | SMS is the channel by which users arrive at your site/app via links from text messages.                                                                                     |
| Unassigned            | Others                        |



| Channel                   | Conditions                                                                                                                                                                         |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Direct**                | Source exactly matches ""(direct)""<br>AND<br>Medium is one of (""(not set)"", ""(none)"")                                                                                                |
| **Cross-network**         | Campaign Name contains ""cross-network""<br>Cross-network includes Demand Gen, Performance Max and Smart Shopping.                                                                   |
| **Paid Shopping**         | Source matches a list of shopping sites (alibaba, amazon, google shopping, shopify, etsy, ebay, stripe, walmart)<br>OR<br>Campaign Name matches regex `^(.*(([^a-df-z]\|^)shop\|shopping).*)$`<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$` |
| **Paid Search**           | Source matches a list of search sites (baidu,bing,duckduckgo,ecosia,google,yahoo,yandex)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|paid.*)$`|
| **Paid Social**           | Source matches a regex list of social sites (badoo,facebook,fb,instagram,linkedin,pinterest,tiktok,twitter,whatsapp)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$`                                                                     |
| **Paid Video**            | Source matches a list of video sites (dailymotion,disneyplus,netflix,youtube,vimeo,twitch,vimeo,youtube)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$`                                                                            |
| **Display**               | Medium is one of (“display”, “banner”, “expandable”, “interstitial”, “cpm”)                                                                                                        |
| **Organic Shopping**      | Source matches a list of shopping sites (alibaba,amazon,google shopping,shopify,etsy,ebay,stripe,walmart)<br>OR<br>Campaign name matches regex `^(.*(([^a-df-z]\|^)shop\|shopping).*)$`                                                                 |
| **Organic Social**        | Source matches a regex list of social sites (badoo,facebook,fb,instagram,linkedin,pinterest,tiktok,twitter,whatsapp)<br>OR<br>Medium is one of (“social”, “social-network”, “social-media”, “sm”, “social network”, “social media”)                          |
| **Organic Video**         | Source matches a list of video sites (dailymotion,disneyplus,netflix,youtube,vimeo,twitch,vimeo,youtube)<br>OR<br>Medium matches regex `^(.*video.*)$`                                                                                                  |
| **Organic Search**        | Source matches a list of search sites (baidu,bing,duckduckgo,ecosia,google,yahoo,yandex)<br>OR<br>Medium exactly matches organic                                                                                 |
| **Referral**              | Medium exactly matches Referral                                                                                                    |
| **Email**                 | Source = email\|e-mail\|e_mail\|e mail<br>OR<br>Medium = email\|e-mail\|e_mail\|e mail                                                                                              |
| **Affiliates**            | Medium = affiliate                                                                                                                                                                 |
| **Audio**                 | Medium exactly matches audio                                                                                                                                                       |
| **SMS**                   | Source exactly matches sms<br>OR<br>Medium exactly matches sms                                                                                                                      |
| **Mobile Push Notifications** | Medium ends with ""push""<br>OR<br>Medium contains ""mobile"" or ""notification""                                                          |
| **Unassigned** | Others                                     |

",snowflake
383,ga014,ga4,"Can you provide the total number of sessions for each traffic channel in December 2020, using the information from the 'event_params' ",,"# Channel Group

| Channel                | Description                                                                                                                                                                |
|------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Affiliates             | Affiliates is the channel by which users arrive at your site/app via links on affiliate sites.                                                                              |
| Audio                  | Audio is the channel by which users arrive at your site/app via ads on audio platforms (e.g., podcast platforms).                                                           |
| Cross-network          | Cross-network is the channel by which users arrive at your site/app via ads that appear on a variety of networks (e.g., Search and Display).                                 |
| Direct                 | Direct is the channel by which users arrive at your site/app via a saved link or by entering your URL.                                                                      |
| Display                | Display is the channel by which users arrive at your site/app via display ads, including ads on the Google Display Network.                                                 |
| Email                  | Email is the channel by which users arrive at your site/app via links in email.                                                                                             |
| Mobile Push Notifications | Mobile Push Notifications is the channel by which users arrive at your site/app via links in mobile-device messages when they're not actively using the app.                 |
| Organic Search         | Organic Search is the channel by which users arrive at your site/app via non-ad links in organic-search results.                                                            |
| Organic Shopping       | Organic Shopping is the channel by which users arrive at your site/app via non-ad links on shopping sites like Amazon or eBay.                                              |
| Organic Social         | Organic Social is the channel by which users arrive at your site/app via non-ad links on social sites like Facebook or Twitter.                                             |
| Organic Video          | Organic Video is the channel by which users arrive at your site/app via non-ad links on video sites like YouTube, TikTok, or Vimeo.                                         |
| Paid Search            | Paid Search is the channel by which users arrive at your site/app via ads on search-engine sites like Bing, Baidu, or Google.                                               |
| Paid Shopping          | Paid Shopping is the channel by which users arrive at your site/app via paid ads on shopping sites like Amazon or eBay or on individual retailer sites.                     |
| Paid Social            | Paid Social is the channel by which users arrive at your site/app via ads on social sites like Facebook and Twitter.                                                        |
| Paid Video             | Paid Video is the channel by which users arrive at your site/app via ads on video sites like TikTok, Vimeo, and YouTube.                                                   |
| Referral               | Referral is the channel by which users arrive at your site via non-ad links on other sites/apps (e.g., blogs, news sites).                                                  |
| SMS                    | SMS is the channel by which users arrive at your site/app via links from text messages.                                                                                     |
| Unassigned            | Others                        |



| Channel                   | Conditions                                                                                                                                                                         |
|---------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| **Direct**                | Source exactly matches ""(direct)""<br>AND<br>Medium is one of (""(not set)"", ""(none)"")                                                                                                |
| **Cross-network**         | Campaign Name contains ""cross-network""<br>Cross-network includes Demand Gen, Performance Max and Smart Shopping.                                                                   |
| **Paid Shopping**         | Source matches a list of shopping sites (alibaba, amazon, google shopping, shopify, etsy, ebay, stripe, walmart)<br>OR<br>Campaign Name matches regex `^(.*(([^a-df-z]\|^)shop\|shopping).*)$`<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$` |
| **Paid Search**           | Source matches a list of search sites (baidu,bing,duckduckgo,ecosia,google,yahoo,yandex)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|paid.*)$`|
| **Paid Social**           | Source matches a regex list of social sites (badoo,facebook,fb,instagram,linkedin,pinterest,tiktok,twitter,whatsapp)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$`                                                                     |
| **Paid Video**            | Source matches a list of video sites (dailymotion,disneyplus,netflix,youtube,vimeo,twitch,vimeo,youtube)<br>AND<br>Medium matches regex `^(.*cp.*\|ppc\|retargeting\|paid.*)$`                                                                            |
| **Display**               | Medium is one of (“display”, “banner”, “expandable”, “interstitial”, “cpm”)                                                                                                        |
| **Organic Shopping**      | Source matches a list of shopping sites (alibaba,amazon,google shopping,shopify,etsy,ebay,stripe,walmart)<br>OR<br>Campaign name matches regex `^(.*(([^a-df-z]\|^)shop\|shopping).*)$`                                                                 |
| **Organic Social**        | Source matches a regex list of social sites (badoo,facebook,fb,instagram,linkedin,pinterest,tiktok,twitter,whatsapp)<br>OR<br>Medium is one of (“social”, “social-network”, “social-media”, “sm”, “social network”, “social media”)                          |
| **Organic Video**         | Source matches a list of video sites (dailymotion,disneyplus,netflix,youtube,vimeo,twitch,vimeo,youtube)<br>OR<br>Medium matches regex `^(.*video.*)$`                                                                                                  |
| **Organic Search**        | Source matches a list of search sites (baidu,bing,duckduckgo,ecosia,google,yahoo,yandex)<br>OR<br>Medium exactly matches organic                                                                                 |
| **Referral**              | Medium exactly matches Referral                                                                                                    |
| **Email**                 | Source = email\|e-mail\|e_mail\|e mail<br>OR<br>Medium = email\|e-mail\|e_mail\|e mail                                                                                              |
| **Affiliates**            | Medium = affiliate                                                                                                                                                                 |
| **Audio**                 | Medium exactly matches audio                                                                                                                                                       |
| **SMS**                   | Source exactly matches sms<br>OR<br>Medium exactly matches sms                                                                                                                      |
| **Mobile Push Notifications** | Medium ends with ""push""<br>OR<br>Medium contains ""mobile"" or ""notification""                                                          |
| **Unassigned** | Others                                     |

",snowflake
384,ga011,ga4,"What is the page with the second highest total page views, after cleaning up its URL (removing extra slashes) and extracting the correct page path, on the website 'shop.googlemerchandisestore.com' during December 2020?",,,snowflake
385,ga012,ga4,"On November 30, 2020, identify the item category with the highest tax rate by dividing tax value in usd by purchase revenue in usd for purchase events, and then retrieve the transaction IDs, total item quantities, and both purchase revenue in usd and purchase revenue for those purchase events in that top-tax-rate category.","WITH top_category AS (
  SELECT
    product.item_category,
    SUM(ecommerce.tax_value_in_usd) / SUM(ecommerce.purchase_revenue_in_usd) AS tax_rate
  FROM
    bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201130,
    UNNEST(items) AS product
  WHERE
    event_name = 'purchase'
  GROUP BY
    product.item_category
  ORDER BY
    tax_rate DESC
  LIMIT 1
)

SELECT
    ecommerce.transaction_id,
    SUM(ecommerce.total_item_quantity) AS total_item_quantity,
    SUM(ecommerce.purchase_revenue_in_usd) AS purchase_revenue_in_usd,
    SUM(ecommerce.purchase_revenue) AS purchase_revenue
FROM
    bigquery-public-data.ga4_obfuscated_sample_ecommerce.events_20201130, 
    UNNEST(items) AS product
JOIN top_category
ON product.item_category = top_category.item_category
WHERE
    event_name = 'purchase'
GROUP BY
    ecommerce.transaction_id;",,snowflake
386,ga019,firebase,Could you determine what percentage of users either did not uninstall our app within seven days or never uninstalled it after installing during August and September 2018?,"WITH
--List of users who installed
sept_cohort AS (
  SELECT DISTINCT user_pseudo_id,
  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_first_open,
  FROM `firebase-public-project.analytics_153293282.events_*`
  WHERE event_name = 'first_open'
  AND _TABLE_SUFFIX BETWEEN '20180801' and '20180930'
),
--Get the list of users who uninstalled
uninstallers AS (
  SELECT DISTINCT user_pseudo_id,
  FORMAT_DATE('%Y-%m-%d', PARSE_DATE('%Y%m%d', event_date)) AS date_app_remove,
  FROM `firebase-public-project.analytics_153293282.events_*`
  WHERE event_name = 'app_remove'
  AND _TABLE_SUFFIX BETWEEN '20180801' and '20180930'
),
--Join the 2 tables and compute for # of days to uninstall
joined AS (
  SELECT a.*,
  b.date_app_remove,
  DATE_DIFF(DATE(b.date_app_remove), DATE(a.date_first_open), DAY) AS days_to_uninstall
  FROM sept_cohort a
  LEFT JOIN uninstallers b
  ON a.user_pseudo_id = b.user_pseudo_id
)
--Compute for the percentage
SELECT
COUNT(DISTINCT
CASE WHEN days_to_uninstall > 7 OR days_to_uninstall IS NULL THEN user_pseudo_id END) /
COUNT(DISTINCT user_pseudo_id)
AS percent_users_7_days
FROM joined
",,snowflake
387,ga030,firebase,"Can you group users by the week of their first session start, starting from July 2, 2018? For each group, calculate the retention rate in the fourth week (i.e., the percentage of users from the original group who returned in the fourth week after their first session). Please identify the cohort with the highest retention rate in the fourth week, and name the group by the Monday date of the cohort's first session week. Return the result in the format ""YYYY-MM-DD"".",,"How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
388,ga005,firebase,"Conduct a weekly cohort analysis for user retention, starting from July 9, 2018, and ending on October 2, 2018. Group users by the week of their first session_start event (with weeks starting on Monday), and identify new users as those where the event_date matches the date of their user_first_touch_timestamp. Calculate the Week 2 retention rate for each weekly cohort, defined as the percentage of users who had a session_start event exactly 2 weeks after their first week. Only include cohorts from July 9, 2018 through September 17, 2018 (the last cohort that can be analyzed for 2-week retention given the available data). Present the results with each weekly cohort and its corresponding Week 2 retention rate, ordered by cohort date.",,"How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
389,ga028,firebase,"Please perform a 7-day retention analysis for users who first session start the app during the week starting on July 2, 2018. For each week from Week 0 (the week of their first session) to Week 4, provide the total number of new users in Week 0 and the number of retained users for each subsequent week. Ensuring that you only count events up to October 2, 2018, and group dates by Monday-based weeks","WITH dates AS (
    SELECT 
        DATE('2018-07-02') AS start_date,
        DATE('2018-10-02') AS end_date,
        DATE_ADD(DATE_TRUNC(DATE('2018-10-02'), WEEK(TUESDAY)), INTERVAL -4 WEEK) AS min_date
),

date_table AS (
    SELECT DISTINCT 
        PARSE_DATE('%Y%m%d', `event_date`) AS event_date,
        user_pseudo_id,
        CASE 
            WHEN DATE_DIFF(PARSE_DATE('%Y%m%d', `event_date`), DATE(TIMESTAMP_MICROS(user_first_touch_timestamp)), DAY) = 0 
            THEN 1 
            ELSE 0 
        END AS is_new_user
    FROM 
        `firebase-public-project.analytics_153293282.events_*` 
    WHERE 
        event_name = 'session_start'
),

new_user_list AS (
    SELECT DISTINCT 
        user_pseudo_id,
        event_date
    FROM 
        date_table
    WHERE 
        is_new_user = 1
),

days_since_start_table AS (
    SELECT DISTINCT 
        is_new_user,
        nu.event_date AS date_cohort,
        dt.user_pseudo_id,
        dt.event_date,
        DATE_DIFF(dt.event_date, nu.event_date, DAY) AS days_since_start
    FROM 
        date_table dt
    JOIN 
        new_user_list nu ON dt.user_pseudo_id = nu.user_pseudo_id
),

weeks_retention AS (
    SELECT 
        date_cohort,
        DATE_TRUNC(date_cohort, WEEK(MONDAY)) AS week_cohort,
        user_pseudo_id,
        days_since_start,
        CASE 
            WHEN days_since_start = 0 
            THEN 0 
            ELSE CEIL(days_since_start / 7) 
        END AS weeks_since_start
    FROM 
        days_since_start_table
),
RETENTION_INFO AS (
  SELECT 
      week_cohort,
      weeks_since_start,
      COUNT(DISTINCT user_pseudo_id) AS retained_users
  FROM 
      weeks_retention
  WHERE 
      week_cohort <= (SELECT min_date FROM dates)
  GROUP BY 
      week_cohort,
      weeks_since_start
  HAVING 
      weeks_since_start <= 4
  ORDER BY 
      week_cohort,
      weeks_since_start
)

SELECT weeks_since_start, retained_users
FROM RETENTION_INFO
WHERE week_cohort = DATE('2018-07-02')

","How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
390,ga020,firebase,"Which quickplay event type had the lowest user retention rate during the second week after their initial engagement, for users who first engaged between August 1 and August 15, 2018, as measured by the presence of session_start events??","-- Define the date range and calculate the minimum date for filtering results
WITH dates AS (
    SELECT 
        DATE('2018-08-01') AS start_date,
        DATE('2018-08-15') AS end_date
),
-- Create a table of active dates for each user within the specified date range
dates_active_table AS (
    SELECT
        user_pseudo_id,
        PARSE_DATE('%Y%m%d', `event_date`) AS user_active_date
    FROM 
        `firebase-public-project.analytics_153293282.events_*` 
    WHERE 
        event_name = 'session_start'
        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)
    GROUP BY 
        user_pseudo_id, user_active_date
),
-- Create a table of the earliest quickplay event date for each user within the specified date range
event_table AS (
    SELECT 
        user_pseudo_id,
        event_name,
        MIN(PARSE_DATE('%Y%m%d', `event_date`)) AS event_cohort_date
    FROM 
        `firebase-public-project.analytics_153293282.events_*` 
    WHERE 
        event_name IN ('level_start_quickplay', 'level_end_quickplay', 'level_complete_quickplay', 
                       'level_fail_quickplay', 'level_reset_quickplay', 'level_retry_quickplay')
        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)
    GROUP BY 
        user_pseudo_id, event_name
),
-- Calculate the number of days since each user's initial quickplay event
days_since_event_table AS (
    SELECT
        events.user_pseudo_id,
        events.event_name AS event_cohort,
        events.event_cohort_date,
        days.user_active_date,
        DATE_DIFF(days.user_active_date, events.event_cohort_date, DAY) AS days_since_event
    FROM 
        event_table events
    LEFT JOIN 
        dates_active_table days ON events.user_pseudo_id = days.user_pseudo_id
    WHERE 
        events.event_cohort_date <= days.user_active_date
),
-- Calculate the weeks since each user's initial quickplay event and count the active days in each week
weeks_retention AS (
    SELECT
        event_cohort,
        user_pseudo_id,
        CAST(CASE WHEN days_since_event = 0 THEN 0 ELSE CEIL(days_since_event / 7) END AS INTEGER) AS weeks_since_event,
        COUNT(DISTINCT days_since_event) AS days_active_since_event -- Count Days Active in Week
    FROM 
        days_since_event_table
    GROUP BY 
        event_cohort, user_pseudo_id, weeks_since_event
),
-- Aggregate the weekly retention data
aggregated_weekly_retention_table AS (
    SELECT
        event_cohort,
        weeks_since_event,
        SUM(days_active_since_event) AS weekly_days_active,
        COUNT(DISTINCT user_pseudo_id) AS retained_users
    FROM 
        weeks_retention
    GROUP BY 
        event_cohort, weeks_since_event
),
RETENTION_INFO AS (
-- Select and calculate the weekly retention rate for each event cohort
SELECT
    event_cohort,
    weeks_since_event,
    weekly_days_active,
    retained_users,
    (retained_users / MAX(retained_users) OVER (PARTITION BY event_cohort)) AS retention_rate
FROM 
    aggregated_weekly_retention_table
ORDER BY 
    event_cohort, weeks_since_event
)

SELECT event_cohort
FROM
RETENTION_INFO
WHERE weeks_since_event = 2
ORDER BY retention_rate
LIMIT 1","How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
391,ga021,firebase,"What is the retention rate for users two weeks after their initial quickplay event, calculated separately for each quickplay event type, within the period from July 2, 2018, to July 16, 2018? Please focus on users who started a session (session_start) during this period.","-- Define the date range and calculate the minimum date for filtering results
WITH dates AS (
    SELECT 
        DATE('2018-07-02') AS start_date,
        DATE('2018-07-16') AS end_date
),
-- Create a table of active dates for each user within the specified date range
dates_active_table AS (
    SELECT
        user_pseudo_id,
        PARSE_DATE('%Y%m%d', `event_date`) AS user_active_date
    FROM 
        `firebase-public-project.analytics_153293282.events_*` 
    WHERE 
        event_name = 'session_start'
        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)
    GROUP BY 
        user_pseudo_id, user_active_date
),
-- Create a table of the earliest quickplay event date for each user within the specified date range
event_table AS (
    SELECT 
        user_pseudo_id,
        event_name,
        MIN(PARSE_DATE('%Y%m%d', `event_date`)) AS event_cohort_date
    FROM 
        `firebase-public-project.analytics_153293282.events_*` 
    WHERE 
        event_name IN ('level_start_quickplay', 'level_end_quickplay', 'level_complete_quickplay', 
                       'level_fail_quickplay', 'level_reset_quickplay', 'level_retry_quickplay')
        AND PARSE_DATE('%Y%m%d', `event_date`) BETWEEN (SELECT start_date FROM dates) AND (SELECT end_date FROM dates)
    GROUP BY 
        user_pseudo_id, event_name
),
-- Calculate the number of days since each user's initial quickplay event
days_since_event_table AS (
    SELECT
        events.user_pseudo_id,
        events.event_name AS event_cohort,
        events.event_cohort_date,
        days.user_active_date,
        DATE_DIFF(days.user_active_date, events.event_cohort_date, DAY) AS days_since_event
    FROM 
        event_table events
    LEFT JOIN 
        dates_active_table days ON events.user_pseudo_id = days.user_pseudo_id
    WHERE 
        events.event_cohort_date <= days.user_active_date
),
-- Calculate the weeks since each user's initial quickplay event and count the active days in each week
weeks_retention AS (
    SELECT
        event_cohort,
        user_pseudo_id,
        CAST(CASE WHEN days_since_event = 0 THEN 0 ELSE CEIL(days_since_event / 7) END AS INTEGER) AS weeks_since_event,
        COUNT(DISTINCT days_since_event) AS days_active_since_event -- Count Days Active in Week
    FROM 
        days_since_event_table
    GROUP BY 
        event_cohort, user_pseudo_id, weeks_since_event
),
-- Aggregate the weekly retention data
aggregated_weekly_retention_table AS (
    SELECT
        event_cohort,
        weeks_since_event,
        SUM(days_active_since_event) AS weekly_days_active,
        COUNT(DISTINCT user_pseudo_id) AS retained_users
    FROM 
        weeks_retention
    GROUP BY 
        event_cohort, weeks_since_event
),
RETENTION_INFO AS (
SELECT
    event_cohort,
    weeks_since_event,
    weekly_days_active,
    retained_users,
    (retained_users / MAX(retained_users) OVER (PARTITION BY event_cohort)) AS retention_rate
FROM 
    aggregated_weekly_retention_table
ORDER BY 
    event_cohort, weeks_since_event
)

SELECT event_cohort, retention_rate
FROM
RETENTION_INFO
WHERE weeks_since_event = 2","How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
392,ga022,firebase,"Could you please help me get the weekly customer retention rate in September 2018 for new customers who first used our app (first_open event) within the first week starting from September 1st, 2018 (timezone in Shanghai)? The retention rates should cover the following weeks 1, 2, and 3 period after the initial use and display them in column format.","WITH analytics_data AS (
  SELECT user_pseudo_id, event_timestamp, event_name, 
    UNIX_MICROS(TIMESTAMP(""2018-09-01 00:00:00"", ""+8:00"")) AS start_day,
    3600*1000*1000*24*7 AS one_week_micros
  FROM `firebase-public-project.analytics_153293282.events_*`
  WHERE _table_suffix BETWEEN '20180901' AND '20180930'
)

SELECT
 week_1_cohort / week_0_cohort AS week_1_pct,
 week_2_cohort / week_0_cohort AS week_2_pct,
 week_3_cohort / week_0_cohort AS week_3_pct
FROM (
  WITH week_3_users AS (
    SELECT DISTINCT user_pseudo_id
    FROM analytics_data
    WHERE event_timestamp BETWEEN start_day+(3*one_week_micros) AND start_day+(4*one_week_micros)
  ),
  week_2_users AS (
    SELECT DISTINCT user_pseudo_id
    FROM analytics_data
    WHERE event_timestamp BETWEEN start_day+(2*one_week_micros) AND start_day+(3*one_week_micros)
  ),
  week_1_users AS (
    SELECT DISTINCT user_pseudo_id
    FROM analytics_data
    WHERE event_timestamp BETWEEN start_day+(1*one_week_micros) AND start_day+(2*one_week_micros)
  ), 
  week_0_users AS (
    SELECT DISTINCT user_pseudo_id
    FROM analytics_data
    WHERE event_name = 'first_open'
      AND event_timestamp BETWEEN start_day AND start_day+(1*one_week_micros)
  )
  SELECT 
    (SELECT count(*) 
     FROM week_0_users) AS week_0_cohort,
    (SELECT count(*) 
     FROM week_1_users 
     JOIN week_0_users USING (user_pseudo_id)) AS week_1_cohort,
    (SELECT count(*) 
     FROM week_2_users 
     JOIN week_0_users USING (user_pseudo_id)) AS week_2_cohort,
    (SELECT count(*) 
     FROM week_3_users 
     JOIN week_0_users USING (user_pseudo_id)) AS week_3_cohort
)","How to Calculate User Retention in Big Query from Google Demo Game Analytics Data

As a Product / Website Analyst, I was pretty psyched to discover Google’s public Google Analytics 4 (GA4) gaming event dataset called `firebase-public-project.analytics_153293282.events_*`. The table allows experienced developers and learners alike to experiment with raw GA4 data in Big Query for free. Event data is basically a log of user interactions with your product for analysis .For newbies interested in learning more about event data and GA4, visit my prior blog. This article provides SQL queries for a quick table overview and explains how to utilize Google Big Query to calculate user retention.


What is Retention?
User retention rates are key indicators for whether your product team meets user needs. Improving retention is often central to long term growth strategies.

**Retention measure’s how often and for how long users tend to return to your product.**

The metric can be a powerful proxy for product usefulness and user opinion. Its analysis can reveal critical information like how well your site converts new visitors to users and which items and features are associated with users coming back. If user data is available, retention can also help a business understand the profiles of its most active users (power users). Finally, retention is a great guardrail metric for A/B tests. Successful, product oriented companies prioritize a healthy user bases over short term profits.


**Common Retention Calculation Strategies**

Two common strategies to calculate retention are: 

a) N-Day retention

b) Unbounded Retention.


a) For N-Day retention, analysts calculate how many users with certain characteristics return over successive periods. It is called N days because we define the length of each period. For example, with N=7 day retention, if User A becomes part of our cohort of interest on day 0 and returns to the product day 5 and day 15, they are counted as retained for week 1 (1–7 days) and week 3 (15–21 days) but not week 2 (8–14 days).


b) For Unbounded Retention, a user is counted as retained each week so long as their most recent product contact was after the week. For example, if User A becomes part of the cohort on day 0 and last returned to the site on day 21, they will be counted as retained in week 1 (1–7 days), week 2 (8–14 days), and week 3 (15–21 days) but not week 4 (22–28 days).

By calculating these metrics, companies can evaluate how well their products hold users’ interests. It also them closer to identifying the patterns that generate return users.


",snowflake
393,ga025,firebase,"For all users who first opened the app in September 2018 and then uninstalled within seven days, I want to know what percentage of them experienced an app crash (app_exception). The calculation should be done by converting the timestamps to dates first, and then calculating the days to uninstall based on the dates. Only users who uninstalled within 7 days and experienced a crash should be considered in the final percentage.",,,snowflake
394,local002,E_commerce,"Can you calculate the 5-day symmetric moving average of predicted toy sales for December 5 to 8, 2018, using daily sales data from January 1, 2017, to August 29, 2018, with a simple linear regression model? Finally provide the sum of those four 5-day moving averages?",,,snowflake
395,local003,E_commerce,"According to the RFM definition document, calculate the average sales per order for each customer within distinct RFM segments, considering only 'delivered' orders. Use the customer unique identifier. Clearly define how to calculate Recency based on the latest purchase timestamp and specify the criteria for classifying RFM segments. The average sales should be computed as the total spend divided by the total number of orders. Please analyze and report the differences in average sales across the RFM segments","WITH RecencyScore AS (
    SELECT customer_unique_id,
           MAX(order_purchase_timestamp) AS last_purchase,
           NTILE(5) OVER (ORDER BY MAX(order_purchase_timestamp) DESC) AS recency
    FROM orders
        JOIN customers USING (customer_id)
    WHERE order_status = 'delivered'
    GROUP BY customer_unique_id
),
FrequencyScore AS (
    SELECT customer_unique_id,
           COUNT(order_id) AS total_orders,
           NTILE(5) OVER (ORDER BY COUNT(order_id) DESC) AS frequency
    FROM orders
        JOIN customers USING (customer_id)
    WHERE order_status = 'delivered'
    GROUP BY customer_unique_id
),
MonetaryScore AS (
    SELECT customer_unique_id,
           SUM(price) AS total_spent,
           NTILE(5) OVER (ORDER BY SUM(price) DESC) AS monetary
    FROM orders
        JOIN order_items USING (order_id)
        JOIN customers USING (customer_id)
    WHERE order_status = 'delivered'
    GROUP BY customer_unique_id
),

-- 2. Assign each customer to a group
RFM AS (
    SELECT last_purchase, total_orders, total_spent,
        CASE
            WHEN recency = 1 AND frequency + monetary IN (1, 2, 3, 4) THEN ""Champions""
            WHEN recency IN (4, 5) AND frequency + monetary IN (1, 2) THEN ""Can't Lose Them""
            WHEN recency IN (4, 5) AND frequency + monetary IN (3, 4, 5, 6) THEN ""Hibernating""
            WHEN recency IN (4, 5) AND frequency + monetary IN (7, 8, 9, 10) THEN ""Lost""
            WHEN recency IN (2, 3) AND frequency + monetary IN (1, 2, 3, 4) THEN ""Loyal Customers""
            WHEN recency = 3 AND frequency + monetary IN (5, 6) THEN ""Needs Attention""
            WHEN recency = 1 AND frequency + monetary IN (7, 8) THEN ""Recent Users""
            WHEN recency = 1 AND frequency + monetary IN (5, 6) OR
                recency = 2 AND frequency + monetary IN (5, 6, 7, 8) THEN ""Potentital Loyalists""
            WHEN recency = 1 AND frequency + monetary IN (9, 10) THEN ""Price Sensitive""
            WHEN recency = 2 AND frequency + monetary IN (9, 10) THEN ""Promising""
            WHEN recency = 3 AND frequency + monetary IN (7, 8, 9, 10) THEN ""About to Sleep""
        END AS RFM_Bucket
    FROM RecencyScore
        JOIN FrequencyScore USING (customer_unique_id)
        JOIN MonetaryScore USING (customer_unique_id)
)

SELECT RFM_Bucket, 
       AVG(total_spent / total_orders) AS avg_sales_per_customer
FROM RFM
GROUP BY RFM_Bucket","# Introduction to the RFM Model

The RFM (Recency, Frequency, Monetary) model segments and scores customers based on three key dimensions:

• Recency (R): How long it has been since the customer’s last purchase. A lower R score (e.g., R = 1) indicates a very recent purchase, while a higher R score (e.g., R = 5) indicates a longer time since the last purchase.

• Frequency (F): How often the customer purchases within a given time period. A lower F score (e.g., F = 1) signifies that the customer buys very frequently, whereas a higher F score (e.g., F = 5) indicates less frequent purchasing.

• Monetary (M): The total amount of money the customer spends. A lower M score (e.g., M = 1) indicates higher overall spending, while a higher M score (e.g., M = 5) signifies lower spending over the measured period.

Each customer’s R, F, and M scores are determined by their respective percentiles when compared to other customers. By concatenating the three scores, you get an “RFM cell”—for instance, a customer with R=1, F=5, and M=2 would fall into the 152 segment.

# RFM Segmentation Calculation

After scoring customers on Recency, Frequency, and Monetary values, the next step is to group them into segments that require different marketing or sales strategies. Typically:

1. Determine each customer’s recency score (R) from 1 to 5 (1 = very recent purchase, 5 = not recent).  
2. Determine each customer’s frequency score (F) from 1 to 5 (1 = most frequent purchases, 5 = least frequent).  
3. Determine each customer’s monetary score (M) from 1 to 5 (1 = highest spending, 5 = lowest spending).  
4. Concatenate these three scores into an RFM score (e.g., 153, 514).

By analyzing the distribution of RFM scores and placing them into buckets—for example, “Champions,” “Loyal Customers,” “At Risk,” “Lost,” etc.—you can tailor marketing, sales, and retention strategies to maximize the potential of each segment. 

For instance, a “Champion” (R=1, F=1, M=1) is a recent, frequent, and high-spending user who is highly valuable to your business, whereas a “Lost” customer (e.g., R=5, F=5, M=5) may require re-engagement offers or might no longer be cost-effective to target. Different segments can thus be prioritized based on their profitability and likelihood of responding positively to marketing efforts.

## RFM Segmentation Logic

Customers are assigned to specific segments (RFM Buckets) based on a combination of their Recency, Frequency, and Monetary scores. The segmentation logic is as follows:

- **Champions**: Customers who have made a recent purchase, with high frequency and high monetary value. These are considered the most valuable customers.
  - Criteria: Recency = 1 and Frequency + Monetary score between 1 and 4.

- **Can't Lose Them**: Previously frequent and high-spending customers who have not made a recent purchase. These customers are at risk of leaving and need attention.
  - Criteria: Recency = 4 or 5 and Frequency + Monetary score between 1 and 2.

- **Hibernating**: Customers whose last purchase was a while ago, with low to moderate frequency and spending. These customers might have lost interest in the products.
  - Criteria: Recency = 4 or 5 and Frequency + Monetary score between 3 and 6.

- **Lost**: Customers who have not purchased in a long time and have low frequency and monetary value. These customers are likely lost.
  - Criteria: Recency = 4 or 5 and Frequency + Monetary score between 7 and 10.

- **Loyal Customers**: Customers who are frequent buyers with decent spending levels, and they have made a purchase relatively recently. These customers are likely to be very loyal.
  - Criteria: Recency = 2 or 3 and Frequency + Monetary score between 1 and 4.

- **Needs Attention**: Customers whose purchase frequency and spending are moderate. They haven't bought very recently, but they could be incentivized to become more active.
  - Criteria: Recency = 3 and Frequency + Monetary score between 5 and 6.

- **Recent Users**: Customers who made a purchase recently, but their frequency and spending are moderate. These are relatively new or inconsistent buyers.
  - Criteria: Recency = 1 and Frequency + Monetary score between 7 and 8.

- **Potential Loyalists**: Customers who show potential to become loyal customers. They have good frequency and monetary scores, and they have made recent purchases. With the right engagement, they could become loyal customers.
  - Criteria:
    - Recency = 1 and Frequency + Monetary score between 5 and 6.
    - OR Recency = 2 and Frequency + Monetary score between 5 and 8.

- **Price Sensitive**: Customers who have made recent purchases but tend to spend less, indicating they may be more sensitive to price.
  - Criteria: Recency = 1 and Frequency + Monetary score between 9 and 10.

- **Promising**: These customers exhibit high potential with decent frequency and monetary scores, and they could become more valuable over time.
  - Criteria: Recency = 2 and Frequency + Monetary score between 9 and 10.

- **About to Sleep**: Customers whose frequency and spending are low, and their last purchase was some time ago. These customers are likely to become inactive.
  - Criteria: Recency = 3 and Frequency + Monetary score between 7 and 10.

## Summary

This segmentation logic groups customers based on their behavior in terms of when they last purchased (Recency), how often they purchase (Frequency), and how much they spend (Monetary). By understanding which group a customer belongs to, organizations can tailor marketing strategies to engage the right audience more effectively, improving customer retention and maximizing value.
",snowflake
396,local004,E_commerce,"Could you tell me the number of orders, average payment per order and customer lifespan in weeks of the 3 custumers with the highest average payment per order, where the lifespan is calculated by subtracting the earliest purchase date from the latest purchase date in days, dividing by seven, and if the result is less than seven days, setting it to 1.0?","WITH CustomerData AS (
    SELECT
        customer_unique_id,
        COUNT(DISTINCT orders.order_id) AS order_count,
        SUM(payment_value) AS total_payment,
        JULIANDAY(MIN(order_purchase_timestamp)) AS first_order_day,
        JULIANDAY(MAX(order_purchase_timestamp)) AS last_order_day
    FROM customers
        JOIN orders USING (customer_id)
        JOIN order_payments USING (order_id)
    GROUP BY customer_unique_id
)
SELECT
    customer_unique_id,
    order_count AS PF,
    ROUND(total_payment / order_count, 2) AS AOV,
    CASE
        WHEN (last_order_day - first_order_day) < 7 THEN
            1
        ELSE
            (last_order_day - first_order_day) / 7
        END AS ACL
FROM CustomerData
ORDER BY AOV DESC
LIMIT 3
",,snowflake
397,local007,Baseball,"Could you help me calculate the average single career span value in years for all baseball players? Please precise the result as a float number. First, calculate the difference in years, months, and days between the debut and final game dates. For each player, the career span is computed as the sum of the absolute number of years, plus the absolute number of months divided by 12, plus the absolute number of days divided by 365. Round each part to two decimal places before summing. Finally, average the career spans and round the result to a float number.",,,snowflake
398,local008,Baseball,"I would like to know the given names of baseball players who have achieved the highest value of games played, runs, hits, and home runs, with their corresponding score values.","WITH player_stats AS (
    SELECT
        b.player_id,
        p.name_given AS player_name,
        SUM(b.g) AS games_played,
        SUM(b.r) AS runs,
        SUM(b.h) AS hits,
        SUM(b.hr) AS home_runs
    FROM player p
    JOIN batting b ON p.player_id = b.player_id
    GROUP BY b.player_id, p.name_given
)

SELECT 'Games Played' AS Category, player_name AS Player_Name, games_played AS Batting_Table_Topper
FROM player_stats
WHERE games_played = (SELECT MAX(games_played) FROM player_stats)

UNION ALL

SELECT 'Runs' AS Category, player_name AS Player_Name, runs AS Batting_Table_Topper
FROM player_stats
WHERE runs = (SELECT MAX(runs) FROM player_stats)

UNION ALL

SELECT 'Hits' AS Category, player_name AS Player_Name, hits AS Batting_Table_Topper
FROM player_stats
WHERE hits = (SELECT MAX(hits) FROM player_stats)

UNION ALL

SELECT 'Home Runs' AS Category, player_name AS Player_Name, home_runs AS Batting_Table_Topper
FROM player_stats
WHERE home_runs = (SELECT MAX(home_runs) FROM player_stats);
",,snowflake
399,local009,Airlines,What is the distance of the longest route where Abakan is either the departure or destination city (in kilometers)?,,"
# Flight Route Distance Calculation

## Introduction

This document describes the method used to calculate the distance between two cities for flight routes. The calculation is based on the Haversine formula, which is commonly used to find the shortest distance between two points on a sphere given their latitude and longitude. This method is especially useful for determining flight distances between airports located in different cities around the world.

## City and Coordinate Extraction

For each flight, the following data is obtained:

- **Departure city** (referred to as `from_city`) and its geographical coordinates (longitude and latitude).
- **Arrival city** (referred to as `to_city`) and its geographical coordinates (longitude and latitude).

The coordinates are extracted as decimal values, with longitude and latitude represented in degrees. This ensures that trigonometric operations can be applied during the distance calculation.

## Haversine Formula

The Haversine formula is used to calculate the great-circle distance between two points on a sphere using their latitude and longitude. The formula is given as:

\[
d = 2r \cdot \arcsin\left(\sqrt{\sin^2\left(\frac{\Delta \phi}{2}\right) + \cos(\phi_1) \cdot \cos(\phi_2) \cdot \sin^2\left(\frac{\Delta \lambda}{2}\right)}\right)
\]

Where:

- \( d \) is the distance between the two points (in kilometers).
- \( r \) is the radius of the Earth (approximately 6371 km).
- \( \phi_1 \) and \( \phi_2 \) are the latitudes of the departure and arrival points, respectively, in radians.
- \( \Delta \phi = \phi_2 - \phi_1 \) is the difference in latitudes.
- \( \lambda_1 \) and \( \lambda_2 \) are the longitudes of the departure and arrival points, respectively, in radians.
- \( \Delta \lambda = \lambda_2 - \lambda_1 \) is the difference in longitudes.

### Conversion to Radians

Since the input coordinates are in degrees, they must be converted to radians before applying the Haversine formula. This conversion is done using the formula:

\[
\text{radians} = \text{degrees} \times \frac{\pi}{180}
\]

## Symmetry of Routes

To identify unique flight routes between two cities, we standardize the order of cities in each route. Specifically, we ensure that the lexicographically smaller city name is always listed as the first city (`city1`), and the larger city is listed as the second city (`city2`). This ensures that a flight from City A to City B is treated the same as a flight from City B to City A.

## Average Route Distance

Once the distances for all flights between two cities are computed, the average distance for each city pair is calculated by summing the distances and dividing by the total number of flights between those cities:

\[
\text{Average Distance} = \frac{\sum \text{Flight Distances}}{\text{Number of Flights}}
\]

## Conclusion

This method of flight route distance calculation provides a reliable way to determine the great-circle distance between cities based on the coordinates of their respective airports. The use of the Haversine formula ensures accurate results for distances on the Earth's surface, making it ideal for aviation and travel analysis.
",snowflake
400,local010,Airlines,"Distribute all the unique city pairs into the distance ranges 0, 1000, 2000, 3000, 4000, 5000, and 6000+, based on their average distance of all routes between them. Then how many pairs are there in the distance range with the fewest unique city paires?",,"
# Flight Route Distance Calculation

## Introduction

This document describes the method used to calculate the distance between two cities for flight routes. The calculation is based on the Haversine formula, which is commonly used to find the shortest distance between two points on a sphere given their latitude and longitude. This method is especially useful for determining flight distances between airports located in different cities around the world.

## City and Coordinate Extraction

For each flight, the following data is obtained:

- **Departure city** (referred to as `from_city`) and its geographical coordinates (longitude and latitude).
- **Arrival city** (referred to as `to_city`) and its geographical coordinates (longitude and latitude).

The coordinates are extracted as decimal values, with longitude and latitude represented in degrees. This ensures that trigonometric operations can be applied during the distance calculation.

## Haversine Formula

The Haversine formula is used to calculate the great-circle distance between two points on a sphere using their latitude and longitude. The formula is given as:

\[
d = 2r \cdot \arcsin\left(\sqrt{\sin^2\left(\frac{\Delta \phi}{2}\right) + \cos(\phi_1) \cdot \cos(\phi_2) \cdot \sin^2\left(\frac{\Delta \lambda}{2}\right)}\right)
\]

Where:

- \( d \) is the distance between the two points (in kilometers).
- \( r \) is the radius of the Earth (approximately 6371 km).
- \( \phi_1 \) and \( \phi_2 \) are the latitudes of the departure and arrival points, respectively, in radians.
- \( \Delta \phi = \phi_2 - \phi_1 \) is the difference in latitudes.
- \( \lambda_1 \) and \( \lambda_2 \) are the longitudes of the departure and arrival points, respectively, in radians.
- \( \Delta \lambda = \lambda_2 - \lambda_1 \) is the difference in longitudes.

### Conversion to Radians

Since the input coordinates are in degrees, they must be converted to radians before applying the Haversine formula. This conversion is done using the formula:

\[
\text{radians} = \text{degrees} \times \frac{\pi}{180}
\]

## Symmetry of Routes

To identify unique flight routes between two cities, we standardize the order of cities in each route. Specifically, we ensure that the lexicographically smaller city name is always listed as the first city (`city1`), and the larger city is listed as the second city (`city2`). This ensures that a flight from City A to City B is treated the same as a flight from City B to City A.

## Average Route Distance

Once the distances for all flights between two cities are computed, the average distance for each city pair is calculated by summing the distances and dividing by the total number of flights between those cities:

\[
\text{Average Distance} = \frac{\sum \text{Flight Distances}}{\text{Number of Flights}}
\]

## Conclusion

This method of flight route distance calculation provides a reliable way to determine the great-circle distance between cities based on the coordinates of their respective airports. The use of the Haversine formula ensures accurate results for distances on the Earth's surface, making it ideal for aviation and travel analysis.
",snowflake
401,local015,California_Traffic_Collision,"Please calculate the fatality rate for motorcycle collisions, separated by helmet usage. Specifically, calculate two percentages: 1) the percentage of motorcyclist fatalities in collisions where parties (drivers or passengers) were wearing helmets, and 2) the percentage of motorcyclist fatalities in collisions where parties were not wearing helmets. For each group, compute this by dividing the total number of motorcyclist fatalities by the total number of collisions involving that group. Use the parties table to determine helmet usage (from party_safety_equipment fields).",,,snowflake
402,local017,California_Traffic_Collision,In which year were the two most common causes of traffic accidents different from those in other years?,"WITH AnnualTotals AS (
    SELECT 
        STRFTIME('%Y', collision_date) AS Year, 
        COUNT(case_id) AS AnnualTotal
    FROM 
        collisions
    GROUP BY 
        Year
),
CategoryTotals AS (
    SELECT 
        STRFTIME('%Y', collision_date) AS Year,
        pcf_violation_category AS Category,
        COUNT(case_id) AS Subtotal
    FROM 
        collisions
    GROUP BY 
        Year, Category
),
CategoryPercentages AS (
    SELECT 
        ct.Year,
        ct.Category,
        ROUND((ct.Subtotal * 100.0) / at.AnnualTotal, 1) AS PercentageOfAnnualRoadIncidents
    FROM 
        CategoryTotals ct
    JOIN 
        AnnualTotals at ON ct.Year = at.Year
),
RankedCategories AS (
    SELECT
        Year,
        Category,
        PercentageOfAnnualRoadIncidents,
        ROW_NUMBER() OVER (PARTITION BY Year ORDER BY PercentageOfAnnualRoadIncidents DESC) AS Rank
    FROM
        CategoryPercentages
),
TopTwoCategories AS (
    SELECT
        Year,
        GROUP_CONCAT(Category, ', ') AS TopCategories
    FROM
        RankedCategories
    WHERE
        Rank <= 2
    GROUP BY
        Year
),
UniqueYear AS (
    SELECT
        Year
    FROM
        TopTwoCategories
    GROUP BY
        TopCategories
    HAVING COUNT(Year) = 1
),
results AS (
SELECT 
    rc.Year, 
    rc.Category, 
    rc.PercentageOfAnnualRoadIncidents
FROM 
    UniqueYear u
JOIN 
    RankedCategories rc ON u.Year = rc.Year
WHERE 
    rc.Rank <= 2
)

SELECT distinct Year FROM results",,snowflake
403,local018,California_Traffic_Collision,"For the primary collision factor violation category that was the most common cause of traffic accidents in 2021, how many percentage points did its share of annual road incidents in 2021 decrease compared to its share in 2011?",,,snowflake
404,local019,WWE,"For the NXT title that had the shortest match (excluding titles with ""title change""), what were the names of the two wrestlers involved?","WITH MatchDetails AS (
    SELECT
        b.name AS titles,
        m.duration AS match_duration,
        w1.name || ' vs ' || w2.name AS matches,
        m.win_type AS win_type,
        l.name AS location,
        e.name AS event,
        ROW_NUMBER() OVER (PARTITION BY b.name ORDER BY m.duration ASC) AS rank
    FROM 
        Belts b
    INNER JOIN Matches m ON m.title_id = b.id
    INNER JOIN Wrestlers w1 ON w1.id = m.winner_id
    INNER JOIN Wrestlers w2 ON w2.id = m.loser_id
    INNER JOIN Cards c ON c.id = m.card_id
    INNER JOIN Locations l ON l.id = c.location_id
    INNER JOIN Events e ON e.id = c.event_id
    INNER JOIN Promotions p ON p.id = c.promotion_id
    WHERE
        p.name = 'NXT'
        AND m.duration <> ''
        AND b.name <> ''
        AND b.name NOT IN (
            SELECT name 
            FROM Belts 
            WHERE name LIKE '%title change%'
        )
),
Rank1 AS (
SELECT 
    titles,
    match_duration,
    matches,
    win_type,
    location,
    event
FROM 
    MatchDetails
WHERE 
    rank = 1
)
SELECT
    SUBSTR(matches, 1, INSTR(matches, ' vs ') - 1) AS wrestler1,
    SUBSTR(matches, INSTR(matches, ' vs ') + 4) AS wrestler2
FROM
Rank1
ORDER BY match_duration 
LIMIT 1",,snowflake
405,local026,IPL,"Please help me identify the top 3 bowlers who, in the overs where the maximum runs were conceded in each match, gave up the highest number of runs in a single over across all matches. For each of these bowlers, provide the match in which they conceded these maximum runs. Only consider overs that had the most runs conceded within their respective matches, and among these, determine which bowlers conceded the most runs in a single over overall.",,,snowflake
406,local020,IPL,Which bowler has the lowest bowling average per wicket taken?,,,snowflake
407,local021,IPL,Could you calculate the average of the total runs scored by all strikers who have scored more than 50 runs in any single match?,,,snowflake
408,local022,IPL,Retrieve the names of players who scored no less than 100 runs in a match while playing for the team that lost that match.,"-- Step 1: Calculate players' total runs in each match
WITH player_runs AS (
    SELECT 
        bbb.striker AS player_id, 
        bbb.match_id, 
        SUM(bsc.runs_scored) AS total_runs 
    FROM 
        ball_by_ball AS bbb
    JOIN 
        batsman_scored AS bsc
    ON 
        bbb.match_id = bsc.match_id 
        AND bbb.over_id = bsc.over_id 
        AND bbb.ball_id = bsc.ball_id 
        AND bbb.innings_no = bsc.innings_no
    GROUP BY 
        bbb.striker, bbb.match_id
    HAVING 
        SUM(bsc.runs_scored) >= 100
),

-- Step 2: Identify losing teams for each match
losing_teams AS (
    SELECT 
        match_id, 
        CASE 
            WHEN match_winner = team_1 THEN team_2 
            ELSE team_1 
        END AS loser 
    FROM 
        match
),

-- Step 3: Combine the above results to get players who scored 100 or more runs in losing teams
players_in_losing_teams AS (
    SELECT 
        pr.player_id, 
        pr.match_id 
    FROM 
        player_runs AS pr
    JOIN 
        losing_teams AS lt
    ON 
        pr.match_id = lt.match_id
    JOIN 
        player_match AS pm
    ON 
        pr.player_id = pm.player_id 
        AND pr.match_id = pm.match_id 
        AND lt.loser = pm.team_id
)

-- Step 4: Select distinct player names from the player table
SELECT DISTINCT 
    p.player_name 
FROM 
    player AS p
JOIN 
    players_in_losing_teams AS plt
ON 
    p.player_id = plt.player_id
ORDER BY 
    p.player_name;",,snowflake
409,local023,IPL,"Please help me find the names of top 5 players with the highest average runs per match in season 5, along with their batting averages.","WITH runs_scored AS (
    SELECT 
        bb.striker AS player_id,
        bb.match_id,
        bs.runs_scored AS runs
    FROM 
        ball_by_ball AS bb
    JOIN 
        batsman_scored AS bs ON bb.match_id = bs.match_id 
            AND bb.over_id = bs.over_id 
            AND bb.ball_id = bs.ball_id 
            AND bb.innings_no = bs.innings_no
    WHERE 
        bb.match_id IN (SELECT match_id FROM match WHERE season_id = 5)
),
total_runs AS (
    SELECT 
        player_id, 
        match_id, 
        SUM(runs) AS total_runs 
    FROM 
        runs_scored 
    GROUP BY 
        player_id, match_id
),
batting_averages AS (
    SELECT 
        player_id, 
        SUM(total_runs) AS runs, 
        COUNT(match_id) AS num_matches,
        ROUND(SUM(total_runs) / CAST(COUNT(match_id) AS FLOAT), 3) AS batting_avg
    FROM 
        total_runs 
    GROUP BY 
        player_id 
    ORDER BY 
        batting_avg DESC 
    LIMIT 5
)
SELECT 
    p.player_name,
    b.batting_avg
FROM 
    player AS p
JOIN 
    batting_averages AS b ON p.player_id = b.player_id
ORDER BY 
    b.batting_avg DESC;",,snowflake
410,local024,IPL,"Can you help me find the top 5 countries whose players have the highest average of their individual average runs per match across all seasons? Specifically, for each player, calculate their average runs per match over all matches they played, then compute the average of these player averages for each country, and include these country batting averages in the result.",,,snowflake
411,local025,IPL,"For each match, considering every innings, please combine runs from both batsman scored and extra runs for each over, then identify the single over with the highest total runs, retrieve the bowler for that over from the ball by ball table, and calculate the average of these highest over totals across all matches, ensuring that all runs and bowler details are accurately reflected.",,,snowflake
412,local028,Brazilian_E_Commerce,"Could you generate a report that shows the number of delivered orders for each month in the years 2016, 2017, and 2018? Each column represents a year, and each row represents a month",,,snowflake
413,local031,Brazilian_E_Commerce,"What is the highest monthly delivered orders volume in the year with the lowest annual delivered orders volume among 2016, 2017, and 2018?",,,snowflake
414,local029,Brazilian_E_Commerce,"Please identify the top three customers, based on their customer_unique_id, who have the highest number of delivered orders, and provide the average payment value, city, and state for each of these customers.","WITH customer_orders AS (
    SELECT
        c.customer_unique_id,
        COUNT(o.order_id) AS Total_Orders_By_Customers,
        AVG(p.payment_value) AS Average_Payment_By_Customer,
        c.customer_city,
        c.customer_state
    FROM olist_customers c
    JOIN olist_orders o ON c.customer_id = o.customer_id
    JOIN olist_order_payments p ON o.order_id = p.order_id
    WHERE o.order_status = 'delivered'
    GROUP BY c.customer_unique_id, c.customer_city, c.customer_state
)

SELECT 
    Average_Payment_By_Customer,
    customer_city,
    customer_state
FROM customer_orders
ORDER BY Total_Orders_By_Customers DESC
LIMIT 3;",,snowflake
415,local030,Brazilian_E_Commerce,"Among all cities with delivered orders, find the five cities whose summed payments are the lowest, then calculate the average of their total payments and the average of their total delivered order counts.",,,snowflake
416,local032,Brazilian_E_Commerce,"Could you help me find the sellers who excel in the following categories, considering only delivered orders: the seller with the highest number of distinct customer unique IDs, the seller with the highest profit (calculated as price minus freight value), the seller with the highest number of distinct orders, and the seller with the most 5-star ratings? For each category, please provide the seller ID and the corresponding value, labeling each row with a description of the achievement.",,,snowflake
417,local034,Brazilian_E_Commerce,"Could you help me calculate the average of the total number of payments made using the most preferred payment method for each product category, where the most preferred payment method in a category is the one with the highest number of payments?",,,snowflake
418,local037,Brazilian_E_Commerce,"Identify the top three product categories whose most commonly used payment type has the highest number of payments across all categories, and specify the number of payments made in each category using that payment type.",,,snowflake
419,local035,Brazilian_E_Commerce,"In the “olist_geolocation” table, please identify which two consecutive cities, when sorted by geolocation_state, geolocation_city, geolocation_zip_code_prefix, geolocation_lat, and geolocation_lng, have the greatest distance between them based on the difference in distance computed between each city and its immediate predecessor in that ordering.",,"The distance between two cities can be calculated using the **Spherical Law of Cosines**. This method estimates the distance based on the geographical coordinates (latitude and longitude) of the cities. Below is a detailed explanation of the calculation process, including the relevant formula.

The distance $d$ between two cities is calculated using the following formula:

$$
d = 6371 \times \arccos \left( \cos(\text{lat}_1) \times \cos(\text{lat}_2) \times \cos(\text{lon}_2 - \text{lon}_1) + \sin(\text{lat}_1) \times \sin(\text{lat}_2) \right)
$$

Where:
- $\text{lat}_1$ and $\text{lat}_2$ are the latitudes of the first and second cities in **radians**.
- $\text{lon}_1$ and $\text{lon}_2$ are the longitudes of the first and second cities in **radians**.
- `6371` is the Earth's average radius in kilometers.
",snowflake
420,local038,Pagila,"Could you help me determine which actor starred most frequently in English-language children's category films that were rated either G or PG, had a running time of 120 minutes or less, and were released between 2000 and 2010? Please provide the actor's full name.","SELECT
    actor.first_name || ' ' || actor.last_name AS full_name
FROM
    actor
INNER JOIN film_actor ON actor.actor_id = film_actor.actor_id
INNER JOIN film ON film_actor.film_id = film.film_id
INNER JOIN film_category ON film.film_id = film_category.film_id
INNER JOIN category ON film_category.category_id = category.category_id
-- Join with the language table
INNER JOIN language ON film.language_id = language.language_id
WHERE
    category.name = 'Children' AND
    film.release_year BETWEEN 2000 AND 2010 AND
    film.rating IN ('G', 'PG') AND
    language.name = 'English' AND
    film.length <= 120
GROUP BY
    actor.actor_id, actor.first_name, actor.last_name
ORDER BY
    COUNT(film.film_id) DESC
LIMIT 1;
",,snowflake
421,local039,Pagila,"Please help me find the film category with the highest total rental hours in cities where the city's name either starts with ""A"" or contains a hyphen. ","SELECT
    category.name
FROM
    category
INNER JOIN film_category USING (category_id)
INNER JOIN film USING (film_id)
INNER JOIN inventory USING (film_id)
INNER JOIN rental USING (inventory_id)
INNER JOIN customer USING (customer_id)
INNER JOIN address USING (address_id)
INNER JOIN city USING (city_id)
WHERE
    LOWER(city.city) LIKE 'a%' OR city.city LIKE '%-%'
GROUP BY
    category.name
ORDER BY
    SUM(CAST((julianday(rental.return_date) - julianday(rental.rental_date)) * 24 AS INTEGER)) DESC
LIMIT
    1;",,snowflake
422,local040,modern_data,"In the combined dataset that unifies the trees data with the income data by ZIP code, filling missing ZIP values where necessary, which three boroughs, restricted to records with median and mean income both greater than zero and a valid borough name, contain the highest number of trees, and what is the average mean income for each of these three boroughs?",,,snowflake
423,local041,modern_data,What percentage of trees in the Bronx have a health status of Good?,,,snowflake
424,local049,modern_data,Can you help me calculate the average number of new unicorn companies per year in the top industry from 2019 to 2021?,,,snowflake
425,local054,chinook,"Could you tell me the first names of customers who spent less than $1 on albums by the best-selling artist, along with the amounts they spent?",,,snowflake
426,local055,chinook,"Identify the artist with the highest overall sales of albums (tie broken by alphabetical order) and the artist with the lowest overall sales of albums (tie broken by alphabetical order), then calculate the amount each customer spent specifically on those two artists’ albums. Next, compute the average spending for the customers who purchased from the top-selling artist and the average spending for the customers who purchased from the lowest-selling artist, and finally return the absolute difference between these two averages.",,,snowflake
427,local198,chinook,"Using the sales data, what is the median value of total sales made in countries where the number of customers is greater than 4?",,,snowflake
428,local056,sqlite-sakila,Which customer has the highest average monthly change in payment amounts? Provide the customer's full name.,,,snowflake
429,local058,education_business,"Can you provide a list of hardware product segments along with their unique product counts for 2020 in the output, ordered by the highest percentage increase in unique fact sales products from 2020 to 2021?","WITH UniqueProducts2020 AS (
    SELECT
        dp.segment,
        COUNT(DISTINCT fsm.product_code) AS unique_products_2020
    FROM
        hardware_fact_sales_monthly fsm
    JOIN
        hardware_dim_product dp ON fsm.product_code = dp.product_code
    WHERE
        fsm.fiscal_year = 2020
    GROUP BY
        dp.segment
),
UniqueProducts2021 AS (
    SELECT
        dp.segment,
        COUNT(DISTINCT fsm.product_code) AS unique_products_2021
    FROM
        hardware_fact_sales_monthly fsm
    JOIN
        hardware_dim_product dp ON fsm.product_code = dp.product_code
    WHERE
        fsm.fiscal_year = 2021
    GROUP BY
        dp.segment
)
SELECT
    spc.segment,
    spc.unique_products_2020 AS product_count_2020
FROM
    UniqueProducts2020 spc
JOIN
    UniqueProducts2021 fup ON spc.segment = fup.segment
ORDER BY
    ((fup.unique_products_2021 - spc.unique_products_2020) * 100.0) / (spc.unique_products_2020) DESC;
",,snowflake
430,local059,education_business,"For the calendar year 2021, what is the overall average quantity sold of the top three best-selling hardware products (by total quantity sold) in each division?",,,snowflake
431,local060,complex_oracle,"In the United States, for Q4 2019 and Q4 2020, first select only those cities where total sales (with no promotions) rose by at least 20% from Q4 2019 to Q4 2020. Among these cities, rank products by their overall sales (still excluding promotions) in those quarters and take the top 20%. Then compute each top product’s share of total sales in Q4 2019 and Q4 2020 and calculate the difference in share from Q4 2019 to Q4 2020, returning the results in descending order of that share change.",,,snowflake
432,local063,complex_oracle,"Among all products sold in the United States with promo_id=999, considering only those cities whose sales increased by at least 20% from Q4 2019 (calendar_quarter_id=1772) to Q4 2020 (calendar_quarter_id=1776), which product that ranks in the top 20% of total sales has the smallest percentage-point change in its share of total sales between these two quarters?",,,snowflake
433,local061,complex_oracle,"What is the average projected monthly sales in USD for France in 2021, considering only product sales with promotions where promo_total_id = 1 and channels where channel_total_id = 1, by taking each product’s monthly sales from 2019 and 2020, calculating the growth rate from 2019 to 2020 for that same product and month, applying this growth rate to project 2021 monthly sales, converting all projected 2021 amounts to USD with the 2021 exchange rates, and finally averaging and listing them by month?",,"## Projection Calculation Method

### Steps for Projection Calculation

1. **Aggregate Historical Sales Data**

   - **Data Collection**: Gather sales data for products sold in France, including sales amounts each month for the years 2019, 2020, and 2021.
   - **Summarize Sales**: Sum up the sales amounts for each product, country, month, and year.

2. **Calculate Average Sales**

   - **Monthly Averages**: Compute the average sales amount for each product and month across all available months to establish a baseline of typical sales.

3. **Project Sales for 2021**

   - **Identify Changes**: Determine how sales changed from 2019 to 2020 for each product and month. Calculate the percentage change in sales from 2019 to 2020.
   - **Apply Changes**: Use this percentage change to estimate the sales for each month in 2021.

   **Projection Formula**:
   - For 2021:
     - Calculate the difference in sales between 2020 and 2019.
     - Compute the percentage change relative to 2019 sales.
     - Apply this percentage change to the 2020 sales to estimate 2021 sales.
     - The formula used in the SQL query is:

       ```plaintext
       (((Sales in 2020 - Sales in 2019) / Sales in 2019) * Sales in 2020) + Sales in 2020
       ```

     - This formula calculates the projected sales for 2021 based on the observed trend from 2019 to 2020.

   - For other years (not 2021):
     - Use the average sales value calculated for each month.

4. **Adjust for Currency Conversion**

   - **Conversion Rates**: Convert the projected sales figures into USD using monthly conversion rates.
   - **Currency Adjustment**: Multiply the projected sales figures by the conversion rates to adjust to USD. If specific rates are not available, use a default rate of 1.

5. **Calculate Monthly Averages in USD**

   - **Monthly Projections**: Compute the average projected sales for each month in 2021, adjusting for currency conversion. Round the results to two decimal places.

6. **Compile Results**

   - **Organize Data**: Arrange the projected sales figures in a report, showing the estimated sales for each month in USD.

### Summary

The projection calculation involves analyzing historical sales data from 2019 and 2020 to determine trends, applying these trends to estimate sales for 2021, and adjusting for currency differences. The result is a forecast of monthly sales in USD for 2021.
",snowflake
434,local050,complex_oracle,"What is the median of the average monthly projected sales in USD for France in 2021, calculated by using the monthly sales data from 2019 and 2020 (filtered by promo_total_id=1 and channel_total_id=1), applying the growth rate from 2019 to 2020 to project 2021, converting to USD based on the currency table, and then determining the monthly averages before finding their median?",,"## Projection Calculation Method

### Steps for Projection Calculation

1. **Aggregate Historical Sales Data**

   - **Data Collection**: Gather sales data for products sold in France, including sales amounts each month for the years 2019, 2020, and 2021.
   - **Summarize Sales**: Sum up the sales amounts for each product, country, month, and year.

2. **Calculate Average Sales**

   - **Monthly Averages**: Compute the average sales amount for each product and month across all available months to establish a baseline of typical sales.

3. **Project Sales for 2021**

   - **Identify Changes**: Determine how sales changed from 2019 to 2020 for each product and month. Calculate the percentage change in sales from 2019 to 2020.
   - **Apply Changes**: Use this percentage change to estimate the sales for each month in 2021.

   **Projection Formula**:
   - For 2021:
     - Calculate the difference in sales between 2020 and 2019.
     - Compute the percentage change relative to 2019 sales.
     - Apply this percentage change to the 2020 sales to estimate 2021 sales.
     - The formula used in the SQL query is:

       ```plaintext
       (((Sales in 2020 - Sales in 2019) / Sales in 2019) * Sales in 2020) + Sales in 2020
       ```

     - This formula calculates the projected sales for 2021 based on the observed trend from 2019 to 2020.

   - For other years (not 2021):
     - Use the average sales value calculated for each month.

4. **Adjust for Currency Conversion**

   - **Conversion Rates**: Convert the projected sales figures into USD using monthly conversion rates.
   - **Currency Adjustment**: Multiply the projected sales figures by the conversion rates to adjust to USD. If specific rates are not available, use a default rate of 1.

5. **Calculate Monthly Averages in USD**

   - **Monthly Projections**: Compute the average projected sales for each month in 2021, adjusting for currency conversion. Round the results to two decimal places.

6. **Compile Results**

   - **Organize Data**: Arrange the projected sales figures in a report, showing the estimated sales for each month in USD.

### Summary

The projection calculation involves analyzing historical sales data from 2019 and 2020 to determine trends, applying these trends to estimate sales for 2021, and adjusting for currency differences. The result is a forecast of monthly sales in USD for 2021.
",snowflake
435,local062,complex_oracle,"Please group all Italian customers into ten buckets for December 2021 by summing their profits from all products purchased (where profit is calculated as quantity_sold multiplied by the difference between unit_price and unit_cost), then divide the overall range of total monthly profits into ten equal intervals. For each bucket, provide the number of customers, and identify the minimum and maximum total profits within that bucket.",,,snowflake
436,local067,complex_oracle,Can you provide the highest and lowest profits for Italian customers segmented into ten evenly divided tiers based on their December 2021 sales profits?,,,snowflake
437,local070,city_legislation,"Please examine our database records for Chinese cities (country_code_2 = 'cn') during July 2021 and identify both the shortest and longest streaks of consecutive date entries. For each date in these streaks, return exactly one record per date along with the corresponding city name. In your output, please ensure the first letter of each city name is capitalized and the rest are lowercase. Display the dates and city names for both the shortest and longest consecutive date streaks, ordered by date.",,,snowflake
438,local071,city_legislation,Could you review our records in June 2022 and identify which countries have the longest streak of consecutive inserted city dates? Please list the 2-letter length country codes of these countries.,,,snowflake
439,local072,city_legislation,"Identify the country with data inserted on nine different days in January 2022. Then, find the longest consecutive period with data insertions for this country during January 2022, and calculate the proportion of entries that are from its capital city within this longest consecutive insertion period.",,,snowflake
440,local068,city_legislation,"Calculate the number of new cities inserted in April, May, and June for each year from 2021 to 2023. For each month, compute the cumulative running total of cities added for that specific month across the years up to and including the given year (i.e., sum the counts of that month over the years). Additionally, calculate the year-over-year growth percentages for both the monthly total and the running total for each month, comparing each year to the previous year. Present the results only for 2022 and 2023, listing the year, the month, the total number of cities added in that month, the cumulative running total for that month, and the year-over-year growth percentages for both the monthly total and the running total. Use the data from 2021 solely as a baseline for calculating growth rates, and exclude it from the final output.",,,snowflake
441,local073,modern_data,"For each pizza order, provide a single result row with the row ID, order ID, customer ID, pizza name, and final set of ingredients. The final ingredients are determined by starting with the standard toppings from the pizza’s recipe, removing any excluded toppings, and adding any extra toppings. Present the ingredients in a string starting with the pizza name followed by ': ', with ingredients listed in alphabetical order. Ingredients appearing multiple times (e.g., from standard and extra toppings) should be prefixed with '2x' and listed first, followed by single-occurrence ingredients, both in alphabetical order. Group by row ID, order ID, pizza name, and order time to ensure each order appears once. Sort results by row ID in ascending order. Assign pizza_id 1 to 'Meatlovers' pizzas and pizza_id 2 to all others.",,,snowflake
442,local066,modern_data,"Based on our customer pizza order information, summarize the total quantity of each ingredient used in the pizzas we delivered. Output the name and quantity for each ingredient.","WITH cte_cleaned_customer_orders AS (
    SELECT
        *,
        ROW_NUMBER() OVER () AS original_row_number
    FROM 
        pizza_clean_customer_orders
),
split_regular_toppings AS (
    SELECT
        pizza_id,
        TRIM(SUBSTR(toppings, 1, INSTR(toppings || ',', ',') - 1)) AS topping_id,
        SUBSTR(toppings || ',', INSTR(toppings || ',', ',') + 1) AS remaining_toppings
    FROM 
        pizza_recipes
    UNION ALL
    SELECT
        pizza_id,
        TRIM(SUBSTR(remaining_toppings, 1, INSTR(remaining_toppings, ',') - 1)) AS topping_id,
        SUBSTR(remaining_toppings, INSTR(remaining_toppings, ',') + 1) AS remaining_toppings
    FROM 
        split_regular_toppings
    WHERE
        remaining_toppings <> ''
),
cte_base_toppings AS (
    SELECT
        t1.order_id,
        t1.customer_id,
        t1.pizza_id,
        t1.order_time,
        t1.original_row_number,
        t2.topping_id
    FROM 
        cte_cleaned_customer_orders AS t1
    LEFT JOIN 
        split_regular_toppings AS t2
    ON 
        t1.pizza_id = t2.pizza_id
),
split_exclusions AS (
    SELECT
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        TRIM(SUBSTR(exclusions, 1, INSTR(exclusions || ',', ',') - 1)) AS topping_id,
        SUBSTR(exclusions || ',', INSTR(exclusions || ',', ',') + 1) AS remaining_exclusions
    FROM 
        cte_cleaned_customer_orders
    WHERE 
        exclusions IS NOT NULL
    UNION ALL
    SELECT
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        TRIM(SUBSTR(remaining_exclusions, 1, INSTR(remaining_exclusions, ',') - 1)) AS topping_id,
        SUBSTR(remaining_exclusions, INSTR(remaining_exclusions, ',') + 1) AS remaining_exclusions
    FROM 
        split_exclusions
    WHERE
        remaining_exclusions <> ''
),
split_extras AS (
    SELECT
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS topping_id,
        SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras
    FROM 
        cte_cleaned_customer_orders
    WHERE 
        extras IS NOT NULL
    UNION ALL
    SELECT
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS topping_id,
        SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1) AS remaining_extras
    FROM 
        split_extras
    WHERE
        remaining_extras <> ''
),
cte_combined_orders AS (
    SELECT 
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        topping_id
    FROM 
        cte_base_toppings
    WHERE topping_id NOT IN (SELECT topping_id FROM split_exclusions WHERE split_exclusions.order_id = cte_base_toppings.order_id)
    UNION ALL
    SELECT 
        order_id,
        customer_id,
        pizza_id,
        order_time,
        original_row_number,
        topping_id
    FROM 
        split_extras
)
SELECT
    t2.topping_name,
    COUNT(*) AS topping_count
FROM 
    cte_combined_orders AS t1
JOIN 
    pizza_toppings AS t2
ON 
    t1.topping_id = t2.topping_id
GROUP BY 
    t2.topping_name
ORDER BY 
    topping_count DESC;
",,snowflake
443,local065,modern_data,Calculate the total income from Meat Lovers pizzas priced at $12 and Vegetarian pizzas at $10. Include any extra toppings charged at $1 each. Ensure that canceled orders are filtered out. How much money has Pizza Runner earned in total?,"WITH get_extras_count AS (
    WITH RECURSIVE split_extras AS (
        SELECT
            order_id,
            TRIM(SUBSTR(extras, 1, INSTR(extras || ',', ',') - 1)) AS each_extra,
            SUBSTR(extras || ',', INSTR(extras || ',', ',') + 1) AS remaining_extras
        FROM
            pizza_clean_customer_orders
        UNION ALL
        SELECT
            order_id,
            TRIM(SUBSTR(remaining_extras, 1, INSTR(remaining_extras, ',') - 1)) AS each_extra,
            SUBSTR(remaining_extras, INSTR(remaining_extras, ',') + 1)
        FROM
            split_extras
        WHERE
            remaining_extras <> ''
    )
    SELECT
        order_id,
        COUNT(each_extra) AS total_extras
    FROM
        split_extras
    GROUP BY
        order_id
),
calculate_totals AS (
    SELECT
        t1.order_id,
        t1.pizza_id,
        SUM(
            CASE
                WHEN pizza_id = 1 THEN 12
                WHEN pizza_id = 2 THEN 10
            END
        ) AS total_price,
        t3.total_extras
    FROM 
        pizza_clean_customer_orders AS t1
    JOIN
        pizza_clean_runner_orders AS t2 
    ON
        t2.order_id = t1.order_id
    LEFT JOIN
        get_extras_count AS t3
    ON
        t3.order_id = t1.order_id
    WHERE
        t2.cancellation IS NULL
    GROUP BY 
        t1.order_id,
        t1.pizza_id,
        t3.total_extras
)
SELECT 
    SUM(total_price) + SUM(total_extras) AS total_income
FROM 
    calculate_totals;
",,snowflake
444,local074,bank_sales_trading,"Please generate a summary of the closing balances at the end of each month for each customer transactions, show the monthly changes and monthly cumulative bank account balances. Ensure that even if a customer has no account activity in a given month, the balance for that month is still included in the output.",,,snowflake
445,local064,bank_sales_trading,"For each customer and each month of 2020, first calculate the month-end balance by adding all deposit amounts and subtracting all withdrawal amounts that occurred during that specific month. Then determine which month in 2020 has the highest count of customers with a positive month-end balance and which month has the lowest count. For each of these two months, compute the average month-end balance across all customers and provide the difference between these two averages",,,snowflake
446,local297,bank_sales_trading,"For each customer, group all deposits and withdrawals by the first day of each month to obtain a monthly net amount, then calculate each month’s closing balance by cumulatively summing these monthly nets. Next, determine the most recent month’s growth rate by comparing its closing balance to the prior month’s balance, treating deposits as positive and withdrawals as negative, and if the previous month’s balance is zero, the growth rate should be the current month’s balance multiplied by 100. Finally, compute the percentage of customers whose most recent month shows a growth rate of more than 5%.",,,snowflake
447,local298,bank_sales_trading,"For each month, calculate the total balance from all users for the previous month (measured as of the 1st of each month), replacing any negative balances with zero. Ensure that data from the first month is used only as a baseline for calculating previous total balance, and exclude it from the final output. Sort the results in ascending order by month. ",,,snowflake
448,local299,bank_sales_trading,"For a bank database with customer transactions, calculate each customer's daily running balance (where deposits add to the balance and other transaction types subtract). For each customer and each day, compute the 30-day rolling average balance (only after having 30 days of data, and treating negative averages as zero). Then group these daily averages by month and find each customer's maximum 30-day average balance within each month. Sum these maximum values across all customers for each month. Consider the first month of each customer's transaction history as the baseline period and exclude it from the final results, presenting monthly totals of these summed maximum 30-day average balances.",,,snowflake
449,local300,bank_sales_trading,"For each customer, calculate their daily balances for every day between their earliest and latest transaction dates, including days without transactions by carrying forward the previous day's balance. Treat any negative daily balances as zero. Then, for each month, determine the highest daily balance each customer had during that month. Finally, for each month, sum these maximum daily balances across all customers to obtain a monthly total.",,,snowflake
450,local075,bank_sales_trading,"Can you provide a breakdown of how many times each product was viewed, how many times they were added to the shopping cart, and how many times they were left in the cart without being purchased? Also, give me the count of actual purchases for each product. Ensure that products with a page id in (1, 2, 12, 13) are filtered out.","WITH product_viewed AS (
    SELECT
        t1.page_id,
        SUM(CASE WHEN event_type = 1 THEN 1 ELSE 0 END) AS n_page_views,
        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS n_added_to_cart
    FROM
        shopping_cart_page_hierarchy AS t1
    JOIN
        shopping_cart_events AS t2
    ON
        t1.page_id = t2.page_id
    WHERE
        t1.product_id IS NOT NULL
    GROUP BY
        t1.page_id
),
product_purchased AS (
    SELECT
        t2.page_id,
        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS purchased_from_cart
    FROM
        shopping_cart_page_hierarchy AS t1
    JOIN
        shopping_cart_events AS t2
    ON
        t1.page_id = t2.page_id
    WHERE
        t1.product_id IS NOT NULL
        AND EXISTS (
            SELECT
                visit_id
            FROM
                shopping_cart_events
            WHERE
                event_type = 3
                AND t2.visit_id = visit_id
        )
        AND t1.page_id NOT IN (1, 2, 12, 13)
    GROUP BY
        t2.page_id
),
product_abandoned AS (
    SELECT
        t2.page_id,
        SUM(CASE WHEN event_type = 2 THEN 1 ELSE 0 END) AS abandoned_in_cart
    FROM
        shopping_cart_page_hierarchy AS t1
    JOIN
        shopping_cart_events AS t2
    ON
        t1.page_id = t2.page_id
    WHERE
        t1.product_id IS NOT NULL
        AND NOT EXISTS (
            SELECT
                visit_id
            FROM
                shopping_cart_events
            WHERE
                event_type = 3
                AND t2.visit_id = visit_id
        )
        AND t1.page_id NOT IN (1, 2, 12, 13)
    GROUP BY
        t2.page_id
)
SELECT
    t1.page_id,
    t1.page_name,
    t2.n_page_views AS 'number of product being viewed',
    t2.n_added_to_cart AS 'number added to the cart',
    t4.abandoned_in_cart AS 'without being purchased in cart',
    t3.purchased_from_cart AS 'count of actual purchases'
FROM
    shopping_cart_page_hierarchy AS t1
JOIN
    product_viewed AS t2 
ON
    t2.page_id = t1.page_id
JOIN
    product_purchased AS t3 
ON 
    t3.page_id = t1.page_id
JOIN
    product_abandoned AS t4 
ON 
    t4.page_id = t1.page_id;
",,snowflake
451,local077,bank_sales_trading,"Please analyze our interest data from September 2018 to August 2019. For each month, calculate the average composition for each interest by dividing the composition by the index value. Identify the interest with the highest average composition value each month and report its average composition as the max index composition for that month. Compute the three-month rolling average of these monthly max index compositions. Ensure the output includes the date, the interest name, the max index composition for that month, the rolling average, and the names and max index compositions of the top interests from one month ago and two months ago.",,,snowflake
452,local078,bank_sales_trading,"Identify the top 10 and bottom 10 interest categories based on their highest composition values across all months. For each category, display the time(MM-YYYY), interest name, and the composition value","WITH get_interest_rank AS (
    SELECT
        t1.month_year,
        t2.interest_name,
        t1.composition,
        RANK() OVER (
            PARTITION BY t2.interest_name
            ORDER BY t1.composition DESC
        ) AS interest_rank
    FROM 
        interest_metrics AS t1
    JOIN 
        interest_map AS t2
    ON 
        t1.interest_id = t2.id
    WHERE 
        t1.month_year IS NOT NULL
),
get_top_10 AS (
    SELECT
        month_year,
        interest_name,
        composition
    FROM 
        get_interest_rank
    WHERE 
        interest_rank = 1
    ORDER BY 
        composition DESC
    LIMIT 10
),
get_bottom_10 AS (
    SELECT
        month_year,
        interest_name,
        composition
    FROM 
        get_interest_rank
    WHERE 
        interest_rank = 1
    ORDER BY 
        composition ASC
    LIMIT 10
)
SELECT * 
FROM 
    get_top_10
UNION
SELECT * 
FROM 
    get_bottom_10
ORDER BY 
    composition DESC;
",,snowflake
453,local081,northwind,"Considering only the customers who placed orders in 1998, calculate the total amount each customer spent by summing the unit price multiplied by the quantity of all products in their orders, excluding any discounts. Assign each customer to a spending group based on the customer group thresholds, and determine how many customers are in each spending group and what percentage of the total number of customers who placed orders in 1998 each group represents.",,,snowflake
454,local085,northwind,"Among employees who have more than 50 total orders, which three have the highest percentage of late orders, where an order is considered late if the shipped date is on or after its required date? Please list each employee's ID, the number of late orders, and the corresponding late-order percentage.",,,snowflake
455,local096,Db-IMDB,"For each year, calculate the percentage of films that had exclusively female actors (meaning no male actors and no actors with unknown/unspecified gender). Consider actors with gender marked as 'Male' or 'None' as non-female. For the results, display the year, the total number of movies in that year, and the percentage of movies with exclusively female actors. Extract the year from the Movie.year field by taking the last 4 characters and converting to a number.",,,snowflake
456,local097,Db-IMDB,"Could you analyze our data and identify which ten-year period starting from any movie release year present in the data had the largest number of films, considering consecutive ten-year periods beginning at each unique year? Only output the start year and the total count for that specific period.",,,snowflake
457,local098,Db-IMDB,"From the first year each actor appeared in a film to the last, how many actors in the database never had a gap longer than three consecutive years without at least one new movie appearance, meaning there is no four-year span anywhere in their active career without at least a single film credit?",,,snowflake
458,local099,Db-IMDB,I need you to look into the actor collaborations and tell me how many actors have made more films with Yash Chopra than with any other director. This will help us understand his influence on the industry better.,"WITH YASH_CHOPRAS_PID AS (
    SELECT
        TRIM(P.PID) AS PID
    FROM
        Person P
    WHERE
        TRIM(P.Name) = 'Yash Chopra'
),
NUM_OF_MOV_BY_ACTOR_DIRECTOR AS (
    SELECT
        TRIM(MC.PID) AS ACTOR_PID,
        TRIM(MD.PID) AS DIRECTOR_PID,
        COUNT(DISTINCT TRIM(MD.MID)) AS NUM_OF_MOV
    FROM
        M_Cast MC
    JOIN
        M_Director MD ON TRIM(MC.MID) = TRIM(MD.MID)
    GROUP BY
        ACTOR_PID,
        DIRECTOR_PID
),
NUM_OF_MOVIES_BY_YC AS (
    SELECT
        NM.ACTOR_PID,
        NM.DIRECTOR_PID,
        NM.NUM_OF_MOV AS NUM_OF_MOV_BY_YC
    FROM
        NUM_OF_MOV_BY_ACTOR_DIRECTOR NM
    JOIN
        YASH_CHOPRAS_PID YCP ON NM.DIRECTOR_PID = YCP.PID
),
MAX_MOV_BY_OTHER_DIRECTORS AS (
    SELECT
        ACTOR_PID,
        MAX(NUM_OF_MOV) AS MAX_NUM_OF_MOV
    FROM
        NUM_OF_MOV_BY_ACTOR_DIRECTOR NM
    JOIN
        YASH_CHOPRAS_PID YCP ON NM.DIRECTOR_PID <> YCP.PID
    GROUP BY
        ACTOR_PID
),
ACTORS_MOV_COMPARISION AS (
    SELECT
        NMY.ACTOR_PID,
        CASE WHEN NMY.NUM_OF_MOV_BY_YC > IFNULL(NMO.MAX_NUM_OF_MOV, 0) THEN 'Y' ELSE 'N' END AS MORE_MOV_BY_YC
    FROM
        NUM_OF_MOVIES_BY_YC NMY
    LEFT OUTER JOIN
        MAX_MOV_BY_OTHER_DIRECTORS NMO ON NMY.ACTOR_PID = NMO.ACTOR_PID
)
SELECT
    COUNT(DISTINCT TRIM(P.PID)) AS ""Number of actor""
FROM
    Person P
WHERE
    TRIM(P.PID) IN (
        SELECT
            DISTINCT ACTOR_PID
        FROM
            ACTORS_MOV_COMPARISION
        WHERE
            MORE_MOV_BY_YC = 'Y'
    );
",,snowflake
459,local100,Db-IMDB,"Find out how many actors have a 'Shahrukh number' of 2? This means they acted in a film with someone who acted with Shahrukh Khan, but not directly with him.",,,snowflake
460,local114,education_business,"Provide a detailed web sales report for each region, including the number of orders, total sales amount, and the name and sales amount of all sales representatives who achieved the highest total sales amount in that region (include all representatives in case of a tie).",,,snowflake
461,local128,BowlingLeague,"List the bowlers (including their ID, first name, and last name), match number, game number, handicap score, tournament date, and location for only those bowlers who have won games with a handicap score of 190 or less at all three venues: Thunderbird Lanes, Totem Lanes, and Bolero Lanes. Only include the specific game records where they won with a handicap score of 190 or less at these three locations.",,,snowflake
462,local130,school_scheduling,"Could you provide a list of last names for all students who have completed English courses (where completion is defined as having a ClassStatus of 2), along with their quintile ranks based on their individual grades in those courses? The quintile should be determined by calculating how many students have grades greater than or equal to each student's grade, then dividing this ranking by the total number of students who completed English courses. The quintiles should be labeled as ""First"" (top 20%), ""Second"" (top 21-40%), ""Third"" (top 41-60%), ""Fourth"" (top 61-80%), and ""Fifth"" (bottom 20%). Please sort the results from highest performing quintile to lowest (First to Fifth).",,,snowflake
463,local131,EntertainmentAgency,"Could you list each musical style with the number of times it appears as a 1st, 2nd, or 3rd preference in a single row per style?","SELECT 
  Musical_Styles.StyleName,
  COUNT(RankedPreferences.FirstStyle)
    AS FirstPreference,
  COUNT(RankedPreferences.SecondStyle)
    AS SecondPreference,
  COUNT(RankedPreferences.ThirdStyle)
    AS ThirdPreference
FROM Musical_Styles,
 (SELECT (CASE WHEN
    Musical_Preferences.PreferenceSeq = 1
               THEN Musical_Preferences.StyleID
               ELSE Null END) As FirstStyle,
         (CASE WHEN
    Musical_Preferences.PreferenceSeq = 2
               THEN Musical_Preferences.StyleID
               ELSE Null END) As SecondStyle,
         (CASE WHEN
    Musical_Preferences.PreferenceSeq = 3
               THEN Musical_Preferences.StyleID
               ELSE Null END) AS ThirdStyle
   FROM Musical_Preferences)  AS RankedPreferences
WHERE Musical_Styles.StyleID =
         RankedPreferences.FirstStyle
  OR Musical_Styles.StyleID =
         RankedPreferences.SecondStyle
  OR Musical_Styles.StyleID =
         RankedPreferences.ThirdStyle
GROUP BY StyleID, StyleName
HAVING COUNT(FirstStyle) > 0
     OR     COUNT(SecondStyle) > 0
     OR     COUNT(ThirdStyle) > 0
ORDER BY FirstPreference DESC,
        SecondPreference DESC,
        ThirdPreference DESC, StyleID;",,snowflake
464,local133,EntertainmentAgency,"Given a database of musical styles and user preferences, where Musical_Preferences contains user rankings of musical styles (PreferenceSeq=1 for first choice, PreferenceSeq=2 for second choice, PreferenceSeq=3 for third choice): Calculate a weighted score for each musical style by assigning 3 points for each time it was ranked as first choice, 2 points for each second choice, and 1 point for each third choice ranking. Calculate the total weighted score for each musical style that has been ranked by at least one user. Then, compute the absolute difference between each style's total weighted score and the average total weighted score across all such styles.	",,,snowflake
465,local132,EntertainmentAgency,"Show all pairs of entertainers and customers who each have up to three style strengths or preferences, where the first and second style preferences of the customers match the first and second style strengths of the entertainers (or in reverse order). Only return the entertainer’s stage name and the customer’s last name",,,snowflake
466,local141,AdventureWorks,"How did each salesperson's annual total sales compare to their annual sales quota? Provide the difference between their total sales and the quota for each year, organized by salesperson and year.",,,snowflake
467,local152,imdb_movies,"Can you provide the top 9 directors by movie count, including their ID, name, number of movies, average inter-movie duration (rounded to the nearest integer), average rating (rounded to 2 decimals), total votes, minimum and maximum ratings, and total movie duration? Sort the output first by movie count in descending order and then by total movie duration in descending order.",,,snowflake
468,local230,imdb_movies,"Determine the top three genres with the most movies rated above 8, and then identify the top four directors who have directed the most films rated above 8 within those genres. List these directors and their respective movie counts.",,,snowflake
469,local156,bank_sales_trading,"Analyze the annual average purchase price per Bitcoin by region, computed as the total dollar amount spent divided by the total quantity purchased each year, excluding the first year's data for each region. Then, for each year, rank the regions based on these average purchase prices, and calculate the annual percentage change in cost for each region compared to the previous year.",,,snowflake
470,local157,bank_sales_trading,"Using the ""bitcoin_prices"" table, please calculate the daily percentage change in trading volume for each ticker from August 1 to August 10, 2021, ensuring that any volume ending in ""K"" or ""M"" is accurately converted to thousands or millions, any ""-"" volume is treated as zero, only non-zero volumes are used to determine the previous day's volume, and the results are ordered by ticker and date.",,,snowflake
471,local163,education_business,"Which university faculty members' salaries are closest to the average salary for their respective ranks? Please provide the ranks, first names, last names, and salaries.university","WITH AvgSalaries AS (
    SELECT 
        facrank AS FacRank,
        AVG(facsalary) AS AvSalary
    FROM 
        university_faculty
    GROUP BY 
        facrank
),
SalaryDifferences AS (
    SELECT 
        university_faculty.facrank AS FacRank, 
        university_faculty.facfirstname AS FacFirstName, 
        university_faculty.faclastname AS FacLastName, 
        university_faculty.facsalary AS Salary, 
        ABS(university_faculty.facsalary - AvgSalaries.AvSalary) AS Diff
    FROM 
        university_faculty
    JOIN 
        AvgSalaries ON university_faculty.facrank = AvgSalaries.FacRank
),
MinDifferences AS (
    SELECT 
        FacRank, 
        MIN(Diff) AS MinDiff
    FROM 
        SalaryDifferences
    GROUP BY 
        FacRank
)
SELECT 
    s.FacRank, 
    s.FacFirstName, 
    s.FacLastName, 
    s.Salary
FROM 
    SalaryDifferences s
JOIN 
    MinDifferences m ON s.FacRank = m.FacRank AND s.Diff = m.MinDiff;
",,snowflake
472,local168,city_legislation,"Among job postings that specifically have the Data Analyst, require a non-null annual average salary, and are remote, what is the overall average salary when considering only the top three most frequently demanded skills for these positions?",,,snowflake
473,local169,city_legislation,"What is the annual retention rate of legislators who began their first term between January 1, 1917 and December 31, 1999, measured as the proportion of this cohort still in office on December 31st for each of the first 20 years following their initial term start? The results should show all 20 periods in sequence regardless of whether any legislators were retained in a particular year.",,,snowflake
474,local171,city_legislation,"For male legislators from Louisiana, how many distinct legislators were actively serving on December 31 of each year from more than 30 years since their first term up to less than 50 years, grouping the results by the exact number of years elapsed since their first term?",,,snowflake
475,local167,city_legislation,"Based on the state each female legislator first represented, which state has the highest number of female legislators whose terms included December 31st at any point, and what is that count? Please provide the state's abbreviation.",,,snowflake
476,local170,city_legislation,"Identify the state abbreviations where, for both male and female legislators, the retention rate remains greater than zero at specific intervals of 0, 2, 4, 6, 8, and 10 years after their first term start date. A legislator is considered retained if they are serving on December 31 of the respective year. Only include states where both gender cohorts maintain non-zero retention rates at all six of these time points during the first decade of service.",,,snowflake
477,local193,sqlite-sakila,"Could you find out the average percentage of the total lifetime sales (LTV) that occur in the first 7 and 30 days after a customer's initial purchase? Also, include the average total lifetime sales (LTV). Please exclude customers with zero lifetime sales. The 7- and 30-day periods should be based on the exact number of hours-minutes-seconds, not calendar days.",,,snowflake
478,local194,sqlite-sakila,"Please provide a list of the top three revenue-generating films for each actor, along with the average revenue per actor in those films, calculated by dividing the total film revenue equally among the actors for each film.",,,snowflake
479,local195,sqlite-sakila,Please find out how widespread the appeal of our top five actors is. What percentage of our customers have rented films featuring these actors?,,,snowflake
480,local196,sqlite-sakila,For each rating category of the first movie rented by customers—where the first movie is identified based on the earliest payment date per customer—please provide the average total amount spent per customer and the average number of subsequent rentals (calculated as the total number of rentals minus one) for customers whose first rented movie falls into that rating category.,,,snowflake
481,local197,sqlite-sakila,"Among our top 10 paying customers, can you identify the largest change in payment amounts from one month to the immediately following month? Specifically, please determine for which customer and during which month this maximum month-over-month difference occurred, and provide the difference rounded to two decimal places.","WITH result_table AS (
  SELECT 
    strftime('%m', pm.payment_date) AS pay_mon, 
    customer_id,
    COUNT(pm.amount) AS pay_countpermon, 
    SUM(pm.amount) AS pay_amount 
  FROM 
    payment AS pm 
  GROUP BY 
    pay_mon, 
    customer_id
), 
top10_customer AS (
  SELECT 
    customer_id,
    SUM(tb.pay_amount) AS total_payments 
  FROM 
    result_table AS tb 
  GROUP BY 
    customer_id
  ORDER BY 
    SUM(tb.pay_amount) DESC 
  LIMIT 
    10
), 
difference_per_mon AS (
  SELECT 
    pay_mon AS month_number, 
    pay_mon AS month, 
    tb.pay_countpermon, 
    tb.pay_amount, 
    ABS(tb.pay_amount - LAG(tb.pay_amount) OVER (PARTITION BY tb.customer_id)) AS diff 
  FROM 
    result_table tb 
    JOIN top10_customer top ON top.customer_id = tb.customer_id
) 
SELECT 
  month, 
  ROUND(max_diff, 2) AS max_diff 
FROM (
  SELECT 
    month, 
    diff, 
    month_number, 
    MAX(diff) OVER (PARTITION BY month) AS max_diff 
  FROM 
    difference_per_mon
) AS max_per_mon 
WHERE 
  diff = max_diff 
ORDER BY 
  max_diff DESC 
LIMIT 
  1;
",,snowflake
482,local199,sqlite-sakila,"Can you identify the year and month with the highest rental orders created by the store's staff for each store? Please list the store ID, the year, the month, and the total rentals for those dates.","WITH result_table AS (
  SELECT 
    strftime('%Y', RE.RENTAL_DATE) AS YEAR, 
    strftime('%m', RE.RENTAL_DATE) AS RENTAL_MONTH, 
    ST.STORE_ID, 
    COUNT(RE.RENTAL_ID) AS count 
  FROM 
    RENTAL RE 
    JOIN STAFF ST ON RE.STAFF_ID = ST.STAFF_ID 
  GROUP BY 
    YEAR, 
    RENTAL_MONTH, 
    ST.STORE_ID 
), 
monthly_sales AS (
  SELECT 
    YEAR, 
    RENTAL_MONTH, 
    STORE_ID, 
    SUM(count) AS total_rentals 
  FROM 
    result_table 
  GROUP BY 
    YEAR, 
    RENTAL_MONTH, 
    STORE_ID
),
store_max_sales AS (
  SELECT 
    STORE_ID, 
    YEAR, 
    RENTAL_MONTH, 
    total_rentals, 
    MAX(total_rentals) OVER (PARTITION BY STORE_ID) AS max_rentals 
  FROM 
    monthly_sales
)
SELECT 
  STORE_ID, 
  YEAR, 
  RENTAL_MONTH, 
  total_rentals 
FROM 
  store_max_sales 
WHERE 
  total_rentals = max_rentals
ORDER BY 
  STORE_ID;
",,snowflake
483,local201,modern_data,"Identify the first 10 words, sorted alphabetically, that are 4 to 5 characters long, start with 'r', and have at least one anagram of the same length, considering case-sensitive letters. Provide the count of such anagrams for each word.",,,snowflake
484,local202,city_legislation,"For alien data, how many of the top 10 states by alien population have a higher percentage of friendly aliens than hostile aliens, with an average alien age exceeding 200?",,,snowflake
485,local209,delivery_center,"In the dataset of orders joined with store information, which store has the highest total number of orders, and among that store’s orders, what is the ratio of orders that appear in the deliveries table with a 'DELIVERED' status to the total orders for that store?",,,snowflake
486,local210,delivery_center,Can you identify the hubs that saw more than a 20% increase in finished orders from February to March?,"WITH february_orders AS (
    SELECT
        h.hub_name AS hub_name,
        COUNT(*) AS orders_february
    FROM 
        orders o 
    LEFT JOIN
        stores s ON o.store_id = s.store_id 
    LEFT JOIN 
        hubs h ON s.hub_id = h.hub_id 
    WHERE o.order_created_month = 2 AND o.order_status = 'FINISHED'
    GROUP BY
        h.hub_name
),
march_orders AS (
    SELECT
        h.hub_name AS hub_name,
        COUNT(*) AS orders_march
    FROM 
        orders o 
    LEFT JOIN
        stores s ON o.store_id = s.store_id 
    LEFT JOIN 
        hubs h ON s.hub_id = h.hub_id 
    WHERE o.order_created_month = 3 AND o.order_status = 'FINISHED'
    GROUP BY
        h.hub_name
)
SELECT
    fo.hub_name
FROM
    february_orders fo
LEFT JOIN 
    march_orders mo ON fo.hub_name = mo.hub_name
WHERE 
    fo.orders_february > 0 AND 
    mo.orders_march > 0 AND
    (CAST((mo.orders_march - fo.orders_february) AS REAL) / CAST(fo.orders_february AS REAL)) > 0.2  -- Filter for hubs with more than a 20% increase",,snowflake
487,local212,delivery_center,Can you find 5 delivery drivers with the highest average number of daily deliveries?,,,snowflake
488,local218,EU_soccer,Can you calculate the median from the highest season goals of each team?,,,snowflake
489,local219,EU_soccer,"In each league, considering all seasons, which single team has the fewest total match wins based on comparing home and away goals, including teams with zero wins, ensuring that if multiple teams tie for the fewest wins, only one team is returned for each league?","WITH match_view AS(
SELECT
    M.id,
    L.name AS league,
    M.season,
    M.match_api_id,
    T.team_long_name AS home_team,
    TM.team_long_name AS away_team,
    M.home_team_goal,
    M.away_team_goal,
    P1.player_name AS home_gk,
    P2.player_name AS home_center_back_1,
    P3.player_name AS home_center_back_2,
    P4.player_name AS home_right_back,
    P5.player_name AS home_left_back,
    P6.player_name AS home_midfield_1,
    P7.player_name AS home_midfield_2,
    P8.player_name AS home_midfield_3,
    P9.player_name AS home_midfield_4,
    P10.player_name AS home_second_forward,
    P11.player_name AS home_center_forward,
    P12.player_name AS away_gk,
    P13.player_name AS away_center_back_1,
    P14.player_name AS away_center_back_2,
    P15.player_name AS away_right_back,
    P16.player_name AS away_left_back,
    P17.player_name AS away_midfield_1,
    P18.player_name AS away_midfield_2,
    P19.player_name AS away_midfield_3,
    P20.player_name AS away_midfield_4,
    P21.player_name AS away_second_forward,
    P22.player_name AS away_center_forward,
    M.goal,
    M.card
FROM
    match M
LEFT JOIN
    league L ON M.league_id = L.id
LEFT JOIN
    team T ON M.home_team_api_id = T.team_api_id
LEFT JOIN
    team TM ON M.away_team_api_id = TM.team_api_id
LEFT JOIN
    player P1 ON M.home_player_1 = P1.player_api_id
LEFT JOIN
    player P2 ON M.home_player_2 = P2.player_api_id
LEFT JOIN
    player P3 ON M.home_player_3 = P3.player_api_id
LEFT JOIN
    player P4 ON M.home_player_4 = P4.player_api_id
LEFT JOIN
    player P5 ON M.home_player_5 = P5.player_api_id
LEFT JOIN
    player P6 ON M.home_player_6 = P6.player_api_id
LEFT JOIN
    player P7 ON M.home_player_7 = P7.player_api_id
LEFT JOIN
    player P8 ON M.home_player_8 = P8.player_api_id
LEFT JOIN
    player P9 ON M.home_player_9 = P9.player_api_id
LEFT JOIN
    player P10 ON M.home_player_10 = P10.player_api_id
LEFT JOIN
    player P11 ON M.home_player_11 = P11.player_api_id
LEFT JOIN
    player P12 ON M.away_player_1 = P12.player_api_id
LEFT JOIN
    player P13 ON M.away_player_2 = P13.player_api_id
LEFT JOIN
    player P14 ON M.away_player_3 = P14.player_api_id
LEFT JOIN
    player P15 ON M.away_player_4 = P15.player_api_id
LEFT JOIN
    player P16 ON M.away_player_5 = P16.player_api_id
LEFT JOIN
    player P17 ON M.away_player_6 = P17.player_api_id
LEFT JOIN
    player P18 ON M.away_player_7 = P18.player_api_id
LEFT JOIN
    player P19 ON M.away_player_8 = P19.player_api_id
LEFT JOIN
    player P20 ON M.away_player_9 = P20.player_api_id
LEFT JOIN
    player P21 ON M.away_player_10 = P21.player_api_id
LEFT JOIN
    player P22 ON M.away_player_11 = P22.player_api_id
),
match_score AS
(
    SELECT  -- Displaying teams and their goals as home_team
        id,
        home_team AS team,
        CASE
            WHEN home_team_goal > away_team_goal THEN 1 ELSE 0 END AS Winning_match
    FROM
        match_view

    UNION ALL

    SELECT  -- Displaying teams and their goals as away_team
        id,
        away_team AS team,
        CASE
            WHEN away_team_goal > home_team_goal THEN 1 ELSE 0 END AS Winning_match
    FROM
        match_view
),
winning_matches AS
(
    SELECT  -- Displaying total match wins for each team
        MV.league,
        M.team,
        COUNT(CASE WHEN M.Winning_match = 1 THEN 1 END) AS wins,
        ROW_NUMBER() OVER(PARTITION BY MV.league ORDER BY COUNT(CASE WHEN M.Winning_match = 1 THEN 1 END) ASC) AS rn
    FROM
        match_score M
    JOIN
        match_view MV
    ON
        M.id = MV.id
    GROUP BY
        MV.league,
        team
    ORDER BY
        league,
        wins ASC
)
SELECT
    league,
    team
FROM
    winning_matches
WHERE
    rn = 1  -- Getting the team with the least number of wins in each league
ORDER BY
    league;",,snowflake
490,local221,EU_soccer,Tell me top10 teams with the most wins across the league,,,snowflake
491,local220,EU_soccer,"Which player has participated in the highest number of winning matches and which player has participated in the highest number of losing matches, considering only matches where they actually played (excluding null entries) and where their team won or lost (excluding draws)?",,,snowflake
492,local228,IPL,"For each IPL season, identify the top three batsmen with the highest total runs scored and the top three bowlers with the most wickets taken, excluding ‘run out’, ‘hit wicket’, and ‘retired hurt’ dismissals. In the event of ties in runs or wickets, break the tie using the smaller player ID. Then output these six players in matched positions—batsman 1 with bowler 1, batsman 2 with bowler 2, and batsman 3 with bowler 3—in ascending order of the season ID, along with each player’s total runs or wickets.",,,snowflake
493,local229,IPL,"Find the IDs of players who scored the highest number of partnership runs for each match. The output should include the IDs of two players, each with their individual scores and the total partnership score. For each pair, the player with the higher individual score should be listed as player 1, and the player with the lower score as player 2. In cases where both players have the same score, the player with the higher ID should be player 1, and the player with the lower ID should be player 2. There can be multiple rows for a single match.",,,snowflake
494,local244,music,"Calculate the duration of each track, classify them as short, medium, or long, output the minimum and maximum time for each kind (in minutes) and the total revenue for each category, group by the category.",,"# Music Length Types

## Short 
- Duration between the minimum value and the midpoint between the minimum and average values.

## Medium 
- Duration between the midpoint between the minimum and average values and the midpoint between the average and maximum values.

## Long 
- Duration between the midpoint between the average and maximum values and the maximum value.",snowflake
495,local253,education_business,"Using a Salary Dataset where the salary values need to be cleaned by removing non-numeric characters and converting them to a numeric type, write a detailed SQL query that identifies the top 5 companies by average salary in each of Mumbai, Pune, New Delhi, and Hyderabad, then compares each company’s average salary in those cities to the overall national average salary. The final result should display four columns: Location, Company Name, Average Salary in State, and Average Salary in Country, listing only the top 5 companies in each of the specified locations.",,,snowflake
496,local258,IPL,"Calculate the total number of wickets taken by each bowler (excluding run-outs and other dismissals not attributed to the bowler), their economy rate (total runs conceded divided by total overs bowled, considering only runs scored off the bat and ignoring any extra runs like wides and no-balls), their strike rate (average number of balls bowled per wicket taken), and their best bowling performance in a single match (the match with the most wickets taken by the bowler, formatted as ""wickets-runs"" where runs are the runs conceded excluding extras).",,"# Special Words Definition

## Batting Average
- Batting average is a measure of a batsman's performance, calculated by dividing the total number of runs scored by the number of times they have been dismissed. A higher batting average indicates a more consistent and successful batsman.
- Batting Average = Total Runs ÷ Total Dismissals

## Strike Rate
- In batting, strike rate indicates the scoring rate of a batsman, showing the average number of runs scored per 100 balls faced. A higher strike rate reflects an aggressive and fast-scoring batsman.
- Strike Rate = (Total Runs ÷ Balls Faced) × 100

## Wickets Taken
- Wickets taken refers to the total number of times a bowler has successfully dismissed an opposing batsman. This statistic is a core measure of a bowler's effectiveness and impact on the game.

## Economy Rate
- Economy rate measures a bowler's efficiency by indicating the average number of runs conceded per over (6 balls) bowled. A lower economy rate shows greater control and efficiency in restricting the opposition's scoring.
- Economy Rate = (Total Runs Conceded ÷ Balls Bowled) × 6

## Wickets Taken
- The number of times a bowler successfully dismisses a batsman in a single match.  This metric is a direct indicator of the bowler's effectiveness, showing how many players they were able to remove from play.

## Runs Given
- The total number of runs scored by the batting side off a particular bowler’s deliveries in a single match.  This value reflects the runs the bowler has conceded to the batting team, and a lower runs-given value is typically favorable for the bowler's performance.",snowflake
497,local259,IPL,"For each player, list their ID, name, their most frequent role across all matches, batting hand, bowling skill, total runs scored, total matches played, total times they were dismissed, batting average (total runs divided by total dismissals), highest score in a single match, the number of matches in which they scored at least 30 runs, at least 50 runs, and at least 100 runs, total balls faced in their career, strike rate (total runs divided by total balls faced, multiplied by 100), total wickets taken, economy rate (average runs conceded per over), and their best bowling performance in a single match (most wickets taken in a match, formatted as ""wickets taken-runs given"", where the best performance is the one with the most wickets, and if tied, the fewest runs conceded). Ignore the extra runs data.",,"# Special Words Definition

## Batting Average
- Batting average is a measure of a batsman's performance, calculated by dividing the total number of runs scored by the number of times they have been dismissed. A higher batting average indicates a more consistent and successful batsman.
- Batting Average = Total Runs ÷ Total Dismissals

## Strike Rate
- In batting, strike rate indicates the scoring rate of a batsman, showing the average number of runs scored per 100 balls faced. A higher strike rate reflects an aggressive and fast-scoring batsman.
- Strike Rate = (Total Runs ÷ Balls Faced) × 100

## Wickets Taken
- Wickets taken refers to the total number of times a bowler has successfully dismissed an opposing batsman. This statistic is a core measure of a bowler's effectiveness and impact on the game.

## Economy Rate
- Economy rate measures a bowler's efficiency by indicating the average number of runs conceded per over (6 balls) bowled. A lower economy rate shows greater control and efficiency in restricting the opposition's scoring.
- Economy Rate = (Total Runs Conceded ÷ Balls Bowled) × 6

## Wickets Taken
- The number of times a bowler successfully dismisses a batsman in a single match.  This metric is a direct indicator of the bowler's effectiveness, showing how many players they were able to remove from play.

## Runs Given
- The total number of runs scored by the batting side off a particular bowler’s deliveries in a single match.  This value reflects the runs the bowler has conceded to the batting team, and a lower runs-given value is typically favorable for the bowler's performance.",snowflake
498,local262,stacking,"Which problems exceed the total number of times they appear in the solution table when counting all occurrences, across steps 1, 2, and 3, where any non-""Stack"" model's maximum test score is lower than the ""Stack"" model's test score for the same step and version?",,,snowflake
499,local263,stacking,"Identify the L1_model associated with each model (specified by name and version) that occurs most frequently for each status ('strong' or 'soft'), along with the number of times it occurs. A model has a 'strong' status if, for any of its steps, the maximum test score among non-'Stack' models is less than the 'Stack' model's test score. It has a 'soft' status if the maximum test score among non-'Stack' models equals the 'Stack' model's test score. Count how many times each L1_model is associated with a 'strong' or 'soft' status across all models, and determine which L1_model has the highest occurrence for each status.",,,snowflake
500,local264,stacking,"Which model category (L1_model) appears the most frequently across all steps and versions when comparing traditional models to the Stack model, and what is the total count of its occurrences?",,,snowflake
501,local269,oracle_sql,"What is the average total quantity across all final packaging combinations, considering only the leaf-level items within each combination after fully expanding any nested packaging relationships?",,,snowflake
502,local270,oracle_sql,"Which top-level packaging containers, meaning those not contained within any other packaging, have any item for which the total quantity accumulated across all nested levels in the hierarchy exceeds 500, and what are the names of both these containers and the corresponding items?",,,snowflake
503,local272,oracle_sql,"For order 423, identify the product IDs, aisles, and positions from which to pick the exact quantities needed for each order line, ensuring that the total picked quantity for each product matches the cumulative quantities ordered without exceeding the available inventory in warehouse 1. Calculate the quantities to be picked from each location by prioritizing inventory with earlier purchased dates and smaller quantities, and ensure that picking respects the sequence and cumulative quantities of the order lines for products with multiple entries.",,,snowflake
504,local273,oracle_sql,"Calculate the average pick percentage for each product name, using a first-in-first-out approach that selects from inventory locations based on the earliest purchase date and smallest available quantity, ensuring that the picked quantity reflects only the overlapping range between each order’s required quantity and the inventory’s available quantity, and then grouping and ordering the results by product name?",,,snowflake
505,local274,oracle_sql,"Which products were picked for order 421, and what is the average number of units picked for each product, using FIFO (First-In, First-Out) method?",,,snowflake
506,local275,oracle_sql,"Based on monthly sales data starting in January 2016 and using a centered moving average to adjust for seasonality, which products had a seasonality-adjusted sales ratio that stayed consistently above 2 for every month in the year 2017?",,"
# Explanation of Metrics

## 1. Sales-to-CMA Ratio
- **Definition**: This ratio compares actual sales to the centered moving average (CMA) of sales.
- **Calculation**:
  - **Centered Moving Average (CMA)**: The CMA is a smoothed value of sales calculated over a rolling 12-month period. It averages sales from the months before and after a given month, specifically using two overlapping windows (5 months before and 6 months after, and vice versa).
  - **Sales-to-CMA Ratio**: The ratio is computed by dividing the actual sales amount for a month by its corresponding CMA value. A ratio greater than 2 indicates that the actual sales are more than twice the smoothed average for that period, suggesting significantly higher-than-average sales.

## 2. 12-Month Overlapping Windows
- **Definition**: A method to smooth sales data over time by averaging values in a specified window.
- **Calculation**:
  - **Window Size**: The window spans 12 months, with the specific approach involving overlapping periods. 
  - **First Window**: For a given month, this window includes 5 months before and 6 months after.
  - **Second Window**: Another window includes 6 months before and 5 months after the given month.
  - **Averaging**: Sales data is averaged over these two overlapping windows to compute the CMA. This method smooths out fluctuations by considering both the past and future sales in the calculation.

## 3. Restriction to the 7th and 30th Months
- **Definition**: A filter applied to focus calculations within a specific range of months.
- **Calculation**:
  - **Time Range**: Only the months between the 7th and 30th time steps (which correspond to specific periods) are considered for calculating the CMA and ratio.
  - **Purpose**: This restriction is used to avoid edge effects in the data where the moving average might be less reliable (e.g., at the very beginning or end of the available data). By focusing on these months, the calculations are more stable and meaningful.
",snowflake
507,local277,oracle_sql,"What is the average forecasted annual sales for products 4160 and 7790 during 2018, using monthly sales data starting from January 2016 for the first 36 months, applying seasonality adjustments from time steps 7 through 30, and employing a weighted regression method to estimate sales?",,"
# Explanation of Metrics

## 1. Sales-to-CMA Ratio
- **Definition**: This ratio compares actual sales to the centered moving average (CMA) of sales.
- **Calculation**:
  - **Centered Moving Average (CMA)**: The CMA is a smoothed value of sales calculated over a rolling 12-month period. It averages sales from the months before and after a given month, specifically using two overlapping windows (5 months before and 6 months after, and vice versa).
  - **Sales-to-CMA Ratio**: The ratio is computed by dividing the actual sales amount for a month by its corresponding CMA value. A ratio greater than 2 indicates that the actual sales are more than twice the smoothed average for that period, suggesting significantly higher-than-average sales.

## 2. 12-Month Overlapping Windows
- **Definition**: A method to smooth sales data over time by averaging values in a specified window.
- **Calculation**:
  - **Window Size**: The window spans 12 months, with the specific approach involving overlapping periods. 
  - **First Window**: For a given month, this window includes 5 months before and 6 months after.
  - **Second Window**: Another window includes 6 months before and 5 months after the given month.
  - **Averaging**: Sales data is averaged over these two overlapping windows to compute the CMA. This method smooths out fluctuations by considering both the past and future sales in the calculation.

## 3. Restriction to the 7th and 30th Months
- **Definition**: A filter applied to focus calculations within a specific range of months.
- **Calculation**:
  - **Time Range**: Only the months between the 7th and 30th time steps (which correspond to specific periods) are considered for calculating the CMA and ratio.
  - **Purpose**: This restriction is used to avoid edge effects in the data where the moving average might be less reliable (e.g., at the very beginning or end of the available data). By focusing on these months, the calculations are more stable and meaningful.
",snowflake
508,local279,oracle_sql,"Using a recursive monthly inventory adjustment model starting from December 2018 inventory levels, where we restock a product if its ending inventory drops below the minimum required level, determine for each product the month in 2019 where the absolute difference between its ending inventory and the minimum required level is the smallest, and return the product_id, that month, and the absolute difference.",,,snowflake
509,local283,EU_soccer,"Analyze the soccer match dataset to determine the champion team for each season across all countries and leagues, awarding 3 points for every win, 1 point for every tie, and 0 points for every loss. For each season, return the champion’s team name, the league, the country, and the total points accumulated.",,,snowflake
510,local284,bank_sales_trading,"For veg whsle data, can you generate a summary of our items' loss rates? Include the average loss rate, and also break down the count of items that are below, above, and within one standard deviation from this average.",,,snowflake
511,local285,bank_sales_trading,"For veg whsle data, can you analyze our financial performance over the years 2020 to 2023? I need insights into the average wholesale price, maximum wholesale price, minimum wholesale price, wholesale price difference, total wholesale price, total selling price, average loss rate, total loss, and profit for each category within each year. Round all calculated values to two decimal places.",,,snowflake
512,local286,electronic_sales,"Prepare a comprehensive performance report on our sellers, focusing on total sales, average item price, average review scores, and packing times. Ensure that the report includes only those sellers who have sold a quantity of more than 100 products and highlight the product category names in English with the highest sales volume.",,,snowflake
513,local301,bank_sales_trading,"For weekly-sales data, I need an analysis of our sales performance around mid-June for the years 2018, 2019, and 2020. Specifically, calculate the percentage change in sales between the four weeks leading up to June 15 and the four weeks following June 15 for each year.","SELECT 
    before_effect,
    after_effect,
    after_effect - before_effect AS change_amount,
    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,
    '2018' AS year
FROM (
    SELECT 
        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,
        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect
    FROM (
        SELECT 
            week_date,
            ROUND((JULIANDAY(week_date) - JULIANDAY('2018-06-15')) / 7.0) + 1 AS delta_weeks,
            sales 
        FROM cleaned_weekly_sales
    ) add_delta_weeks
) AS add_before_after
UNION ALL
SELECT 
    before_effect,
    after_effect,
    after_effect - before_effect AS change_amount,
    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,
    '2019' AS year
FROM (
    SELECT 
        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,
        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect
    FROM (
        SELECT 
            week_date,
            ROUND((JULIANDAY(week_date) - JULIANDAY('2019-06-15')) / 7.0) + 1 AS delta_weeks,
            sales 
        FROM cleaned_weekly_sales
    ) add_delta_weeks
) AS add_before_after
UNION ALL
SELECT 
    before_effect,
    after_effect,
    after_effect - before_effect AS change_amount,
    ROUND(((after_effect * 1.0 / before_effect) - 1) * 100, 2) AS percent_change,
    '2020' AS year
FROM (
    SELECT 
        SUM(CASE WHEN delta_weeks BETWEEN 1 AND 4 THEN sales END) AS after_effect,
        SUM(CASE WHEN delta_weeks BETWEEN -3 AND 0 THEN sales END) AS before_effect
    FROM (
        SELECT 
            week_date,
            ROUND((JULIANDAY(week_date) - JULIANDAY('2020-06-15')) / 7.0) + 1 AS delta_weeks,
            sales 
        FROM cleaned_weekly_sales
    ) add_delta_weeks
) AS add_before_after
ORDER BY year;
",,snowflake
514,local302,bank_sales_trading,"Analyze the average percentage change in sales between the 12 weeks before and after June 15, 2020, for each attribute type: region, platform, age band, demographic, and customer type. For each attribute type, calculate the average percentage change in sales across all its attribute values. Identify the attribute type with the highest negative impact on sales and provide its average percentage change in sales.",,,snowflake
515,local329,log,"How many unique sessions visited the /regist/input page and then the /regist/confirm page, in that order?",,,snowflake
516,local330,log,"Using the activity log table, compute the total number of unique user sessions where each web page appears as either a landing page (the first page visited in a session based on timestamp) or an exit page (the last page visited in a session based on timestamp), or both. Count each session only once per page even if the page serves as both landing and exit for that session. ",,,snowflake
517,local331,log,"Which three distinct third-page visits are most frequently observed immediately after two consecutive visits to the '/detail' page, and how many times does each third-page visit occur?",,,snowflake
518,local358,log,"How many users are there in each age category (20s, 30s, 40s, 50s, and others)?",,,snowflake
519,local360,log,"For each user session in the activity log table, identify the number of events that occurred before the first '/detail' click or '/complete' conversion, counting only events that have a non-empty search type. Find the sessions with the minimum count of such pre-click/pre-conversion events. If multiple sessions share this minimum count, include all of them in the results. Return each qualifying session along with the corresponding path and search type.",,,snowflake
520,local344,f1,"Considering all races where pit stop data is available, and focusing on instances when a driver was not behind another car on the previous lap but is behind on the current lap (accounting for retirements, pit-stop entries, pit-stop exits, and race starts), how many times has each type of overtake occurred in Formula 1?",,"# Overtake Label Classification

In racing, overtakes are categorized into different states based on specific conditions, reflecting the circumstances in which the overtaking occurred. Below are the classifications and their detailed explanations:

## 1. R (Retirement) - Overtake during Retirement
An overtake is labeled as **R (Retirement)** if the overtaken driver retired on the same lap as the overtake. This indicates that the overtake occurred just before or during the overtaken driver's retirement, meaning they could no longer continue the race after that lap.

## 2. P (Pit) - Overtake related to Pit Stops
An overtake is classified as **P (Pit)** under two scenarios:
   - **Pit Entry**: If the overtake occurred while the overtaken driver was entering the pit lane, and the driver pitted on the same lap, it indicates that the overtaking happened due to the overtaken driver reducing speed to enter the pit lane.
   - **Pit Exit**: If the overtake occurred as the overtaken driver was exiting the pit lane, especially if the driver pitted on the previous lap and the time gap between the drivers was less than a typical pit stop duration. This suggests that the overtake happened while the overtaken driver was potentially at a lower speed, rejoining the race track from the pit lane.

## 3. S (Start) - Overtake at Race Start
If the overtake took place on the first lap of the race, and the two drivers were within two grid positions of each other at the start, the overtake is classified as **S (Start)**. This classification indicates that the overtake was part of the initial racing shuffle during the race's launch phase, where close position changes are common.

## 4. T (Track) - Overtake under Normal Racing Conditions
If none of the above conditions apply, the overtake is categorized as **T (Track)**, meaning it occurred during normal racing conditions on the track, without any external factors like pit stops or retirements influencing the outcome. This is the default classification for overtakes that happen during regular competition.

---

These classifications help to identify and record the context of each overtake with clarity, ensuring accurate representation of race dynamics.






",snowflake
521,local336,f1,"In the first five laps of the race, how many overtakes occurred in each category—retirements, pit stops, start-related overtakes, and standard on-track passes?",,"# Overtake Label Classification

In racing, overtakes are categorized into different states based on specific conditions, reflecting the circumstances in which the overtaking occurred. Below are the classifications and their detailed explanations:

## 1. R (Retirement) - Overtake during Retirement
An overtake is labeled as **R (Retirement)** if the overtaken driver retired on the same lap as the overtake. This indicates that the overtake occurred just before or during the overtaken driver's retirement, meaning they could no longer continue the race after that lap.

## 2. P (Pit) - Overtake related to Pit Stops
An overtake is classified as **P (Pit)** under two scenarios:
   - **Pit Entry**: If the overtake occurred while the overtaken driver was entering the pit lane, and the driver pitted on the same lap, it indicates that the overtaking happened due to the overtaken driver reducing speed to enter the pit lane.
   - **Pit Exit**: If the overtake occurred as the overtaken driver was exiting the pit lane, especially if the driver pitted on the previous lap and the time gap between the drivers was less than a typical pit stop duration. This suggests that the overtake happened while the overtaken driver was potentially at a lower speed, rejoining the race track from the pit lane.

## 3. S (Start) - Overtake at Race Start
If the overtake took place on the first lap of the race, and the two drivers were within two grid positions of each other at the start, the overtake is classified as **S (Start)**. This classification indicates that the overtake was part of the initial racing shuffle during the race's launch phase, where close position changes are common.

## 4. T (Track) - Overtake under Normal Racing Conditions
If none of the above conditions apply, the overtake is categorized as **T (Track)**, meaning it occurred during normal racing conditions on the track, without any external factors like pit stops or retirements influencing the outcome. This is the default classification for overtakes that happen during regular competition.

---

These classifications help to identify and record the context of each overtake with clarity, ensuring accurate representation of race dynamics.






",snowflake
522,local335,f1,"In Formula 1 seasons since 2001, considering only drivers who scored points in a season, which five constructors have had the most seasons where their drivers scored the fewest total points among all point-scoring drivers in that season?",,,snowflake
523,local309,f1,"For each year, which driver and which constructor scored the most points? I want the full name of each driver.","with year_points as (
    select races.year,
           drivers.forename || ' ' || drivers.surname as driver,
           constructors.name as constructor,
           sum(results.points) as points
    from results
    left join races on results.race_id = races.race_id  -- Ensure these columns exist in your schema
    left join drivers on results.driver_id = drivers.driver_id  -- Ensure these columns exist in your schema
    left join constructors on results.constructor_id = constructors.constructor_id  -- Ensure these columns exist in your schema
    group by races.year, driver
    union
    select races.year,
           null as driver,
           constructors.name as constructor,
           sum(results.points) as points
    from results
    left join races on results.race_id = races.race_id  -- Ensure these columns exist in your schema
    left join drivers on results.driver_id = drivers.driver_id  -- Ensure these columns exist in your schema
    left join constructors on results.constructor_id = constructors.constructor_id  -- Ensure these columns exist in your schema
    group by races.year, constructor
),
max_points as (
    select year,
           max(case when driver is not null then points else null end) as max_driver_points,
           max(case when constructor is not null then points else null end) as max_constructor_points
    from year_points
    group by year
)
select max_points.year,
       drivers_year_points.driver,
       constructors_year_points.constructor
from max_points
left join year_points as drivers_year_points on
    max_points.year = drivers_year_points.year and
    max_points.max_driver_points = drivers_year_points.points and
    drivers_year_points.driver is not null
left join year_points as constructors_year_points on
    max_points.year = constructors_year_points.year and
    max_points.max_constructor_points = constructors_year_points.points and
    constructors_year_points.constructor is not null
order by max_points.year;",,snowflake
524,local310,f1,"Using only the data from the ‘results’ table, find the three years in which the sum of the highest total points earned by any driver and the highest total points earned by any constructor in that year (both calculated by summing up points from the ‘results’ table) is smallest, and list those three years in order of ascending total.",,,snowflake
525,local311,f1,"Which constructors had the top 3 combined points from their best driver and team, and in which years did they achieve them?",,,snowflake
526,local354,f1,"Among Formula 1 drivers who raced during the 1950s, which drivers completed a season in that decade with the same constructor in both the first and the last race they participated in, while also taking part in at least two distinct race rounds during that season?",,,snowflake
527,local355,f1,"Calculate the overall average first round and average last round of races missed by Formula 1 drivers across all years. Include only drivers who missed fewer than three races in a given year and who switched teams between their appearances immediately before and after their hiatus (i.e., the constructor ID for the race right before their first missed race must be different from the constructor ID for the race right after their last missed race in that year). Do not group results by year; return just the overall averages across the entire dataset.",,,snowflake
528,local356,f1,"Provide the full names of drivers who have been overtaken on track more times than they have overtaken others on track during race laps, excluding position changes due to pit stops (both at pit entry and exit), retirements, or position changes that occurred during the first lap of a race (considered as start movements).",,,snowflake
529,sf001,GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI,"Assuming today is April 1, 2024, I would like to know the daily snowfall amounts greater than 6 inches for each U.S. postal code during the week ending after the first two full weeks of the previous year. Show the postal code, date, and snowfall amount.","WITH timestamps AS
(   
    SELECT
        DATE_TRUNC(year,DATEADD(year,-1,DATE '2024-08-29')) AS ref_timestamp,
        LAST_DAY(DATEADD(week,2 + CAST(WEEKISO(ref_timestamp) != 1 AS INTEGER),ref_timestamp),week) AS end_week,
        DATEADD(day, day_num - 7, end_week) AS date_valid_std
    FROM
    (   
        SELECT
            ROW_NUMBER() OVER (ORDER BY SEQ1()) AS day_num
        FROM
            TABLE(GENERATOR(rowcount => 7))
    ) 
)
SELECT
    country,
    postal_code,
    date_valid_std,
    tot_snowfall_in 
FROM 
    GLOBAL_WEATHER__CLIMATE_DATA_FOR_BI.standard_tile.history_day
NATURAL INNER JOIN
    timestamps
WHERE
    country='US' AND
    tot_snowfall_in > 6.0 
ORDER BY 
    postal_code,date_valid_std
;",,snowflake
530,sf003,GLOBAL_GOVERNMENT,"For each year from 2015 to 2020, which Census Zip Code Tabulation Area had the second-highest annual population growth rate? Include only areas with a population estimate of at least 25,000 people in that year (based on 5-Year American Community Survey estimates). For each year, provide the zip code, state abbreviation, and the annual growth rate percentage.",,,snowflake
531,sf002,FINANCE__ECONOMICS,"As of December 31, 2022, list the top 10 active banks with assets exceeding $10 billion, ranked by the highest percentage of uninsured assets, where the percentage is calculated as one minus the value of the '% Insured (Estimated)' variable from quarterly estimates. Provide the names of these banks and their respective percentages of uninsured assets.","WITH big_banks AS (
    SELECT id_rssd
    FROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries
    WHERE variable = 'ASSET'
      AND date = '2022-12-31'
      AND value > 1E10
)
SELECT name
FROM FINANCE__ECONOMICS.CYBERSYN.financial_institution_timeseries AS ts
INNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_attributes AS att ON (ts.variable = att.variable)
INNER JOIN FINANCE__ECONOMICS.CYBERSYN.financial_institution_entities AS ent ON (ts.id_rssd = ent.id_rssd)
INNER JOIN big_banks ON (big_banks.id_rssd = ts.id_rssd)
WHERE ts.date = '2022-12-31'
  AND att.variable_name = '% Insured (Estimated)'
  AND att.frequency = 'Quarterly'
  AND ent.is_active = True
ORDER BY (1 - value) DESC
LIMIT 10;",,snowflake
532,sf044,FINANCE__ECONOMICS,"What was the percentage change in post-market close prices for the Magnificent 7 tech companies from January 1 to June 30, 2024?","WITH ytd_performance AS (
  SELECT
    ticker,
    MIN(date) OVER (PARTITION BY ticker) AS start_of_year_date,
    FIRST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS start_of_year_price,
    MAX(date) OVER (PARTITION BY ticker) AS latest_date,
    LAST_VALUE(value) OVER (PARTITION BY ticker ORDER BY date ASC ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) AS latest_price
  FROM FINANCE__ECONOMICS.CYBERSYN.stock_price_timeseries
  WHERE
    ticker IN ('AAPL', 'MSFT', 'AMZN', 'GOOGL', 'META', 'TSLA', 'NVDA')
    AND date BETWEEN DATE '2024-01-01' AND DATE '2024-06-30'  -- Adjusted to cover only from the start of 2024 to the end of June 2024
    AND variable_name = 'Post-Market Close'
)
SELECT
  ticker,
  (latest_price - start_of_year_price) / start_of_year_price * 100 AS percentage_change_ytd
FROM
  ytd_performance
GROUP BY
  ticker, start_of_year_date, start_of_year_price, latest_date, latest_price
ORDER BY percentage_change_ytd DESC;",,snowflake
533,sf006,FINANCE__ECONOMICS,"For each U.S. state, find how the number of active financial branch entities has changed from March 1, 2020, to December 31, 2021. An entity is considered active on a specific date if its start date is on or before that date and its end date is either null or on or after that date. For each state, calculate the number of entities active on March 1, 2020, the number of entities active on December 31, 2021, and the percentage change in these counts",,,snowflake
534,sf008,US_REAL_ESTATE,"Determine the percentage change in gross income inflow and the seasonally-adjusted purchase-only home price index for the Phoenix-Mesa-Scottsdale, AZ Metro Area from January 1, 2023, to December 31, 2023. Gross income inflow refers to the total adjusted gross income from all financial entities within the specified metro area",,,snowflake
535,sf010,US_REAL_ESTATE,"What are the cumulative ratios of mortgages near default in California for each recorded date in 2023, including those that are 90 to 180 days past due, in forbearance, or undergoing foreclosure, bankruptcy, or deed-in-lieu processes?",,,snowflake
536,sf037,US_REAL_ESTATE,"How can we find the shortest straight-line distance in miles between each 'The Home Depot' store and its nearest 'Lowe's Home Improvement' location? Using the US_REAL_ESTATE.CYBERSYN database, join the point_of_interest_index table with point_of_interest_addresses_relationships and us_addresses tables to get geographic coordinates. For each 'The Home Depot' location (identified by its poi_id), calculate its distance to all 'Lowe's Home Improvement' stores using ST_DISTANCE and ST_MAKEPOINT functions, convert the distance from meters to miles (dividing by 1609), and return only the record with the minimum distance for each 'The Home Depot' store using QUALIFY with ROW_NUMBER().",,,snowflake
537,sf012,WEATHER__ENVIRONMENT,"Using data from the FEMA National Flood Insurance Program Claim Index, for each year from 2010 through 2019, what were the total building damage amounts and total contents damage amounts reported under the National Flood Insurance Program for the NFIP community named 'City Of New York,' grouped by each year of loss?","SELECT 
    YEAR(claims.date_of_loss)               AS year_of_loss,
    claims.nfip_community_name,
    SUM(claims.building_damage_amount) AS total_building_damage_amount,
    SUM(claims.contents_damage_amount) AS total_contents_damage_amount
FROM WEATHER__ENVIRONMENT.CYBERSYN.fema_national_flood_insurance_program_claim_index claims
WHERE 
    claims.nfip_community_name = 'City Of New York' 
    AND year_of_loss >=2010 AND year_of_loss <=2019
GROUP BY year_of_loss, claims.nfip_community_name
ORDER BY year_of_loss, claims.nfip_community_name;",,snowflake
538,sf018,BRAZE_USER_EVENT_DEMO_DATASET,"Examine user engagement with push notifications within a specified one-hour window on June 1, 2023.","WITH push_send AS (
    SELECT
        id,
        app_group_id,
        user_id,
        campaign_id,
        message_variation_id,
        platform,
        ad_tracking_enabled,
        TO_TIMESTAMP(TIME) AS ""TIME"",
        'Send' AS ""EVENT_TYPE""
    FROM
        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_SEND_VIEW
    WHERE
        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'
),
push_bounce AS (
    SELECT
        id,
        app_group_id,
        user_id,
        campaign_id,
        message_variation_id,
        platform,
        ad_tracking_enabled,
        TO_TIMESTAMP(TIME) AS ""TIME"",
        'Bounce' AS ""EVENT_TYPE""
    FROM
        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_BOUNCE_VIEW
    WHERE
        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'
),
push_open AS (
    SELECT
        id,
        app_group_id,
        user_id,
        campaign_id,
        message_variation_id,
        platform,
        ad_tracking_enabled,
        TO_TIMESTAMP(TIME) AS ""TIME"",
        'Open' AS ""EVENT_TYPE"",
        carrier,
        browser,
        device_model
    FROM
        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_OPEN_VIEW
    WHERE
        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'
),
push_open_influence AS (
    SELECT
        id,
        app_group_id,
        user_id,
        campaign_id,
        message_variation_id,
        platform,
        TO_TIMESTAMP(TIME) AS ""TIME"",
        'Influenced Open' AS ""EVENT_TYPE"",
        carrier,
        browser,
        device_model
    FROM
        BRAZE_USER_EVENT_DEMO_DATASET.PUBLIC.USERS_MESSAGES_PUSHNOTIFICATION_INFLUENCEDOPEN_VIEW
    WHERE
        TO_TIMESTAMP(TIME) BETWEEN '2023-06-01 08:00:00' AND '2023-06-01 09:00:00'
)
SELECT
    ps.app_group_id,
    ps.campaign_id,
    ps.user_id,
    ps.time,
    po.time push_open_time,
    ps.message_variation_id,
    ps.platform,
    ps.ad_tracking_enabled,
    po.carrier,
    po.browser,
    po.device_model,
    COUNT(
        DISTINCT ps.id
    ) push_notification_sends,
    COUNT(
        DISTINCT ps.user_id
    ) unique_push_notification_sends,
    COUNT(
        DISTINCT pb.id
    ) push_notification_bounced,
    COUNT(
        DISTINCT pb.user_id
    ) unique_push_notification_bounced,
    COUNT(
        DISTINCT po.id
    ) push_notification_open,
    COUNT(
        DISTINCT po.user_id
    ) unique_push_notification_opened,
    COUNT(
        DISTINCT poi.id
    ) push_notification_influenced_open,
    COUNT(
        DISTINCT poi.user_id
    ) unique_push_notification_influenced_open
FROM
    push_send ps
    LEFT JOIN push_bounce pb
    ON ps.message_variation_id = pb.message_variation_id
    AND ps.user_id = pb.user_id
    AND ps.app_group_id = pb.app_group_id
    LEFT JOIN push_open po
    ON ps.message_variation_id = po.message_variation_id
    AND ps.user_id = po.user_id
    AND ps.app_group_id = po.app_group_id
    LEFT JOIN push_open_influence poi
    ON ps.message_variation_id = poi.message_variation_id
    AND ps.user_id = poi.user_id
    AND ps.app_group_id = poi.app_group_id
GROUP BY
    1,2,3,4,5,6,7,8,9,10,11;
","# Push Notification Analysis

## Overview
This document details an SQL query that analyzes user responses to push notifications within the timeframe of 8:00 AM to 9:00 AM on June 1, 2023. It is designed to assess the impact and effectiveness of push notification campaigns across various metrics.

## Data Sources
The analysis utilizes four distinct subqueries pulling from push notification event views:
- **Send Events**
- **Bounce Events**
- **Open Events**
- **Influenced Open Events**

## Query Description
Each subquery counts and categorizes events based on the type of interaction (sent, bounced, opened, influenced open), along with detailed user and device information.

### Send Events
- **Table**: `USERS_MESSAGES_PUSHNOTIFICATION_SEND_VIEW`
- **Columns**: ID, app group ID, user ID, campaign ID, message variation ID, platform, ad tracking enabled, event timestamp

### Bounce Events
- **Table**: `USERS_MESSAGES_PUSHNOTIFICATION_BOUNCE_VIEW`
- **Columns**: Same as Send Events

### Open Events
- **Table**: `USERS_MESSAGES_PUSHNOTIFICATION_OPEN_VIEW`
- **Columns**: Same as Send Events plus carrier, browser, and device model

### Influenced Open Events
- **Table**: `USERS_MESSAGES_PUSHNOTIFICATION_INFLUENCEDOPEN_VIEW`
- **Columns**: Same as Open Events

## Results Aggregation
The final output of the query is grouped by:
- App Group ID
- Campaign ID
- User ID
- Message Variation ID
- Platform
- Ad Tracking Enabled
- Device Information (Carrier, Browser, Device Model)

### Metrics Included
- `push_notification_sends`: Total number of notifications sent.
- `unique_push_notification_sends`: Number of unique users who received notifications.
- `push_notification_bounced`: Total number of notifications that bounced.
- `unique_push_notification_bounced`: Number of unique users who received bounced notifications.
- `push_notification_open`: Total number of notifications opened.
- `unique_push_notification_opened`: Number of unique users who opened notifications.
- `push_notification_influenced_open`: Total number of indirectly opened notifications.
- `unique_push_notification_influenced_open`: Number of unique users who opened notifications indirectly.

## Conclusion
This analysis provides valuable insights into the reach and responsiveness of users to push notifications, helping to guide future marketing strategies and campaign optimizations.
",snowflake
539,sf035,BRAZE_USER_EVENT_DEMO_DATASET,"How many unique users started sessions each day within each app group between June 1, 2023, and June 7, 2023? Also show the app group ID and the start day of the session.",,,snowflake
540,sf029,AMAZON_VENDOR_ANALYTICS__SAMPLE_DATASET,"Generate a daily detailed sales report for each product under the 'Manufacturing' distributor view, covering the 30 days leading up to February 6, 2022, by joining the sales, traffic, inventory, and net PPM data on date, ASIN, program, period, and distributor_view. The report must include total ordered units, ordered revenue, average selling price, glance views, conversion rate, shipped units, shipped revenue, average net PPM, average procurable product OOS, total on-hand units and value, net received units and value, open purchase order quantities, unfilled customer ordered units, and average vendor confirmation rate, receive fill rate, sell-through rate, and vendor lead time.",,,snowflake
541,sf040,US_ADDRESSES__POI,"Find the top 10 northernmost addresses in Florida's largest zip code area. What are their address numbers, street names, and types?","WITH zip_areas AS (
    SELECT
        geo.geo_id,
        geo.geo_name AS zip,
        states.related_geo_name AS state,
        countries.related_geo_name AS country,
        ST_AREA(TRY_TO_GEOGRAPHY(value)) AS area
    FROM US_ADDRESSES__POI.CYBERSYN.geography_index AS geo
    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS states
        ON (geo.geo_id = states.geo_id AND states.related_level = 'State')
    JOIN US_ADDRESSES__POI.CYBERSYN.geography_relationships AS countries
        ON (geo.geo_id = countries.geo_id AND countries.related_level = 'Country')
    JOIN US_ADDRESSES__POI.CYBERSYN.geography_characteristics AS chars
        ON (geo.geo_id = chars.geo_id AND chars.relationship_type = 'coordinates_geojson')
    WHERE geo.level = 'CensusZipCodeTabulationArea'
),

zip_area_ranks AS (
    SELECT
        *,
        ROW_NUMBER() OVER (PARTITION BY country, state ORDER BY area DESC, geo_id) AS zip_area_rank
    FROM zip_areas
)

SELECT addr.number, addr.street, addr.street_type
FROM US_ADDRESSES__POI.CYBERSYN.us_addresses AS addr
JOIN zip_area_ranks AS areas
    ON (addr.id_zip = areas.geo_id)
WHERE addr.state = 'FL' AND areas.country = 'United States' AND areas.zip_area_rank = 1
ORDER BY LATITUDE DESC
LIMIT 10;",,snowflake
542,sf009,NETHERLANDS_OPEN_MAP_DATA,"A real estate company needs a detailed side-by-side comparison of buildings in Amsterdam and Rotterdam. They require a report showing each building class and subclass, with the total surface area (in square meters) and the total number of buildings for each classification category in both cities. The data should be organized by building class and subclass in ascending order, with Amsterdam and Rotterdam statistics presented in parallel columns to facilitate direct comparison. Can you generate this comprehensive building classification comparison report?",,,snowflake
543,sf013,NETHERLANDS_OPEN_MAP_DATA,"Compare the total road lengths in Amsterdam and Rotterdam by creating a side-by-side analysis of both cities. For each combination of road class and subclass, calculate the total length of roads (in meters) specifically for QUADKEY segments '12020210' and '12020211'. Present the results with columns for class, subclass, Amsterdam's road lengths, and Rotterdam's road lengths.",,,snowflake
544,sf041,YES_ENERGY__SAMPLE_DATA,"Produce a report for ERCOT on October 1, 2022, that combines hourly data on day-ahead and real-time prices from node ID 10000697078, load forecasts (datatypeid 19060) and actual loads, plus wind (forecast datatypeid 9285, actual datatypeid 16) and solar (forecast datatypeid 662, actual datatypeid 650) generation forecasts and actuals from object ID 10000712973. This report should include time zone alignments, peak classifications, and net load calculations, providing insights into daily operational dynamics and efficiency.",,"### ERCOT Daily Market Dynamics Report: Data Columns Description

This documentation outlines the columns included in the dataset for the ERCOT daily market dynamics report generated for October 1, 2022. Each column represents specific data points critical for assessing market behavior and operational effectiveness.

#### Column Descriptions

- **iso**: The ISO (Independent System Operator) code for ERCOT, indicating the specific grid management entity.

- **datetime**: Records the specific date and time for which the dataset provides data, targeted at October 1, 2022.

- **timezone**: Specifies the local time zone applicable to the datetime values.

- **datetime_utc**: Provides the Coordinated Universal Time (UTC) equivalent of the local datetime.

- **onpeak**: A binary indicator noting whether the reported time falls within peak demand hours.

- **offpeak**: A binary indicator showing whether the time is considered off-peak in terms of energy demand.

- **wepeak**: Indicates whether the datetime falls during weekend peak hours.

- **wdpeak**: Indicates whether the datetime falls during weekday peak hours.

- **marketday**: Signifies the market day that corresponds with the datetime, useful for financial and trading analyses.

- **price_node_name**: The descriptive name of the price node from which energy pricing data is sourced.

- **price_node_id**: The unique identifier for the price node, facilitating precise data querying and aggregation.

- **dalmp**: The Day-Ahead Locational Marginal Price, reflecting the planned price of electricity for the next day.

- **rtlmp**: The Real-Time Locational Marginal Price, indicating the actual price of electricity at the time of generation and consumption.

- **load_zone_name**: The name assigned to the load zone which is the focus of the load and generation data.

- **load_zone_id**: The unique numeric identifier for the load zone concerned.

- **load_forecast**: The forecasted electrical load for the datetime, which predicts the total power demand.

- **load_forecast_publish_date**: The date when the load forecast data was published.

- **rtload**: The actual measured electrical load at the reported datetime, providing insight into real-time demand.

- **wind_gen_forecast**: The forecasted amount of wind-generated power for the datetime.

- **wind_gen_forecast_publish_date**: The publication date of the wind generation forecast data.

- **wind_gen**: The actual measured amount of electricity generated from wind at the reported datetime, aggregated to an hourly basis.

- **solar_gen_forecast**: The forecasted solar power generation for the datetime.

- **solar_gen_forecast_publish_date**: The date when the solar generation forecast was published.

- **solar_gen**: The actual measured solar energy generation at the datetime, summarized hourly from finer granularity data.

- **net_load_forecast**: Computed as the total forecasted load minus the combined forecasted wind and solar generation, representing the net load expected from non-renewable sources.

- **net_load_real_time**: Calculated as the real-time load minus the sum of actual wind and solar generation, providing a real-time assessment of net load reliance on non-renewable energy sources.

",snowflake
545,sf011,CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE,"Determine the population distribution within each block group relative to its census tract in New York State using 2021 ACS data. Include block group ID, census value, state county tract ID, total tract population, and the population ratio of each block group."," WITH TractPop AS (
    SELECT
        CG.""BlockGroupID"",
        FCV.""CensusValue"",
        CG.""StateCountyTractID"",
        CG.""BlockGroupPolygon""
    FROM
        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.""Dim_CensusGeography"" CG
    JOIN
        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.""Fact_CensusValues_ACS2021"" FCV
        ON CG.""BlockGroupID"" = FCV.""BlockGroupID""
    WHERE
        CG.""StateAbbrev"" = 'NY'
        AND FCV.""MetricID"" = 'B01003_001E'
),

TractGroup AS (
    SELECT
        CG.""StateCountyTractID"",
        SUM(FCV.""CensusValue"") AS ""TotalTractPop""
    FROM
        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.""Dim_CensusGeography"" CG
    JOIN
        CENSUS_GALAXY__ZIP_CODE_TO_BLOCK_GROUP_SAMPLE.PUBLIC.""Fact_CensusValues_ACS2021"" FCV
        ON CG.""BlockGroupID"" = FCV.""BlockGroupID""
    WHERE
        CG.""StateAbbrev"" = 'NY'
        AND FCV.""MetricID"" = 'B01003_001E'
    GROUP BY
        CG.""StateCountyTractID""
)

SELECT
    TP.""BlockGroupID"",
    TP.""CensusValue"",
    TP.""StateCountyTractID"",
    TG.""TotalTractPop"",
    CASE WHEN TG.""TotalTractPop"" <> 0 THEN TP.""CensusValue"" / TG.""TotalTractPop"" ELSE 0 END AS ""BlockGroupRatio""
FROM
    TractPop TP
JOIN
    TractGroup TG
    ON TP.""StateCountyTractID"" = TG.""StateCountyTractID"";",,snowflake
546,sf014,CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE,"What is the New York State ZIP code with the highest number of commuters traveling over one hour, according to 2021 ACS data? Include the zip code, the total commuters, state benchmark for this duration, and state population.","WITH Commuters AS (
    SELECT
        GE.""ZipCode"",
        SUM(CASE WHEN M.""MetricID"" = 'B08303_013E' THEN F.""CensusValueByZip"" ELSE 0 END +
            CASE WHEN M.""MetricID"" = 'B08303_012E' THEN F.""CensusValueByZip"" ELSE 0 END) AS ""Num_Commuters_1Hr_Travel_Time""
    FROM
        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.""LU_GeographyExpanded"" GE
    JOIN
        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.""Fact_CensusValues_ACS2021_ByZip"" F
        ON GE.""ZipCode"" = F.""ZipCode""
    JOIN
        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.""Dim_CensusMetrics"" M
        ON F.""MetricID"" = M.""MetricID""
    WHERE
        GE.""PreferredStateAbbrev"" = 'NY'
        AND (M.""MetricID"" = 'B08303_013E' OR M.""MetricID"" = 'B08303_012E') -- Metric IDs for commuters with 1+ hour travel time
    GROUP BY
        GE.""ZipCode""
),

StateBenchmark AS (
    SELECT
        SB.""StateAbbrev"",
        SUM(SB.""StateBenchmarkValue"") AS ""StateBenchmark_Over1HrTravelTime"",
        SB.""TotalStatePopulation""
    FROM
        CENSUS_GALAXY__AIML_MODEL_DATA_ENRICHMENT_SAMPLE.PUBLIC.""Fact_StateBenchmark_ACS2021"" SB
    WHERE
        SB.""MetricID"" IN ('B08303_013E', 'B08303_012E')
        AND SB.""StateAbbrev"" = 'NY'
    GROUP BY
        SB.""StateAbbrev"", SB.""TotalStatePopulation""
)

SELECT
    C.""ZipCode"",
    SUM(C.""Num_Commuters_1Hr_Travel_Time"") AS ""Total_Commuters_1Hr_Travel_Time"",
    SB.""StateBenchmark_Over1HrTravelTime"",
    SB.""TotalStatePopulation"",
FROM
    Commuters C
CROSS JOIN
    StateBenchmark SB
GROUP BY
    C.""ZipCode"", SB.""StateBenchmark_Over1HrTravelTime"", SB.""TotalStatePopulation""
ORDER BY
    ""Total_Commuters_1Hr_Travel_Time"" DESC
LIMIT 1;


",,snowflake
